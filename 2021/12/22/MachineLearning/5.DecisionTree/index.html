<!DOCTYPE html>
<html lang="zh-Hans">

<!-- Head tag -->
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="icon" href="https://s3.bmp.ovh/imgs/2022/01/7b8dab551214d77c.png" type="image/png">
    <link rel="shortcut icon" href="https://s3.bmp.ovh/imgs/2022/01/7b8dab551214d77c.png" type="image/png">


    <!--Description-->
    
        <meta name="description" content="决策树1. 历史背景关于树形结构的历史可以追溯到古巴比伦，在这里就不过多阐述。
1963：最早的发表在文献中的回归树算法是Automatic Interaction Detection(AID, Morgan &amp;amp; Sonquist)。AID 从根节点开始递归的将数据拆分为两个子节点，选择划分">
    

    <!--Author-->
    
        <meta name="author" content="He1o">
    

    <!-- Title -->
    
    <title>5. Decision Tree | He1o</title>

    <!-- Bootstrap Core CSS -->
    <link href="//cdn.jsdelivr.net/npm/bootstrap@3.3.6/dist/css/bootstrap.min.css" rel="stylesheet">
    <!-- <link href="//cdn.bootcss.com/bootstrap/3.3.6/css/bootstrap.min.css" rel="stylesheet"> -->

    <!-- Custom CSS -->
    
<link rel="stylesheet" href="/css/style.css">


    <!-- Custom Fonts -->
    <link href="//cdn.jsdelivr.net/npm/font-awesome@4.5.0/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <!-- <link href="//cdn.bootcss.com/font-awesome/4.5.0/css/font-awesome.min.css" rel="stylesheet" type="text/css"> -->
    <link href='//fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>
    <link href='//fonts.googleapis.com/css?family=Noto+Serif:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>

    <!-- jQuery -->
    <script src="//cdn.bootcss.com/jquery/2.2.1/jquery.min.js"></script>
    <!-- Bootstrap -->
    <script src="//cdn.jsdelivr.net/npm/bootstrap@3.3.6/dist/js/bootstrap.min.js"></script>
    <!-- <script src="//cdn.bootcss.com/bootstrap/3.3.6/js/bootstrap.min.js"></script> -->

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
    <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->
    <link rel="stylesheet"
        href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/base16/tomorrow.min.css">
    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
    <script >hljs.initHighlightingOnLoad();</script> 
<meta name="generator" content="Hexo 5.4.0"><link rel="alternate" href="/atom.xml" title="He1o" type="application/atom+xml">
</head>

<body>

    <!-- Content -->
    <section class="article-container">
    <!-- Back Home -->
    <a class="nav-back" href="/">
    <!-- <i class="fa fa-puzzle-piece"></i> -->
    <!-- <svg t="1641282408158" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="1990" width="20" height="20"><path d="M176.01639 524.791207s-27.260759-212.767201-4.060113-284.047483c13.969751-42.859491 33.233691-71.366668 76.352339-70.132591 70.490473 2.048568 194.416477 193.34283 194.416476 193.34283L192.478976 529.591766z" fill="#C95065" p-id="1991"></path><path d="M193.490919 536.329826l-23.077238-6.812104-0.518312-3.998409c-1.12301-8.737265-27.149692-214.914494-3.813298-286.688408 12.291406-37.614664 31.654073-75.846367 82.399315-74.390156 31.098738 0.900876 74.229726 34.430746 128.220592 99.664051a1175.61869 1175.61869 0 0 1 71.206238 96.480133l3.319666 5.121419z m-11.847138-16.36386l9.872615 2.900081L434.172939 362.201574c-9.058125-13.574846-35.380985-52.078046-67.047398-90.309749-50.86865-61.444689-92.012775-94.332839-119.002037-95.122648-34.788628-0.999602-54.521518 17.462188-70.342384 65.887366-20.781855 63.86348 0.296178 247.839666 3.862661 277.309423z" fill="#A5213D" p-id="1992"></path><path d="M847.971269 524.791207s27.310122-212.705497 4.072454-283.985779c-13.982091-42.921195-33.184328-71.428372-76.37702-70.194295-70.502814 2.048568-194.428818 193.34283-194.428818 193.34283l250.270798 165.637803z" fill="#C95065" p-id="1993"></path><path d="M830.49674 536.329826l-2.394109-1.591959-255.330513-169.068537 3.319666-5.121419a1170.361522 1170.361522 0 0 1 71.23092-96.443111c53.990865-65.233306 97.134194-98.726153 128.220591-99.664051 50.782265-1.468552 70.132591 36.775492 82.399316 74.390156 23.447461 71.773913-2.690288 277.951143-3.813298 286.688408l-0.505972 3.998409z m-240.644998-174.128252l242.668884 160.664473 9.872616-2.900081c3.566482-29.482097 24.681538-213.495306 3.825638-277.309423-15.783844-48.425178-35.504393-66.886969-70.342384-65.887366-26.989262 0.789809-68.133386 33.677959-119.002037 95.122648-31.666414 38.231703-58.001615 76.747243-67.059739 90.309749z" fill="#A5213D" p-id="1994"></path><path d="M867.198187 614.434554c0 82.880605-27.75439 129.787869-99.108716 181.656122-63.530279 46.166817-165.144172 57.347554-258.045483 57.347554-102.428384 0-210.101594-18.794991-275.199151-73.612688-60.358702-50.794606-78.018342-101.453463-78.018343-176.806199 0-156.666064 155.987322-316.910951 353.242176-316.910952s357.129518 171.660099 357.129517 328.326163z" fill="#C95065" p-id="1995"></path><path d="M510.043988 859.596274c-56.384974 0-107.870663-5.516324-153.025537-16.413223-52.559336-12.686311-95.023922-32.40686-126.172024-58.630994-30.691493-25.841571-50.831628-51.744845-63.357508-81.523121-11.501597-27.347144-16.857491-59.124625-16.857491-99.96023 0-78.610699 38.120636-159.418056 104.600359-221.714258a375.801102 375.801102 0 0 1 114.041048-73.415236 367.471082 367.471082 0 0 1 140.771153-27.951842 353.63708 353.63708 0 0 1 141.474577 29.728913 387.426106 387.426106 0 0 1 115.36151 77.364281c32.554949 31.493643 59.470166 68.602336 77.82089 107.364692 18.757969 39.490461 28.667607 81.066512 28.667607 120.038661 0 42.970558-7.490847 76.512769-23.570869 105.451872-15.72214 28.383769-39.79898 53.373826-78.080046 81.189921-54.175976 39.330031-139.771551 58.470564-261.673669 58.470564z m0-567.317498c-91.408077 0-178.941152 34.81331-246.371115 98.03507-63.999229 59.963797-100.700676 137.50085-100.700676 212.705497 0 78.980922 20.547381 125.567326 75.821686 172.092025s151.606349 72.144136 271.250105 72.144137c119.199489 0 202.388614-18.363064 254.417296-56.162841 71.292623-51.83123 96.566518-98.047411 96.566519-176.65811 0-74.599949-38.404474-156.271159-102.724562-218.431613-69.145329-66.91165-157.307784-103.724165-248.259253-103.724165z" fill="#A5213D" p-id="1996"></path><path d="M323.673693 613.89156m-86.237295 0a86.237295 86.237295 0 1 0 172.474589 0 86.237295 86.237295 0 1 0-172.474589 0Z" fill="#FFFFFF" p-id="1997"></path><path d="M337.828555 595.42977m-44.61188 0a44.61188 44.61188 0 1 0 89.22376 0 44.61188 44.61188 0 1 0-89.22376 0Z" fill="#42393B" p-id="1998"></path><path d="M342.024416 587.605722m-16.265133 0a16.265134 16.265134 0 1 0 32.530267 0 16.265134 16.265134 0 1 0-32.530267 0Z" fill="#FFFFFF" p-id="1999"></path><path d="M700.326307 613.89156m-86.237294 0a86.237295 86.237295 0 1 0 172.474589 0 86.237295 86.237295 0 1 0-172.474589 0Z" fill="#FFFFFF" p-id="2000"></path><path d="M686.159104 595.42977m-44.61188 0a44.61188 44.61188 0 1 0 89.223761 0 44.61188 44.61188 0 1 0-89.223761 0Z" fill="#42393B" p-id="2001"></path><path d="M681.975584 587.605722m-16.265134 0a16.265134 16.265134 0 1 0 32.530267 0 16.265134 16.265134 0 1 0-32.530267 0Z" fill="#FFFFFF" p-id="2002"></path></svg> -->
    <svg t="1641282544096" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="2285" width="25" height="25"><path d="M169.607158 516.323824s-28.478046-221.801895-4.233745-296.130495c14.56717-44.705255 34.654938-74.457285 79.617564-73.144695 73.505014 2.136175 202.717873 201.598311 202.717873 201.598311L186.77377 521.394023z" fill="#647471" p-id="2286"></path><path d="M187.828989 528.407369l-24.064141-7.103426-0.540478-4.156533c-1.171036-9.123784-28.375097-224.105361-3.976374-298.948702 12.791314-39.223264 33.033503-79.064217 85.92314-77.584336 32.428683 0.939402 77.404177 35.903184 133.703974 103.926207a1228.416518 1228.416518 0 0 1 74.251389 100.606127l3.461633 5.353307z m-12.353784-17.063664l10.29482 3.024104 253.033805-167.535326c-9.445497-14.155377-36.894061-54.292307-69.914695-94.171865-53.04406-64.072385-95.947722-98.367004-124.091186-99.177722-36.263503-1.068088-56.853143 18.208963-73.350592 68.705055-21.670596 66.633222 0.308845 258.374243 4.027848 289.155754z" fill="#283330" p-id="2287"></path><path d="M870.298339 516.323824s28.478046-221.801895 4.246613-296.130495c-14.580039-44.705255-34.667806-74.457285-79.617564-73.144695-73.517883 2.136175-202.74361 201.598311-202.743609 201.598311l260.947947 172.747078z" fill="#647471" p-id="2288"></path><path d="M852.076507 528.407369l-2.496493-1.647171-266.249781-176.298791 3.461634-5.353306A1222.908789 1222.908789 0 0 1 661.030387 244.501973c56.299796-68.023023 101.288159-102.948199 133.703973-103.926207 52.966848-1.518486 73.131827 38.361073 85.923141 77.584337 24.450197 74.843341-2.805338 289.824918-3.976374 298.948701l-0.52761 4.156534z m-250.936235-181.574886l253.046673 167.535326 10.29482-3.024104c3.719004-30.730037 25.73705-222.625481 3.989243-289.155754-16.510317-50.470355-37.099957-69.773142-73.311986-68.705055-28.143464 0.810717-71.047126 35.105336-124.091186 99.177722-33.05924 39.879559-60.520673 80.042225-69.96617 94.171865z" fill="#283330" p-id="2289"></path><path d="M892.380727 609.852263c0 86.425013-28.928444 135.338276-103.334255 189.424687-66.247166 48.15402-172.219469 59.800035-269.093724 59.800035-106.744414 0-219.086636-19.598763-286.968105-76.747883-62.939955-52.966848-81.367683-105.792143-81.367683-184.380224 0-163.353055 162.658155-330.463719 368.348657-330.46372s372.41511 179.01405 372.41511 342.367105z" fill="#647471" p-id="2290"></path><path d="M519.952748 865.511247c-58.79629 0-112.483776-5.765099-159.569708-17.128006-54.807048-13.215975-99.087642-33.792746-131.55493-61.138362-32.01689-26.933823-53.005454-53.944856-66.079876-85.009475-11.993465-28.503783-17.578405-61.653103-17.578405-104.235052 0-81.959635 39.763742-166.235605 109.073617-231.18305a391.975268 391.975268 0 0 1 118.930907-76.567723 383.121723 383.121723 0 0 1 146.778395-29.147209 368.760449 368.760449 0 0 1 147.52477 31.051751 404.290447 404.290447 0 0 1 120.294971 80.685651c33.960037 32.827607 62.02629 71.523261 81.161786 111.956166 19.547289 41.243622 29.880715 84.520472 29.880715 125.159274 0 44.808204-7.798326 79.784854-24.566014 109.961545-16.407369 29.597607-41.513861 55.65637-81.419157 84.674894-56.505693 40.960515-145.761781 60.919597-272.877071 60.919596z m0-591.591826c-95.304295 0-186.593611 36.314977-256.907231 102.24043-66.73617 62.528162-105.007163 143.368236-105.007163 221.789027 0 82.358559 21.426094 130.950109 79.077086 179.451579s158.089828 75.242265 282.837308 75.242265c124.30995 0 211.043808-19.161234 265.310378-58.564656 74.341468-54.047805 100.683339-102.24043 100.683339-184.225803 0-77.790233-40.033981-162.954131-107.117601-227.77289-72.102345-69.760273-164.035087-108.159952-258.876116-108.159952z" fill="#283330" p-id="2291"></path><path d="M516.735617 428.328851a117.42529 117.42529 0 0 1-5.031593-23.536532 199.886797 199.886797 0 0 1 0-47.060195 116.614573 116.614573 0 0 1 5.05733-23.536532 117.541106 117.541106 0 0 1 5.070199 23.536532 201.739865 201.739865 0 0 1 0 47.060195 117.463895 117.463895 0 0 1-5.095936 23.536532zM586.959158 381.255787a48.2441 48.2441 0 0 1 0.527609 15.056174 71.343102 71.343102 0 0 1-2.959761 13.949481 70.468042 70.468042 0 0 1-5.559202 13.113027 47.201749 47.201749 0 0 1-9.110916 12.006334 47.870913 47.870913 0 0 1-0.540478-15.056174 73.350592 73.350592 0 0 1 8.583306-27.023903 47.973861 47.973861 0 0 1 9.059442-12.044939zM615.591625 452.495941a67.482545 67.482545 0 0 1 7.721115-16.201473 105.521904 105.521904 0 0 1 10.436374-13.769322 103.37286 103.37286 0 0 1 12.495338-11.916254 65.758162 65.758162 0 0 1 15.236333-9.471234 65.809636 65.809636 0 0 1-7.721115 16.201473 101.494056 101.494056 0 0 1-10.449242 13.756453 104.453817 104.453817 0 0 1-12.508206 11.916254 66.517405 66.517405 0 0 1-15.210597 9.484103zM661.467917 469.611079a25.865735 25.865735 0 0 1 4.169402-9.741473 32.338603 32.338603 0 0 1 14.605775-11.581673 25.634102 25.634102 0 0 1 10.449243-1.82733 25.608365 25.608365 0 0 1-4.156534 9.76721 31.566492 31.566492 0 0 1-6.434262 6.858924 32.171312 32.171312 0 0 1-8.158645 4.70988 25.73705 25.73705 0 0 1-10.474979 1.814462zM690.692337 501.846734a38.682786 38.682786 0 0 1 10.397768-8.493227 55.21884 55.21884 0 0 1 11.478724-5.018724 54.356649 54.356649 0 0 1 12.30231-2.367809 38.20665 38.20665 0 0 1 13.383266 1.158167 38.258125 38.258125 0 0 1-10.397768 8.506095 53.790434 53.790434 0 0 1-11.478725 4.992988 54.61402 54.61402 0 0 1-12.302309 2.35494 38.747128 38.747128 0 0 1-13.383266-1.13243zM446.524945 381.255787a48.282705 48.282705 0 0 1 9.085179 12.019202 73.003142 73.003142 0 0 1 8.570437 27.023903 47.124538 47.124538 0 0 1-0.540478 15.056174 47.446251 47.446251 0 0 1-9.098047-12.006334 70.519516 70.519516 0 0 1-5.546334-13.08729 72.269636 72.269636 0 0 1-2.946892-13.949481 48.2441 48.2441 0 0 1 0.476135-15.056174zM417.879609 452.495941a66.144218 66.144218 0 0 1-15.210597-9.484103 108.095609 108.095609 0 0 1-22.957448-25.672707 66.73617 66.73617 0 0 1-7.721115-16.201473 65.629477 65.629477 0 0 1 15.236334 9.471234 107.272024 107.272024 0 0 1 22.918842 25.73705 66.517405 66.517405 0 0 1 7.733984 16.149999zM372.003318 469.611079a25.64697 25.64697 0 0 1-10.436374-1.827331 31.720914 31.720914 0 0 1-8.158645-4.70988 31.257647 31.257647 0 0 1-6.434262-6.858923 25.608365 25.608365 0 0 1-4.156534-9.767211 25.634102 25.634102 0 0 1 10.449242 1.827331 32.171312 32.171312 0 0 1 14.592908 11.581672 25.659839 25.659839 0 0 1 4.143665 9.754342zM342.791766 501.846734a38.772866 38.772866 0 0 1-13.383266 1.13243 54.472466 54.472466 0 0 1-12.30231-2.35494 53.507327 53.507327 0 0 1-11.478724-4.992988 38.258125 38.258125 0 0 1-10.397768-8.506095 38.20665 38.20665 0 0 1 13.383266-1.158167 54.485334 54.485334 0 0 1 12.30231 2.367809 55.514816 55.514816 0 0 1 11.478724 5.018724 38.682786 38.682786 0 0 1 10.397768 8.493227zM520.467489 722.168749a73.955413 73.955413 0 0 1 5.031594 18.196094 118.519114 118.519114 0 0 1 1.3898 18.157488 116.884812 116.884812 0 0 1-1.364063 18.183226 72.835851 72.835851 0 0 1-5.057331 18.183226 72.333978 72.333978 0 0 1-5.070198-18.183226 116.884812 116.884812 0 0 1-1.364064-18.183226 118.519114 118.519114 0 0 1 1.389801-18.183225 73.440672 73.440672 0 0 1 5.044461-18.170357zM590.678161 758.522331a33.535376 33.535376 0 0 1-9.007967-8.5447 45.374419 45.374419 0 0 1-5.572071-9.934501 44.821072 44.821072 0 0 1-3.011235-10.976852 33.213663 33.213663 0 0 1 0.411792-12.366652 33.007766 33.007766 0 0 1 9.007968 8.5447 44.422148 44.422148 0 0 1 5.546334 9.921633 45.631789 45.631789 0 0 1 2.998366 10.98972 33.728404 33.728404 0 0 1-0.373187 12.366652zM619.323498 703.496519a53.584538 53.584538 0 0 1 14.760198 6.125418 81.277603 81.277603 0 0 1 12.379521 8.866413 79.656169 79.656169 0 0 1 10.565059 10.963984 52.490713 52.490713 0 0 1 8.158644 13.730716 53.082665 53.082665 0 0 1-14.773066-6.086813 80.428281 80.428281 0 0 1-12.366653-8.89215 81.30334 81.30334 0 0 1-10.565059-10.976852 53.700354 53.700354 0 0 1-8.158644-13.730716zM665.18692 690.319149a23.240556 23.240556 0 0 1 9.94737 0.167291 27.023902 27.023902 0 0 1 8.02996 3.294343 26.547767 26.547767 0 0 1 6.588684 5.649282 22.777289 22.777289 0 0 1 4.658406 8.789203 23.047528 23.047528 0 0 1-9.947369-0.154423 27.358484 27.358484 0 0 1-14.605776-9.007967 23.008923 23.008923 0 0 1-4.671275-8.737729zM694.41134 665.36708a37.486013 37.486013 0 0 1 13.061553-2.058964 52.760952 52.760952 0 0 1 12.21223 1.492749 51.911629 51.911629 0 0 1 11.581673 4.169402 37.524619 37.524619 0 0 1 10.719481 7.721115 37.537487 37.537487 0 0 1-13.074421 2.071832 52.001709 52.001709 0 0 1-12.199362-1.505617 53.314299 53.314299 0 0 1-11.581672-4.195139 37.666172 37.666172 0 0 1-10.719482-7.695378zM450.282555 758.522331a33.728404 33.728404 0 0 1-0.386056-12.366652 45.631789 45.631789 0 0 1 2.998366-10.98972 44.936889 44.936889 0 0 1 5.546334-9.921633 33.007766 33.007766 0 0 1 9.007968-8.5447 33.213663 33.213663 0 0 1 0.411793 12.366652 44.821072 44.821072 0 0 1-3.011235 10.976852 45.374419 45.374419 0 0 1-5.572071 9.934501 33.535376 33.535376 0 0 1-8.995099 8.5447zM421.598613 703.496519a53.700354 53.700354 0 0 1-8.158645 13.730716 81.30334 81.30334 0 0 1-10.565059 10.976852 80.428281 80.428281 0 0 1-12.366653 8.89215 53.082665 53.082665 0 0 1-14.773066 6.086813 52.851032 52.851032 0 0 1 8.158645-13.743585 79.656169 79.656169 0 0 1 10.565059-10.963983 81.277603 81.277603 0 0 1 12.379521-8.866414 53.584538 53.584538 0 0 1 14.760198-6.112549zM375.73519 690.319149a23.008923 23.008923 0 0 1-4.671275 8.776334 27.229799 27.229799 0 0 1-14.605775 9.007968 23.047528 23.047528 0 0 1-9.94737 0.154422 22.777289 22.777289 0 0 1 4.684143-8.930756 26.547767 26.547767 0 0 1 6.588685-5.649283 27.023902 27.023902 0 0 1 8.029959-3.294342 23.240556 23.240556 0 0 1 9.921633-0.064343zM346.51077 665.36708a37.666172 37.666172 0 0 1-10.719481 7.721115 53.314299 53.314299 0 0 1-11.581673 4.195139 52.001709 52.001709 0 0 1-12.199361 1.505617 37.537487 37.537487 0 0 1-13.074422-2.071832 37.524619 37.524619 0 0 1 10.719482-7.721115 51.911629 51.911629 0 0 1 11.581672-4.169402 52.760952 52.760952 0 0 1 12.21223-1.492749 37.434539 37.434539 0 0 1 13.061553 2.033227z" fill="#4B5451" p-id="2292"></path><path d="M331.248699 594.114057m-74.740392 0a74.740393 74.740393 0 1 0 149.480785 0 74.740393 74.740393 0 1 0-149.480785 0Z" fill="#E8BE48" p-id="2293"></path><path d="M708.656797 594.114057m-74.740392 0a74.740393 74.740393 0 1 0 149.480785 0 74.740393 74.740393 0 1 0-149.480785 0Z" fill="#E8BE48" p-id="2294"></path></svg>
</a>


        <!-- Page Header -->
        <header class="intro-header">
            <div class="container">
                <div class="row">
                    <div class="col-lg-8  col-md-10 ">
                        <div class="post-heading">
                            <h1>
                                5. Decision Tree
                            </h1>
                        </div>
                    </div>
                </div>
            </div>
        </header>

        <!-- Post Content -->
        <article>
            <div class="container">
                <div class="row">
                    <!-- Post Main Content -->
                    <div class="post-content col-lg-10  col-md-10 ">
                        <span id="more"></span>
<h1 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h1><h2 id="1-历史背景"><a href="#1-历史背景" class="headerlink" title="1. 历史背景"></a>1. 历史背景</h2><p>关于树形结构的历史可以追溯到古巴比伦，在这里就不过多阐述。</p>
<p>1963：最早的发表在文献中的回归树算法是Automatic Interaction Detection(AID, Morgan &amp; Sonquist)。AID 从根节点开始递归的将数据拆分为两个子节点，选择划分特征的依据是通过类似于方差的公式 $\sum (X-\overline{X})^2$ ，称为不纯函数，计算划分子节点的误差平方和使其越小说明划分效果越好，因为 $\sum (X-\overline{X})^2=\sum X^2-N\overline{X}^2$，所以最终只要计算划分后 $\sum N_i\overline{X_i}^2$ 即可，选择最大的特征进行划分。当划分后的误差平方减少值小于 $0.02(\sum X^2-N\overline{X}^2)$ 则结束算法。</p>
<p>1972：THeta Automatic Interaction Detection (THAID, Messenger &amp; Mandell) 将上述思想应用到分类问题中，提出了第一个分类树，并用熵函数和基尼指数代替上述不纯函数。</p>
<p>1980：CHi-squared Automatic Interaction Detector (CHAID) 由 Kass 创建，通过 $\mathcal{X}^2$ 拟合优度检验来找到主要特征，假设有两个类别，默认分布概率都是 $1/2$，计算 $\mathcal{X}^2=\sum\sqrt{(n_i-np_i)/(np_i)}$，选取最大的特征进行分割。卡方拟合优度越大意味着观察频数与默认均匀分布的区别越大，也就是越趋于有序，当同一特征将样本均归于同一类时，卡方拟合优度达到最大，当两个类别各有一半时，卡方拟合优度则为0。</p>
<p>1984：Classification And Regression Trees (CART) 伯克利的统计学教授 Leo Breiman 和 Charles Stone 以及斯坦福的 Jerome Friedman 和 Richard Olshen 共同建立。它同样使用 AID 的贪婪搜索方法，但增加了新颖的改进。CART 不再使用停止规则，而是生成一颗大树，通过最低交叉验证对树结构进行剪枝。这解决了 AID 和 THAID 的欠拟合和过拟合问题。</p>
<p>1986：John Ross Quinlan 提出了一个新概念有多个答案的树，而CART 和所有其他算法对每个问题只有两个答案（称为二叉树）。Quinlan 使用称为增益比的杂质标准发明了  Iterative Dichotomiser 3（ID3）。</p>
<p>1993：C4.5 是 ID3 算法的扩展，C4.5 解决了其前身的缺点，在数据挖掘杰出论文的 Top 10 算法中排名第一（Springer LNCS，2008）。</p>
<h2 id="2-决策树结构"><a href="#2-决策树结构" class="headerlink" title="2.决策树结构"></a>2.决策树结构</h2><p>决策树模型是一种对实例进行分类或回归的树形结构。树结构的每个节点均代表输入特征空间的部分子区域，对于不再分割的节点称为终端节点（terminal node）也叫叶节点（leaf node），在图中由矩形框表示；除了叶节点以外的称为内部节点（internal node），在图中用圆形表示。</p>
<p>构建决策树的时候，从根节点开始，通过一定规则选定分割的特征维度和特征值，然后构建下一层的多个节点。特别地，如果每一个节点只通过一个特征的一个值分割为两部分，该决策树则称为二叉决策树。</p>
<p>对于决策树中的每一个节点的后代节点都是不相交的，也就是它们之间互斥，同时一个节点所有后代子集的并集即为该节点。例如图1中所示，$X_2$ 与 $X_3$ 互斥且 $X=X_2\bigcup X_3$。决策树中的每一层中的节点的并集构成整体输入特征空间。</p>
<p>终端节点是输入空间的一个子区域，所有终端节点互斥且完备，即所有终端节点的并集构成整体输入空间。在分类问题中，每个终端节点由一个类标签指定，可能有多个终端节点有相同的类标签。分类器最终分类划分是将同一类对应的所有终端子集放在一起得到的。例如，上图的划分结果为</p>
<script type="math/tex; mode=display">
\begin{aligned}
    A_1&=X_{15} \\
    A_2&=X_{10}\bigcup X_{16}\\
    A_3&=X_{11}\bigcup X_{14}\\
    A_4&=X_{6}\bigcup X_{17}\\
    A_5&=X_8 \\
    A_6&=X_{12} \\

\end{aligned}</script><p>综上所述，可以发现构建决策树需要解决三个问题：</p>
<ol>
<li>在每个中间节点选择拆分的方法</li>
<li>终止条件</li>
<li>每个终端叶子节点的输出结果</li>
</ol>
<p>进一步地，如果想要决策树模型不仅对训练数据有很好的拟合效果，同时对未知数据有很好的预测，也就是避免过拟合现象的发生，我们还可能需要对已生成的决策树进行自下而上的剪枝，使树结构变得更简单，从而使它具有更好的泛化能力，也就是</p>
<ol>
<li>决策树的剪枝 </li>
</ol>
<p>有关第一个问题，在这里先定义一个通用的方法和标准。</p>
<p>定义训练集的输入实例数为 $N$，每个类别的数量为 $N_j$，则每个节点 $t$ 中的某一类别的数量为 $N_j(t)$，落入节点的概率为 $p(t) = N(t)/N$，条件概率$p(t|j)=N_j(t)/N_j$，属于 $j$ 类且落入节点 $t$ 两事件同时发生的概率为 $p(j,t)=p(t|j)p(j)$，在这里我们先假设先验概率 $p(j)$ 就是训练集中的类别概率 $p(j)=N_j/N$。因此，根据贝叶斯公式我们可以得到后验概率</p>
<script type="math/tex; mode=display">
\begin{aligned}
    P(j|t)&=\frac{p(t|j)p(j)}{p(t)}\\
    &=\frac{(N_j(t)/N_j) \ast (N_j/N)}{N(t)/N}\\
    &=\frac{N_j(t)}{N(t)}
\end{aligned}</script><blockquote>
<p>忙活半天，得到了一个很显而易见的结果 -_-!  但这个显而易见的比例实际上是条件概率。</p>
</blockquote>
<p>通常情况下先验概率 $p(j)$ 被视为训练集中的比例 $N_j/N$。但学习样本中的比例无法反映到现实中实际的比例，就例如评价道路的好坏，实际上得到的训练集都是通过 bad case 中的路线，实际上，可能差路线存在的比例会更低。</p>
<p>因此，我们由其他途径的得到先验概率 $\pi(j)$ 取代训练集中的概率 $p(j)$，联合概率即为 </p>
<script type="math/tex; mode=display">p(j,t)=\pi(j)N_j(t)/N_j</script><p>同时任何案例落入节点 $t$ 的概率重新带入估计 </p>
<script type="math/tex; mode=display">p(t)=\sum_jp(j,t)</script><p>最后再去计算条件概率 $p(j|t)$，可以发现，替换后的条件概率依旧满足 </p>
<script type="math/tex; mode=display">\sum_jp(j|t)=1</script><p>决策树划分的目的就是希望能通过特征将实例的类别进行区分出来。决策树的每个内部节点将分割为更多的子区域，每个子区域的条件是一样的，而所希望的就是某一类的条件概率 $P(j|t)$ 尽可能的大。换句话说，希望每个后代节点中的数据比父节点中的数据更“纯”（purer）。</p>
<p>例如，假设有个六分类问题，根节点的条件概率 $(p_1,p_2,p_3,p_4,p_5,p_6)$ 为 $(1/6,1/6,1/6,1/6,1/6,1/6)$，一个好的划分结果就是两个子节点的条件概率也就是类别比例为 $(1/3,1/3,1/3,0,0,0)$ 和 $(0,0,0,1/3,1/3,1/3)$ 。</p>
<p>因此我们需要寻找一个测度去衡量一个节点中子集的不纯度（impurity），定义一个非负函数，要求当所有类均匀混合在一起时它的不纯度最大，当节点中只包含一个类时，该值最小，现将这个函数定义为 $i(t)$。</p>
<p>对于任何中间节点 $t$，假设有一个节点的候选切分 $s$ 将节点分割为 $t_L$ 和 $t_R$，这样 $t$ 中的事件（case）分别进入 $t_L$ 和 $t_R$ 的比例为 $p_L$ 和 $p_R$，那么最终切分点 $s$ 的好坏定义为不纯度的好坏的减少</p>
<script type="math/tex; mode=display">\triangle i(s,t)=i(t)-p_Li(t_L)-p_Ri(t_R)</script><p>对于不是二叉树而是多切分节点的算法只需将上公式扩展为多个即可，但始终 $p_1+p_2+\cdots+p_n=1$。</p>
<p>我们对 $i(t)$ 函数的要求进行总结：</p>
<ol>
<li>当每个类别概率相等时，$i(t)$ 值最大。</li>
<li>当只有一个类别时，$i(t)$ 值最小</li>
<li>$i(t)$ 是轴对称函数</li>
</ol>
<p>至此，我们得到一个通用的标准去衡量切分的好坏程度，在不同的算法中以及回归和分类树中，所不同的只是不纯度函数 $i(t)$ 的不同。</p>
<h2 id="3-CART-算法"><a href="#3-CART-算法" class="headerlink" title="3. CART 算法"></a>3. CART 算法</h2><p>CART 算法由 Breiman 等人在 1984 年提出。CART 同样由特征选择、树的生成以及剪枝组成，既可用于分类也可以用于回归。</p>
<p>CART 是在给定输入随机变量 $X$ 条件下输出随机变量 $Y$ 的条件概率分布的学习方法。CART 是二叉树，递归的二分每个特征，分为“是”或“否”，将输入空间即特征空间划分为有限个单元，并在每个单元上确定预测的概率分布。</p>
<p>CART回归树实际是基于AID的决策树方法，但增加了最优切分点的寻找以及剪枝的过程。CART分类树使用到的基尼指数也不是第一次出现。</p>
<h3 id="3-1-CART-分类"><a href="#3-1-CART-分类" class="headerlink" title="3.1 CART 分类"></a>3.1 CART 分类</h3><p>我们从前述四个问题出发，先解决最直观的第三个问题</p>
<ol>
<li>每个终端叶子节点的输出结果</li>
</ol>
<p><strong>定义1</strong> 一个决策树的所有终端节点的集合为 $T$，每个终端节点 $t\in T$，所有分类标签的集合为 $j\in \{1,2,\cdots,J\}$，则某一节点所分配的类为 $j(t)$。</p>
<blockquote>
<p>$j(t)$ 是类分配结果同时也代表一种类分配规则</p>
</blockquote>
<p>对于任何一种类分配结果 $j(t)$，如果实例落入节点 $t$，则错误分类概率的重新替换估计（resubstitution estimate）为</p>
<script type="math/tex; mode=display">\sum_{j\ne j(t)}p(j|t)</script><p>我们需要最小化这个误分类估计的规则作为我们类分配规则 $j^*(t)$。</p>
<p>由于一个节点最终只能输出一个类别，一个直观的类分配规则就是将某一节点中占比最大的类分配给该节点，这样做的另一个原因就是使误分类的概率最小。</p>
<p><strong>定义2</strong> 类分配规则 $j^\ast(t)$：如果 $\displaystyle p(j|t)=\max_i p(i|t)$，则 $j^\ast(t)=j$，如果两个或多个不同类别达到最大值，则将 $j^\ast(t)$ 任意指定为任何一个最大化类别。</p>
<p>第三个问题得到解决。</p>
<p>进一步地，我们就可以得到节点 $t$ 的误分类概率的重新带入估计 $r(t)$ 为</p>
<script type="math/tex; mode=display">
\begin{aligned}
    r(t)&=\sum_{j\ne j^*(t)}p(j|t)\\
    &=1-\max_j p(j|t)
\end{aligned}</script><p>整体决策树误分类率的重新代入估计则为 </p>
<script type="math/tex; mode=display">R(T)=\sum_{t\in T}r(t)p(t)</script><p>$R(t)$ 的一个重要特性，以任何方式分割的越多，则 $R(T)$ 就会变得越小。一个节点 $t$ 分割成两部分 $t_L$ 和 $t_R$，则有</p>
<script type="math/tex; mode=display">R(t)\ge R(t_L)+R(t_R)</script><blockquote>
<p>以上公式的推导是在假定所有错误分类为 $i$ 类对象的成本或损失都是一样的，在某些问题中希望能将它们区分出来，例如某一类错误分类的代价将会更大，也就是希望尽可能减少其误分类的概率。因此，引入一组错误分类成本 $c(i|j)$ 代表将 $j$ 类对象误分类为 $i$ 类对象的代价，它应该满足</p>
<script type="math/tex; mode=display">{\displaystyle c(i|j)={\begin{cases}C_i(C_i\ge 0)&i\ne j\\0&i=j\end{cases}}}</script><p>随机一个实例落入节点 $t$ 并被分类为类别 $i$，则估计的预期误分类代价为 </p>
<script type="math/tex; mode=display">\sum_j c(i|j)p(j|t)</script><p>一个自然地节点分配规则是选择 $i$ 来最小化这个表达式，因此，<br>令 $j^*(t)=i_0$，当 $i_0$ 最小化 $\displaystyle \sum_j c(i|j)p(j|t)$，给定节点 $t$ 定义预期错误分类成本的重新替代估计 $r(t)$ 为</p>
<script type="math/tex; mode=display">r(t)=\min_i \sum_j c(i|j)p(j|t)</script><p>当 $c(i|j)=1,i\ne j$ 可以发现 </p>
<script type="math/tex; mode=display">\sum_j c(i|j)p(j|t)=1-p(i|t)</script><p>最小化结果与上面相同</p>
</blockquote>
<p>接下来，我们解决第一个问题</p>
<ol>
<li>在每个中间节点选择拆分的方法</li>
</ol>
<p>在第二节，我们已经定义了一个通用的切分准则，需要确定的只是 $i(t)$ 函数是什么。事实上，CART 分类树是通过基尼指数来作为切分函数的，但基尼指数并不是凭空出现的，也不是唯一可行的。通过实验发现，构建决策树的总体误分类率对分割规则的选择并不敏感，只要在合理的规则类别内，区别是不大的，同时原作者给出了其从开始到最终基尼指数的思考过程，在这里我们从头开始追根溯源。</p>
<p>从误分类率 $r(t)$ 出发，我们可以发现 $r(t)$ 是可以直接作为衡量节点不纯度的标准，当节点中只有一个类别时，$r(t)$ 为 $0$，当节点中均匀分布时，$r(t)$ 最大为 $1-\frac{1}{n}$。因此最好的分割方式就是最大化</p>
<script type="math/tex; mode=display">r(t)-P_Lr(t_L)-P_Rr(t_R)</script><p>用误分类率 $r(t)$ 作为不纯函数，存在两个严重的缺陷。</p>
<p>第一个缺陷，用上式对节点 $t$ 进行切分有可能所有拆分结果都为 $0$。证明如下：</p>
<script type="math/tex; mode=display">
\begin{aligned}
    r(t)&=\sum_{j}c(j^*(t)|j)p(j|t)\\
    &=\sum_{j}c(j^*(t)|j)p(j,t)p(t)\\
    &=\sum_{j}c(j^*(t)|j)[p(j,t_L)+p(j,t_R)]p(t)
\end{aligned}</script><p>让 $p(t)=1$，因此</p>
<script type="math/tex; mode=display">
\begin{aligned}
    r(t)&-P_Lr(t_L)-P_Rr(t_R)\\
    &=\sum_{j}c(j^*(t)|j)[p(j,t_L)+p(j,t_R)]-\min_i\sum_jc(i|j)p(j|t_L)P_L-\min_i\sum_jc(i|j)p(j|t_R)P_R\\
    &=\sum_{j}c(j^*(t)|j)[p(j,t_L)+p(j,t_R)]-\min_i\sum_jc(i|j)p(j,t_L)-\min_i\sum_jc(i|j)p(j,t_R)\\
    &=\sum_{j}c(j^*(t)|j)p(j,t_L)-\min_i\sum_jc(i|j)p(j,t_L) + \sum_{j}c(j^*(t)|j)p(j,t_R)-\min_i\sum_jc(i|j)p(j,t_R)\\
    &=\sum_{j}c(j^*(t)|j)p(j,t_L)-\sum_jc(j^*(t_L)|j)p(j,t_L) + \sum_{j}c(j^*(t)|j)p(j,t_R)-\sum_jc(j^*(t_R)|j)p(j,t_R)\\
\end{aligned}</script><p>等号的右边是大于 $0$ 的，并且仅当 $j^<em>(t)=j^</em>(t_L)=j^*(t_R)$ 时等号成立。</p>
<blockquote>
<p>这个式子可以这么理解，$j^\ast(t)$ 是使 $\displaystyle\sum_j c(i|j)p(j|t)$ 最小化的特征，当节点 $t$ 被切分为两部分后， $j^\ast(t_L)$ 是使 $\displaystyle\sum_j c(i|j)p(j|t_L)$ 最小化的特征，那么 $j^\ast(t)$ 无论是什么都不会使 $\displaystyle \sum_{j}c(j^<em>(t)|j)p(j,t_L)$ 比 $\displaystyle \sum_jc(j^</em>(t_L)|j)p(j,t_L)$ 更小。</p>
</blockquote>
<p>从上式中可以知道，当父节点与切分后的两个节点的最大占比的类别一样时，无论怎么切分 $r(t)-P_Lr(t_L)-P_Rr(t_R)$ 均为 $0$，即不存在单个或少量的最优拆分点。</p>
<p>第二个缺陷是很难对决策树进行准确的评价。换句话说，降低错误分类率似乎不是整个决策树生长过程的好的目标和标准。</p>
<p>举一个简单的例子，父节点 $t$ 的分布为 400（类别1）和 400（类别2），一个切分方式生成两个节点，一个节点分布为 300（类别1）、100（类别2），另一个节点分布为 100（类别1）、300（类别2），其中 200 个实例被分类错误，误分类率为 0.25。另一种切分方式为200（类别1）、400（类别2）和 200（类别1）、0（类别2），误分类率同样为 0.25。</p>
<p>误分类率对两种切分方式评价相同，不纯度值的减少分别为 $0.5-1/2<em>0.25-1/2</em>0.25=0.25$ 和 $0.5-6/8<em>2/6-2/8</em>0=0.25$。但对于决策树未来的生长来看，第二种切分方式更为可取，虽然一个节点的误分类率为 $2/6$ ，但另一个节点误分类率为 $0$，这个节点是终端，无需进一步切分。</p>
<p>综上所述，为了解决误分类率作为不纯度函数的缺陷，需要对不纯度函数进行改进。</p>
<p>从二分类问题出发，误分类率不纯度函数为</p>
<script type="math/tex; mode=display">
\begin{aligned}
    \varphi(p_1,p_2)&=1-\max (p_1,p_2)\\
    &=\min (p_1,p_2)\\
    &=\min (p_1,1-p_1)\\
    &={
    \begin{cases}
        p_1&0\le p_1\le 0.5\\
        1-p_1&0.5<p_1\le 1
    \end{cases}
    }
\end{aligned}</script><p>从上述例子可以想到，为了将两种切分方式区分出来，使第二种切分结果得到的评价更高，就要充分奖励更纯的节点。假设 $p_1&gt;0.5$，那么 $\varphi(p_1)=1-p_1$ 是随着 $p_1$ 线性减少的，为了使纯节点的评价更好，需要使 $\varphi(p_1)$ 随着 $p_1$ 的增加而比线性下降更快。也就是当 $p’’_1&gt;p’_1$ 时，$\varphi’(p’’_1)&lt;\varphi’(p’_1)$，因此要求不纯度函数是严格凸函数。如果 $\varphi$ 在 $[0,1]$ 上有连续的二阶导数，则应该满足 $\varphi’’(p_1)&lt;0,0&lt;p_1&lt;1$。</p>
<p>将不纯函数需要满足的条件用公式表示</p>
<ol>
<li>$\varphi(0)=\varphi(1)=0$</li>
<li>$\varphi(p_1)=\varphi(1-p_1)$</li>
<li>$\varphi(1/2)=\text{maximum}$ </li>
<li>$\varphi’’(p_1)&lt;0,0&lt;p_1&lt;1$</li>
</ol>
<p>最简单的满足条件的函数就是二次多项式</p>
<script type="math/tex; mode=display">\varphi(x)=a+bx+cx^2</script><p>由 1 可以得到 $a=0,b+c=0$，因此</p>
<script type="math/tex; mode=display">\varphi(x)=b(x-x^2)</script><p>公式 4 要求 $b&gt;0$，不失一般性，取 $b=1$，因此得到不纯度函数</p>
<script type="math/tex; mode=display">i(t)=p(1|t)p(2|t)</script><p>这就是基尼指数的原型，公式简单且易于计算。一个直观的解释，假设节点 $t$ 中的所有 1 类对象被赋予数值 1，而 2 类对象被赋予数值 0，那么 $p(1|t)$ 和 $p(2|t)$ 是节点中两个类的比例，则节点中数值的样本方差就是 $p(1|t)p(2|t)$。</p>
<blockquote>
<p>信息熵公式 $i(t)=-p(1|t)\log p(1|t)-p(2|t)\log p(2|t)$ 同样满足上述条件。原作者说想不出任何内在的原因为什么同样一个满足条件的函数应该优于任何其他函数，并且测试表明两个函数给出了相似的结果，因此依据简单性原则选择基尼指数。</p>
</blockquote>
<p>将二分类问题扩展到多分类问题中，给定节点 $t$ 的类估计概率 $p(j|t),j=1,\cdots,J$，基尼指数定义为</p>
<script type="math/tex; mode=display">i(t)=\sum_j\sum_{i\ne j}p(j|t)p(i|t)</script><p>同样可以写成</p>
<script type="math/tex; mode=display">i(t)=\sum_jp(j|t)(1-p(j|t))=1-\sum_jp^2(j|t)</script><p>对于二分类问题则有</p>
<script type="math/tex; mode=display">
\begin{aligned}
    i(t)&=2p(1|t)p(2|t)\\
    &=2p(1-p)\\

\end{aligned}</script><blockquote>
<p>这里与上面的不纯度函数不同，具体在于多了 2，基尼指数是在误差分类率上推导出来的，按理说$\displaystyle i(t)=\min_j\sum_{i\ne j}p(j|t)p(i|t)$，但实际上是累加的，基尼指数的最大值是1，而误分类率最大值是0.5，有些不解。如果按下述所说的，那么实际难道是随机选择分类吗？</p>
</blockquote>
<p>基尼指数的一种解释方法是，不是使用多数选择的规则对节点 $t$ 中的对象进行分类，而是从节点中随机选择实例将该实例的类别分配给对象，因此将以概率 $p(i|t)$ 分配为类别 $i$，则误分类的概率就是 $\displaystyle \sum_{i\ne j}p(j|t)p(i|t)=p(j|t)(1-p(j|t))$，然后再对 $j$ 求和。因此</p>
<script type="math/tex; mode=display">i(t)=\sum_j\sum_{i\ne j}p(j|t)p(i|t)</script><p>另一种解释方法如前所说，在节点 $t$ 中，为所有 $j$ 类对象分配值 $1$，为所有其他对象分配值 $0$。那么这些值的样本方差为$p(j|t)(1-p(j|t))$。对所有 $J$ 类重复操作并求和，则结果就是</p>
<script type="math/tex; mode=display">i(t)=\sum_jp(j|t)(1-p(j|t))=1-\sum_jp^2(j|t)</script><p>根据数据特征值是否为指定值 $a$，将节点 $t$ 分为两部分 $t_L$ 和 $t_R$，即</p>
<script type="math/tex; mode=display">t_L=\{(\mathbf{x},y)|\mathbf{x}^{(i)}=a\},t_R=t-t_L</script><p>因此不纯值的减少即为</p>
<script type="math/tex; mode=display">\triangle i(s,t)=i(t)-p_Li(t_L)-p_Ri(t_R)</script><h3 id="3-2-CART-回归"><a href="#3-2-CART-回归" class="headerlink" title="3.2 CART 回归"></a>3.2 CART 回归</h3><p>回归树同样需要解决上面三个问题</p>
<p>训练数据集：</p>
<script type="math/tex; mode=display">T=\{(\mathbf x_1,y_1),(\mathbf x_2,y_2),...,(\mathbf x_N,y_N)\}</script><p>第三个问题最直观也最好解决，假设输出值的函数为 $d$，总的均方误差为</p>
<script type="math/tex; mode=display">R(d)=\frac{1}{N}\sum_n^N(y_n-d(\mathbf x_n))^2</script><p>对于每个叶子节点，假设输入空间为 $t$，根据最小二乘法，为了使 $R(d)$ 最小，输出值应为</p>
<script type="math/tex; mode=display">d_t(\mathbf x_n)=\overline{y}_t=\frac{1}{N_t}\sum_{\mathbf x_n\in t}y_n</script><p><strong>即回归树建立完后，每个叶子节点的输出值为其分配样本的均值。</strong></p>
<p>进一步的，将预测值带入每个节点 $t$，并用 $R(T)$ 取代 $R(d)$</p>
<script type="math/tex; mode=display">R(T)=\frac{1}{N}\sum_{t\in T}\sum_{\mathbf x_n\in t}(y_n-\overline{y}_t)^2</script><p>定义</p>
<script type="math/tex; mode=display">R(t)=\frac{1}{N}\sum_{\mathbf x_n\in t}(y_n-\overline{y}_t)^2</script><blockquote>
<p>注意这里不是 $N_t$ </p>
</blockquote>
<p>因此，$R(T)$ 还可以写为</p>
<script type="math/tex; mode=display">R(T)=\sum_{t\in T}R(t)</script><p>对以上表达式一个简单的解释就是，对于每个节点 $t$，$\displaystyle \sum_{\mathbf x_n\in t}(y_n-\overline{y}_t)^2$ 是节点内的误差平方和，对 $t$ 求和给出所有节点的总平方和，再除以 $N$ 给出平均值。</p>
<p>假定分割前的均方误差为 $R(t)$，分割点为特征向量 $j$，分割值为 $s$，输入空间 $t$ 将被分为两个区域</p>
<script type="math/tex; mode=display">t_L=\{x|x^{(j)}\le s\},t_R=\{x|x^{(j)}\ge s\}</script><p>为了使分割更为有效，将使均方误差减少的尽可能多，因此目标函数为</p>
<script type="math/tex; mode=display">\triangle R(s,t)=R(t)-R(t_L)-R(t_R)</script><p>与 $s$ 相关的变量只有后两项，因此</p>
<script type="math/tex; mode=display">\max \triangle R(s,t)=\min [R(t_L)+R(t_R)]</script><p>令 $p(t)=\frac{N_t}{N}$ 表示随机选择输入变量落入节点 $t$ 的概率，定义节点内方差</p>
<script type="math/tex; mode=display">s^2(t)=\frac{1}{N_t}\sum_{\mathbf x_n\in t}(y_n-\overline{y}_t)^2</script><p>因此可得 $R(t)=s^2(t)p(t)$，并且</p>
<script type="math/tex; mode=display">R(T)=\sum_{t\in T}s^2(t)p(t)</script><p>$t$ 的最佳分割也就是使加权方差最小化</p>
<script type="math/tex; mode=display">\min[p(t_L)s^2(t_L)+p(t_R)s^2(t_R)]</script><p>实际上概率 $p$ 可以带入进去，再乘以 $N$，就得到李航书中的公式</p>
<script type="math/tex; mode=display">\min_{j,s} \left[\sum_{x_i \in t_L} (y_i-\overline{y}_{t_L})^2+ \sum_{x_i \in t_R} (y_i-\overline{y}_{t_R})^2\right]</script><p>依次选取切分变量 $j$，遍历空间中所有输入点确定最优切分值 $s$，找到最优切分变量和切分值使上式最小。</p>
<p>对每个区域依次重复上述过程，直到达到终止条件。与 AID 不同，CART 终止条件为节点中的样本个数小于特定阈值（通常为5），或者节点中样本均为一个类别，即该节点为纯节点。</p>
<p>至此回归的三个问题均已解决，归纳如下</p>
<p><a target="_blank" rel="noopener" href="https://holypython.com/dt/decision-tree-history/">https://holypython.com/dt/decision-tree-history/</a><br><a target="_blank" rel="noopener" href="https://www.explorium.ai/blog/the-complete-guide-to-decision-trees/">https://www.explorium.ai/blog/the-complete-guide-to-decision-trees/</a><br><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/C4.5_algorithm">https://en.wikipedia.org/wiki/C4.5_algorithm</a><br><a target="_blank" rel="noopener" href="https://www.analyticsvidhya.com/blog/2021/05/implement-of-decision-tree-using-chaid/">https://www.analyticsvidhya.com/blog/2021/05/implement-of-decision-tree-using-chaid/</a><br><a target="_blank" rel="noopener" href="https://www.cnblogs.com/fushengweixie/p/8039991.html">https://www.cnblogs.com/fushengweixie/p/8039991.html</a></p>


                            <!-- Meta -->
                            <div class="post-meta">
                                <hr>
                                <br>
                                <div class="post-tags">
                                    
                                                

<a href="/categories/Machine-learning/">Machine learning</a>

                                                    
                                </div>
                                <div class="post-date">
                                    
                                        2021-12-22
                                    
                                </div>
                            </div>
                    </div>

                    <!-- Comments -->
                    <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                        <!-- Disqus Comments -->


                    </div>
                </div>
            </div>
        </article>
</section>

<!-- Image viewer-->

    <!-- Custom picture view-->
    <link href="/css/viewer.min.css" rel="stylesheet" />
    <script
      src="/js/viewer.min.js"
      type="text/javascript"
      charset="utf-8"
    ></script>
    
    <script type="text/javascript">
      // set image viewer
      Viewer.setDefaults({
        zoomRatio: [0.5],
        navbar: false,
        toolbar: false,
        button: false,
        title: [2, (image, imageData) => `${image.alt}`],
        show: function() {
          this.viewer.zoomTo(0.5);
        }
      });
      var imageList = document.getElementsByTagName("img");
      Array.prototype.forEach.call(imageList, element => {
        var viewer = new Viewer(element);
      });
    </script>

    

<!-- TOC -->

    <aside id="article-toc" role="navigation" class="fixed">
        <div id="article-toc-inner">
            <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91"><span class="toc-text">决策树</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E5%8E%86%E5%8F%B2%E8%83%8C%E6%99%AF"><span class="toc-text">1. 历史背景</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E5%86%B3%E7%AD%96%E6%A0%91%E7%BB%93%E6%9E%84"><span class="toc-text">2.决策树结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-CART-%E7%AE%97%E6%B3%95"><span class="toc-text">3. CART 算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-CART-%E5%88%86%E7%B1%BB"><span class="toc-text">3.1 CART 分类</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-CART-%E5%9B%9E%E5%BD%92"><span class="toc-text">3.2 CART 回归</span></a></li></ol></li></ol></li></ol>
        </div>
    </aside>

    <!-- Scripts -->
    <script type="text/javascript">
    console.log("© He1o 2021-" + new Date().getFullYear());
</script>
  
    <!-- Google Analytics -->
    

    <!-- Service Worker -->
    <!-- if using service worker -->

    
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>

</body>

</html>