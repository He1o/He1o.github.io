<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>He1o</title>
  
  
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2022-02-07T01:35:19.954Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>He1o</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title></title>
    <link href="http://example.com/2022/02/07/ProbabilityStatistics/1.random%20events%20and%20their%20probabilities/"/>
    <id>http://example.com/2022/02/07/ProbabilityStatistics/1.random%20events%20and%20their%20probabilities/</id>
    <published>2022-02-07T01:35:19.954Z</published>
    <updated>2022-02-07T01:35:19.954Z</updated>
    
    <content type="html"><![CDATA[<h1 id="随机事件及其概率"><a href="#随机事件及其概率" class="headerlink" title="随机事件及其概率"></a>随机事件及其概率</h1><h2 id="1-随机事件及其运算"><a href="#1-随机事件及其运算" class="headerlink" title="1. 随机事件及其运算"></a>1. 随机事件及其运算</h2><ul><li><strong>随机现象</strong>：在一定条件下，并不总是出现相同结果的现象。</li><li><strong>随机试验</strong>：可重复的随机现象。</li><li><strong>基本结果 $\omega$</strong>：随机现象的最简单的结果，它将是统计中抽样的基本单元，故又称样本点。</li><li><strong>基本空间 $\Omega$</strong>：随机现象所有基本结果的全体。</li><li><strong>随机事件</strong>：随机现象某些基本结果组成的集合，简称“事件”。</li></ul><blockquote><p>抛硬币的结果就是一个随机现象，具有两个基本结果，正面和反面。随机现象所有基本结果的全体就称为这个随机现象的基本空间。在统计学中，基本结果 $\omega$ 是抽样的基本单元，故基本结果又称 <strong>样本点</strong>，基本空间又称 <strong>样本空间</strong> ，抛硬币的基本空间：</p><script type="math/tex; mode=display">\Omega_1=\{\omega_0,\omega_1\}=\{正面，反面\}</script><p>掷一颗骰子，“出现6点”、“出现偶数点”、“出现点数不超过2”都是<strong>随机事件</strong>，可以用其基本空间 $\Omega =\{1,2,3,4,5,6\}$ 的某个子集表示，例如“出现偶数点”可以表示为：</p><script type="math/tex; mode=display">A=\{2,4,6\}</script></blockquote><ul><li><strong>必然事件</strong> </li><li><strong>不可能事件</strong> </li><li><strong>事件的关系</strong><ul><li>事件的包含</li><li>事件的相等</li><li>事件的互不相容</li></ul></li><li><strong>事件的运算</strong> ：事件的基本运算对立、并、交和差，与集合的四种运算余、并、交和差是完全一样的。<ul><li>对立事件：设 $A$ 为一个试验里的事件，则由不在 $A$ 中的一切基本结果组成的事件称为 $A$ 的对立事件，记为 $\overline{A}$。必然事件和不可能事件互为对立事件。</li><li>事件 $A$ 和 $B$ 的并：由事件 $A$ 和 $B$中所有的基本结果组成的一个新事件，记作$A\bigcup B$</li><li>事件 $A$ 和 $B$ 的交：由事件 $A$ 和 $B$中公共的的基本结果组成的一个新事件，记作$A\bigcap B$ 或者 $AB$</li><li>事件 $A$ 对 $B$ 的差：由在事件 $A$ 而不在事件 $B$ 中的基本结果组成的一个新事件，记作$A-B$。$A-B=A-AB=A\overline{B}$</li></ul></li></ul><h2 id="2-事件的概率"><a href="#2-事件的概率" class="headerlink" title="2. 事件的概率"></a>2. 事件的概率</h2><h3 id="2-1-事件的概率"><a href="#2-1-事件的概率" class="headerlink" title="2.1 事件的概率"></a>2.1 事件的概率</h3><p>随机事件的发生是带有偶然性的。但随机事件发生的可能性还是有大小之别的，是可以设法度量的。例如抛硬币正反面的可能性都是$1/2$，用一个 $0$ 到 $1$ 的数来表示一个随机事件发生的可能性大小。概率论中，这种比率就是概率的原形。</p><p><strong>定义1</strong> 在一个随机现象中，用来表示任一个随机事件 $A$ 发生可能性大小的实数（即比率）成为该事件的概率，记为 $P(A)$，并规定</p><ol><li>非负性公理：对任一事件 $A$，必有 $P(A)\ge 0$</li><li>正则性公理：必然事件的概率 $P(\Omega)=1$</li><li>可加性公理：若 $A_1$ 与 $A_2$ 是两个互不相容事件（即$A_1A_2=\emptyset$），则有 $P(A_1\bigcup A_2)=P(A_1)+P(A_2)$</li></ol><p>这就是概率的公理化定义，它虽然刻划了概率的本质，但没有告诉人们如何去确定概率。历史上概率的古典定义、概率的统计定义和概率的主观定义都有各自确定概率的方法，在有了公理化定义之后，把它们看作确定概率的三种方法是很恰当的。</p><h3 id="2-2-排列与组合"><a href="#2-2-排列与组合" class="headerlink" title="2.2 排列与组合"></a>2.2 排列与组合</h3><p>排列和组合是两类计数公式，基于如下两条计数原理：</p><ul><li><strong>乘法原理</strong></li><li><strong>加法原理</strong></li></ul><p>乘法原理，某件事需要 $k$ 个步骤，则每个步骤能选择的方法数量相乘就是总的可选择方法。加法原理，就是某件事由 $k$ 类方法可以完成，那么总的可选择方法就是这几类方法相加。</p><ul><li><strong>排列</strong>　从 $n$ 个不同元素中任取 $r(r\le n)$ 个元素排成一列称为一个排列，按乘法原理，此种排列共有 $n\times (n-1)\times \cdots \times(n-r+1)$ 个，记为 $P^r_n$。若 $r=n$，称为全排列，全排列数共有 $n!$ 个，记为 $P_n$。</li><li><strong>重复排列</strong>　从 $n$ 个不同元素中每次取出一个，放回后再取下一个，如此连续取 $r$ 次所得的排列称为重复排列，此种重复排列数共有 $n^r$ 个。这里 $r$ 允许大于 $n$。</li><li><strong>组合</strong>　从 $n$ 个不同元素中任取 $r(r\le n)$ 个元素并成一组（不考虑其间顺序）称为一个组合，按乘法原理，此种组合总数为<script type="math/tex; mode=display">\dbinom{n}{r}=\frac{P_n^r}{r!}=\frac{n\times (n-1)\times \cdots \times(n-r+1)}{r!}=\frac{n!}{r!(n-r)!}</script>并规定 $0!=1$ 和 $\dbinom{n}{r}=1$，同时 $\dbinom{n}{r}=1$ 还是二项式展开式的系数，即<script type="math/tex; mode=display">(a+b)^n=\sum^n_{r=0}\dbinom{n}{r}a^rb^{n-r}</script>若在上式中令 $a=b=1$ ，可得一重要组合恒等式：<script type="math/tex; mode=display">\dbinom{n}{0}+\dbinom{n}{1}+\cdots+\dbinom{n}{n}=2^n</script></li><li><strong>重复组合</strong>　从 $n$ 个不同元素每次取出一个，放回后再取下一个，如此连续取 $r$ 次所得的组合称为重复组合。重复组合总数为 $\dbinom{n+r-1}{r}$。</li></ul><h3 id="2-3-古典方法"><a href="#2-3-古典方法" class="headerlink" title="2.3 古典方法"></a>2.3 古典方法</h3><p>古典方法是在经验事实的基础上对被考察事件发生可能性进行符合逻辑分析后得出该事件的概率。这种方法简单、直观、不需要做试验，但只能在一类特定的随机现象中使用。基本思想如下：</p><ul><li>所涉及的随机现象只有有限个基本结果</li><li>每个基本结果出现的可能性是相同的（等可能性）</li><li>假如被考察的事件 $A$ 含有 $k$ 个基本结果，则事件 $A$ 的概率就是<script type="math/tex; mode=display">P(A)=\frac{k}{n}=\frac{A中含基本结果的个数}{\Omega中基本结果总数}</script></li></ul><p>下面以扑克游戏为例，计算每种牌型的概率。一副标准的扑克牌由52张组成，有两种颜色、四种花式和13种牌型，假设每张牌被抽出的可能性是相同的。下面来研究一些事件的概率。</p><ol><li>事件 $A=$ “抽五张牌，恰好是同花顺”。在抽五张牌的试验中，共有 $\dbinom{52}{5}$ 个等可能基本结果，事件 $A$ 包含的基本结果总数为 $10\times 4$。故事件 $A=$ 的概率为<script type="math/tex; mode=display">P(A)=\frac{10\times 4}{\dbinom{52}{5}}=\frac{40}{2598960}=0.00001539</script> 发生概率10万分之1.5</li><li>事件 $B=$ “抽五张牌，四张牌型一样”。事件 $B$ 包含的基本结果总数为 $13\times 48$。故事件 $B=$ 的概率为<script type="math/tex; mode=display">P(B)=\frac{624}{\dbinom{52}{5}}=\frac{624}{2598960}=0.00024</script> 发生的概率是万分之二，平均4167次才会发生一次。</li><li>事件 $C=$ “抽五张牌，恰好是葫芦”。事件 $C$ 包含的基本结果总数为 $13\times \dbinom{4}{3}\times 12 \times\dbinom{4}{2}=3744$<script type="math/tex; mode=display">P(C)=\frac{3744}{\dbinom{52}{5}}=\frac{3744}{2598960}=0.00144=\frac{1}{694}</script></li><li>事件 $D=$ “抽五张牌，恰好是同花，但不是顺子”。事件 $D$ 包含的基本结果总数为 $\dbinom{13}{5}\times 4 - 40=5108$<script type="math/tex; mode=display">P(D)=\frac{5108}{\dbinom{52}{5}}=\frac{5108}{2598960}=0.0019654=\frac{1}{508}</script></li><li>事件 $E=$ “抽五张牌，恰好是顺子，但不是同花”。一共有十种顺子，每种顺子按照花色的不同共有 $4^5-4$ 种结果。因此，事件 $E$ 包含的基本结果总数为 $10\times (4^5-4)=10200$<script type="math/tex; mode=display">P(E)=\frac{10200}{\dbinom{52}{5}}=\frac{10200}{2598960}=0.003924=\frac{1}{254}</script></li><li>事件 $F=$ “抽五张牌，恰好是三条，但不是葫芦”。事件 $F$ 包含的基本结果总数为 $13\times \dbinom{4}{3}\times \dbinom{48}{2} - 3744=54912$<script type="math/tex; mode=display">P(F)=\frac{54912}{\dbinom{52}{5}}=\frac{54912}{2598960}=0.021128=\frac{1}{47}</script></li><li>事件 $G=$ “抽五张牌，恰好是两对”。事件 $F$ 包含的基本结果总数为 $\dbinom{4}{2}\times \dbinom{4}{2}\times \dbinom{13}{2}\times \dbinom{11}{1}\times 4=123552$<script type="math/tex; mode=display">P(F)=\frac{54912}{\dbinom{52}{5}}=\frac{54912}{2598960}=0.04753=\frac{1}{21}</script></li><li>事件 $H=$ “抽五张牌，恰好有一对”。事件 $F$ 包含的基本结果总数为 $\dbinom{4}{2}\times \dbinom{13}{1}\times \dbinom{12}{3}\times 4^3=1098240$<script type="math/tex; mode=display">P(F)=\frac{1098240}{\dbinom{52}{5}}=\frac{1098240}{2598960}=0.42256=\frac{1}{2}</script></li></ol><h3 id="2-3-频率方法"><a href="#2-3-频率方法" class="headerlink" title="2.3 频率方法"></a>2.3 频率方法</h3><p>频率方法是在大量重复试验中用频率去获取概率近似值的一个方法。它是最常用，也是最基本获得概率的方法。基本思想如下：</p><ul><li>与考察事件 $A$ 有关的随机现象是允许进行大量重复试验的。</li><li>假如在 $N$ 次重复试验中，事件 $A$ 发生 $K_N$ 次，则事件 $A$ 发生的频率为<script type="math/tex; mode=display">P^*_N(A)=\frac{K_N}{N}=\frac{事件A发生的次数}{重复试验次数}</script></li><li>频率 $P^<em>_N(A)$ 依赖于重复次数 $N$。随着重复次数 $N$ 的增加，频率 $P^</em>_N(A)$ 会稳定在某一常数附近，这个频率的稳定值已与 $N$ 无关，就是事件 $A$ 发生的概率。在现实世界中，我们无法将一个试验无限次的重复下去，在重复次数 $N$ 较大时，频率 $P^*_N(A)$ 就很接近概率 $P(A)$ 。在统计学中把频率称为概率的估计值，实际频率当作概率近似值使用。</li></ul><h3 id="2-4-主观方法"><a href="#2-4-主观方法" class="headerlink" title="2.4 主观方法"></a>2.4 主观方法</h3><p>统计学中有一个贝叶斯学派，他们在研究随机现象之后认为，<strong>一个事件的概率是人们根据经验对该事件发生可能性所给出的个人信念</strong>。这样给出的概率就称为主观概率。</p><p>以经验为基础的主观概率和纯主观还是不同的。主观概率要求当事人对所考察的事件有较为透彻的了解和经验，并能对周围信息和历史信息进行仔细分析，在这个基础上确定的主观概率就能符合实际。</p><p>主观概率在遇到随机现象不能大量重复、无法用频率方法确定事件概率时就变得极为有用，因此在经济领域和决策分析中使用较为广泛。在某种意义上看，主观概率至少是频率方法和古典方法的一种补充，有了主观概率至少使人们在频率观点不适用时也能谈论概率，使用概率与统计方法。</p><h2 id="3-概率的性质"><a href="#3-概率的性质" class="headerlink" title="3. 概率的性质"></a>3. 概率的性质</h2><p>一些概率的常用性质</p><ul><li>定理1　$P(\overline{A})=1-P(A)$</li><li>定理2　$P(\phi)=0$</li><li>定理3　对 $n$ 个互不相容事件 $A_1,\cdots,A_n$，有<script type="math/tex; mode=display">P(\bigcap\limits^n_{i=1} A_i)=\sum^n_{i=1}P(A_i)</script></li><li>定理4　对任意两个事件 $A$ 和 $B$，若$A\supset B$，则<ul><li>$P(A-B)=P(A)-P(B)$</li><li>$P(A)\le P(B)$</li></ul></li><li>定理5　对任一事件 $A$，总有 $0\le P(A)\le 1$</li><li>定理6　对任意两个事件 $A$ 与 $B$，有<ul><li>$P(A\bigcup B)=P(A)+P(B)-P(AB)$</li><li>$P(A\bigcup B)\le P(A)+P(B)$</li></ul></li><li>定理7　对任意三个事件 $A,B,C$，有<ul><li>$P(A\bigcup B \bigcup C)=P(A)+P(B)+P(C)-P(AB)-P(AC)-P(BC)+P(ABC)$</li><li>$P(A\bigcup B \bigcup C)\le P(A)+P(B)+P(C)$</li></ul></li></ul><h2 id="4-独立性"><a href="#4-独立性" class="headerlink" title="4. 独立性"></a>4. 独立性</h2><h3 id="4-1-事件的独立性"><a href="#4-1-事件的独立性" class="headerlink" title="4.1 事件的独立性"></a>4.1 事件的独立性</h3><ul><li>定义1　对任意两个事件 $A$ 和 $B$，若有 $P(AB)=P(A)P(B)$，则称事件 $A$ 与 $B$ 相互独立，简称 $A$ 与 $B$ 独立。否则称事件 $A$ 与 $B$ 不独立。</li></ul><p>两个事件的独立性是指一个事件的发生不影响另一个事件的发生。在概率角度讲，两个事件之间的独立性与这两个事件同时发生的概率有密切关系，即两独立事件同时发生的概率等于它们各自概率的乘积。</p><p>投掷两个骰子，两个骰子的点数是互不影响的，显然任何两个分别与两个骰子相关的事件都是独立的，例如事件 $A=$ “第一颗骰子出现1点”、事件 $B=$ “第二颗骰子出现偶数点”。</p><p>但是，两事件是否具有独立性并不总是显然的。例如同样两个事件 $A=$ “家中男女孩都有”、事件 $B=$ “家里至多一个女孩”。在家中有三个小孩的情况下，这两个事件是独立的。但是，家中有两个或四个小孩时，就不再独立了。</p><ul><li>定理1　若事件 $A$ 和 $B$独立，则事件 $A$ 和 $\overline{B}$ 独立；$\overline A$ 和 $B$ 独立；$\overline A$ 和 $\overline{B}$ 独立。</li></ul><p>证：由事件的运算性质知</p><script type="math/tex; mode=display">A\overline{B}=A(1-B)=A-AB</script><p>其中 $A\supset AB$，再由 $A$ 和 $B$ 的独立性</p><script type="math/tex; mode=display">\begin{aligned}    P(A\overline{B})&= P(A)-P(AB)\\    &= P(A)-P(A)P(B)\\    &= P(A)[1-P(B)]\\    &= P(A)P(\overline{B})\\\end{aligned}</script><h3 id="4-1-试验的独立性"><a href="#4-1-试验的独立性" class="headerlink" title="4.1 试验的独立性"></a>4.1 试验的独立性</h3><p>利用事件的独立性可以定义两个或更多个试验的独立性。假设有两个试验 $E_1$ 和 $E_2$，假如试验 $E_1$ 的任一个结果与试验 $E_2$ 的任一个结果都是相互独立事件，则称这两个试验相互独立。类似的如果 $n$ 个试验相互间的任一结果之间都是相互独立的事件，则这些试验都相互独立。如果这 $n$ 个试验还是相同的，则称其为 $n$ 重独立重复试验。</p><p>$n$ 重伯努利试验是一类常见的随机模型。</p><p><strong>伯努利试验</strong>　只有两个结果的实验就称为伯努利试验，例如抛硬币（正面和反面）、检查一个产品（合格与不合格）等等。在一次伯努利试验中，设成功的概率为 $p$，即</p><script type="math/tex; mode=display">P(A)=p,P(\overline{A})=1-p</script><p><strong>$n$ 重伯努利试验</strong>　由 $n$ 个（次）相同的、独立的伯努利试验组成的随机试验称为 $n$ 重伯努利试验。伯努利试验基本结果可用长为 $n$ 的序列表示，例如$AA\overline{A}AA$ 表示第三次失败，其余四次成功。在 $n$ 重伯努利试验中，在人们最关心的是成功次数。因为成功次数是基本结果中所含的最重要信息，而 $A$ 与 $\overline{A}$ 的排列次序在实际中往往是不感兴趣的信息。记</p><script type="math/tex; mode=display">B_{n,k}=“n重伯努利试验中A出现k次”</script><p>一般概率公式为</p><script type="math/tex; mode=display">P(B_{n,k})=\dbinom{n}{k}p^k(1-p)^{n-k}</script><h2 id="5-条件概率"><a href="#5-条件概率" class="headerlink" title="5. 条件概率"></a>5. 条件概率</h2><h3 id="5-1-条件概率"><a href="#5-1-条件概率" class="headerlink" title="5.1 条件概率"></a>5.1 条件概率</h3><p>假设苹果有两个属性，一个是好看，一个是好吃。现对25个苹果进行统计，得到结果如下</p><div class="table-container"><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">$B:$好看</th><th style="text-align:center">$\overline{B}:$不好看</th><th style="text-align:center"></th></tr></thead><tbody><tr><td style="text-align:center">$A:$好吃</td><td style="text-align:center">10</td><td style="text-align:center">5</td><td style="text-align:center">15</td></tr><tr><td style="text-align:center">$\overline{A}:$不好吃</td><td style="text-align:center">8</td><td style="text-align:center">2</td><td style="text-align:center">10</td></tr><tr><td style="text-align:center"></td><td style="text-align:center">18</td><td style="text-align:center">7</td><td style="text-align:center">25</td></tr></tbody></table></div><p>思考问题，当事件 $B$ 发生后，事件 $A$ 再发生的概率是多少？由于事件 $B$ 发生了，给人们带来新的信息，即 $\overline{B}$ 不可能再发生了，所有可能发生的基本结果局限在 $B$ 的 $18$ 个结果，这时事件 $A$ 所包含的基本结果在 $\Omega_B$ 中所占的比率为 $10/18$，这就是在事件 $B$ 已经发生的情况下，事件 $A$ 的条件概率，即 $P(A|B)=10/18$。</p><p>条件概率 $P(A|B)=10/18$ 中分母是事件 $B$ 中的基本结果数，记为 $N(B)=18$。分子是事件 $A$ 和 $B$ 同时发生的基本结果数，即交事件 $AB$，记为记为 $N(AB)=10$。分子分母同时除以原基本空间 $\Omega$ 中的基本结果数 $N(\Omega)=25$，得到如下关系式</p><script type="math/tex; mode=display">P(A|B)=\frac{N(AB)}{N(B)}=\frac{N(AB)/N(\Omega)}{N(B)/N(\Omega)}=\frac{P(AB)}{P(B)}</script><p>这表明条件概率可用两个特定的无条件概率之商表示，我们可以进一步得到条件概率的一般定义。</p><p><strong>定义1</strong>　设 $A$ 与 $B$ 是基本空间 $\Omega$ 中的两个事件，且$P(B)&gt;0$，在事件 $B$ 已发生的条件下，事件 $A$ 的条件概率 $P(A|B)$ 定义为 $P(AB)/P(B)$，即</p><script type="math/tex; mode=display">P(A|B)=\frac{P(AB)}{P(B)}</script><p>其中 $P(A|B)$ 也称为给定事件 $B$ 下事件 $A$ 的条件概率。</p><p>条件概率满足概率的三条公理，非负、正则、可加。当 $B=\Omega$ 时，条件概率转化为无条件概率，因此把无条件概率看作特殊场合下的条件概率也未尝不可。</p><p>除此之外，条件概率还有一些特殊性质。</p><p><strong>定理1(乘法公式)</strong>　对任意两个事件 $A$ 和 $B$，有</p><script type="math/tex; mode=display">P(AB)=P(A|B)P(B)=P(B|A)P(A)</script><p>对两个等式，分别要求 $P(B)&gt;0$ 和  $P(A)&gt;0$。这个定理表明任意两个事件交的概率等于一事件的概率乘以在这时间已发生的条件下另一事件的条件概率，只要它们的概率都不为零。</p><p><strong>定理2</strong>　假如事件 $A$ 与 $B$ 独立，且 $P(B)&gt;0$，则 $P(A|B)=P(A)$，反之亦然。<br><br>这个性质表明，若两事件独立，则其条件概率就等于其概率，这里事件 $B$ 的发生对事件 $A$ 是否发生没有任何影响，反之，若有 $P(A|B)=P(A)$，则有乘法公式可以得出 $P(AB)=P(A)P(B)$，故 $A$ 与 $B$ 独立。</p><p><strong>定理3(一般乘法公式)</strong>　对任意三个事件 $A_1$、$A_2$、$A_3$，有</p><script type="math/tex; mode=display">P(A_1A_2A_3)=P(A_1)P(A_2|A_1)P(A_3|A_1A_2)</script><p>其中 $P(A_1A_2)&gt;0$。</p><h3 id="5-2-全概率公式"><a href="#5-2-全概率公式" class="headerlink" title="5.2 全概率公式"></a>5.2 全概率公式</h3><p>全概率公式是概率论中的一个基本公式。它可以使一个复杂事件的概率计算化繁就简，得以解决。</p><p><strong>定理4</strong>　设 $A$ 与 $B$ 是任意两个事件，假如 $0&lt;P(B)&lt;1$，则</p><script type="math/tex; mode=display">P(A)=P(A|B)P(B)+P(A|\overline{B})P(\overline{B})</script><p>证：由 $B\bigcup \overline{B}=\Omega$ 和事件运算性质可知</p><script type="math/tex; mode=display">A=A\Omega=A(B\bigcup \overline{B})=AB\bigcup A\overline{B}</script><p>显然 $AB$ 与 $A\overline{B}$ 是互不相容事件，由加法公式和乘法公式得</p><script type="math/tex; mode=display">\begin{aligned}    P(A)&= P(AB)+P(A\overline{B})\\    &= P(A|B)P(B)+P(A|\overline{B})P(\overline{B})\\\end{aligned}</script><p>由于 $P(B)$ 不为0与1，所有 $P(\overline{B})&gt;0$，从而上述两个条件概率 $P(A|B)$ 与 $P(A|\overline{B})$ 都是有意义的。</p><p>将上述全概率公式推广到一般形式，需要一个“分割”的概念。简而言之，两个互为对立的事件组成全集，当多个事件彼此之间没有交集且组成全集的时候，它们就是一个分割。</p><p><strong>定义2</strong>　把基本空间 $\Omega$ 分成 $n$ 个事件 $B_1,B_2,\cdots,B_n$，假如满足如下条件</p><ul><li>$P(B_i)&gt;0,i=1,2,\cdots,n$</li><li>$B_1,B_2,\cdots,B_n$ 互不相容</li><li>$\bigcap\limits^n_{i=1} B_i=\Omega$<br>则称事件组 $B_1,B_2,\cdots,B_n$ 为基本空间 $\Omega$ 的一个分割。</li></ul><p><strong>定理5</strong>　 设 $B_1,B_2,\cdots,B_n$ 是基本空间 $\Omega$ 的一个分割，则对 $\Omega$ 中任一事件 $A$，有</p><script type="math/tex; mode=display">P(A)=\sum_{i=1}^nP(A|B_i)P(B_i)</script><p>由全概率公式可以推出一个很著名的公式。</p><p><strong>定理6(贝叶斯公式)</strong>　设 $B_1,B_2,\cdots,B_n$ 是基本空间 $\Omega$ 的一个分割，且它们各自概率 $P(B_1),P(B_2),\cdots,P(B_n)$ 皆已知且为正，又设 $A$ 是 $\Omega$ 中的一个事件，$P(A)&gt;0$，且在诸 $B_i$ 下事件 $A$ 的条件概率 $P(A|B_1),P(A|B_2),\cdots,P(A|B_n)$ 可通过试验等手段获得，则再 $A$ 给定下，事件 $B_k$ 的条件概率为</p><script type="math/tex; mode=display">P(B_k|A)=\frac{P(A|B_k)P(B_k)}{\sum\limits^n_{i=1}P(A|B_i)P(B_i)},k=1,2,\cdots,n</script>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;随机事件及其概率&quot;&gt;&lt;a href=&quot;#随机事件及其概率&quot; class=&quot;headerlink&quot; title=&quot;随机事件及其概率&quot;&gt;&lt;/a&gt;随机事件及其概率&lt;/h1&gt;&lt;h2 id=&quot;1-随机事件及其运算&quot;&gt;&lt;a href=&quot;#1-随机事件及其运算&quot; class=&quot;</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://example.com/2022/01/12/ProbabilityStatistics/4.statistics%20and%20their%20disribution/"/>
    <id>http://example.com/2022/01/12/ProbabilityStatistics/4.statistics%20and%20their%20disribution/</id>
    <published>2022-01-12T00:33:57.364Z</published>
    <updated>2022-01-12T00:33:57.365Z</updated>
    
    <content type="html"><![CDATA[<h1 id="参数估计"><a href="#参数估计" class="headerlink" title="参数估计"></a>参数估计</h1><p>参数估计不仅指总体分布 $F(x;\theta)$ 中所含的参数 $\theta$，还指分布的各种特征数，如均值、方差、标准差、相关矩，也包括各种事件的概率等。对这些参数要精确确定是困难的，只能通过样本所提供的的信息对其作出某种估计。</p><p>在本章中，设 $\theta$ 是总体的一个待估参数，$\theta$ 的一切可能取值构成的参数空间记为 $\Theta$。$X_1,X_2,\cdots,X_n$ 为从总体中抽取了一个容量为 $n$ 的样本，其观测值记为 $x_1,x_2,\cdots,x_n$</p><p>参数估计的形式主要有两种：点估计与区间估计。</p><p>参数的点估计，是构造一个统计量 $\hat{\theta}=\hat{\theta}(X_1,X_2,\cdots,X_n)$，然后用 $\hat{\theta}$ 去估计 $\theta$，称 $\hat{\theta}$ 为 $\theta$ 的点估计或估计量，简称估计，将样本观测值带入后便得到了 $\theta$ 的一个点估计值 $\hat{\theta}(x_1,x_2,\cdots,x_n)$。</p><p>参数的区间估计，则是构造两个统计量 $\hat{\theta}_L$ 与 $\hat{\theta}_U$，且$\hat{\theta}_L&lt;\hat{\theta}_U$，然后以区间$[\hat{\theta}_L,\hat{\theta}_U]$ 的形式给出未知参数 $\theta$ 的估计，事件“区间$[\hat{\theta}_L,\hat{\theta}_U]$含有$\theta$” 的概率称为置信水平。</p><p>点估计的方法有两种矩法估计与极大似然估计。</p><h2 id="1-矩法估计"><a href="#1-矩法估计" class="headerlink" title="1. 矩法估计"></a>1. 矩法估计</h2><p>1900年英国统计学家 K.Pearson 提出了一个替换原则，用样本矩去替换总体矩，后来人们就称此为矩法估计。</p><p>矩法估计的基本点是：用样本矩估计总体矩，用样本矩的相应函数估计总体矩的函数。</p><p>设 $X_1,X_2,\cdots,X_n$ 是来自某总体 $X$ 的一个样本，则样本的 $k$ 阶原点矩为：</p><script type="math/tex; mode=display">A_k=\frac{1}{n}\sum^n_{i=1}X_i^k,k=1,2,\cdots</script><p>如果总体 $X$ 的 $k$ 阶原点矩 $\mu_k=E(X^k)$ 存在，则用 $A_k$ 去估计 $\mu_k$，记为</p><script type="math/tex; mode=display">\hat{\mu_k}=A_k</script>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;参数估计&quot;&gt;&lt;a href=&quot;#参数估计&quot; class=&quot;headerlink&quot; title=&quot;参数估计&quot;&gt;&lt;/a&gt;参数估计&lt;/h1&gt;&lt;p&gt;参数估计不仅指总体分布 $F(x;\theta)$ 中所含的参数 $\theta$，还指分布的各种特征数，如均值、方差、标准</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://example.com/2022/01/12/ProbabilityStatistics/2.random%20variables%20and%20its%20probability%20distributions/"/>
    <id>http://example.com/2022/01/12/ProbabilityStatistics/2.random%20variables%20and%20its%20probability%20distributions/</id>
    <published>2022-01-12T00:33:57.362Z</published>
    <updated>2022-01-12T00:33:57.363Z</updated>
    
    <content type="html"><![CDATA[<h1 id="随机变量及其概率分布"><a href="#随机变量及其概率分布" class="headerlink" title="随机变量及其概率分布"></a>随机变量及其概率分布</h1><h2 id="1-随机变量"><a href="#1-随机变量" class="headerlink" title="1. 随机变量"></a>1. 随机变量</h2><h3 id="1-1-随机变量"><a href="#1-1-随机变量" class="headerlink" title="1.1 随机变量"></a>1.1 随机变量</h3><p>人们对随机现象的兴趣常常集中在其结果的某个数量方面。例如，再检查产品的过程中关心的是不合格产品的个数。若记20个产品中不合格的个数为 $X$ ，则这个 $X$ 可能取 $0,1,2,\cdots,20$ 中任意一个数，但最终是哪一个数要看检验结果，事先不能确定，因此 $X$ 的取值带有随机性，这样的变量就称为随机变量。有了随机变量之后，有关事件的表示也方便了。如 $X=2$ 表示“20个产品中有两个不合格产品” 这一事件。</p><blockquote><p>随机变量与随机事件的关系也就类似于变量与常量的关系，所以前文中随机事件都是用 $A,B$ 之类的常量字母表示。直观来说，随机变量就是“随机取值的变量”，或者“取值随机会而定的变量”。</p></blockquote><p><strong>定义1</strong>　假如一个变量在数轴上的取值依赖于随机现象的基本结果，则称此变量为<strong>随机变量</strong>，常用大写字母 $X,Y,Z$ 等表示，其取值用小写字母 $x,y,z$ 等表示。假如一个随机变量仅去数轴上的有限个或可列个孤立点，则称此随机变量为<strong>离散随机变量</strong>。假如一个随机变量的可能取值充满数轴上的一个区间 $(a,b)$ ，则称此随机变量为<strong>连续随机变量</strong>，其中 $a$ 可以是 $-\infty$， $b$ 可以是 $+\infty$ 。</p><h3 id="1-2-随机变量的概率分布"><a href="#1-2-随机变量的概率分布" class="headerlink" title="1.2 随机变量的概率分布"></a>1.2 随机变量的概率分布</h3><p>在定义中有随机变量 $X$ 的“取值依赖于基本结果”的说法，意味着随机变量 $X$ 是基本结果 $\omega$ 的函数，即可以把 $X$ 即为 $X(\omega)$，</p><script type="math/tex; mode=display">X=X(\omega),\omega \in \Omega</script><p>对随机变量 $X$ 来说，不容许基本空间 $\Omega$ 中有一个基本结果 $\omega$ 没有实数与其对应，综上所述，随机变量也可看作定义在基本空间 $\Omega$ 的实值函数。因此，“随机变量 $X$ 的取值为 $x$” 就是满足等式 $X(\omega)=x$ 的一切 $\omega$ 组成的集合，用”$X=x$“表示，即</p><script type="math/tex; mode=display">“X=x”=\{\omega:X(w)=x\}\subset \Omega</script><p>类似的，“随机变量 $X$ 的取值小于或等于 $x$”就是满足不等式 $X(\omega)\le x$ 的一切 $\omega$ 组成的集合，用“$X\le x$”表示，即</p><script type="math/tex; mode=display">“X\le x”=\{\omega:X(w)\le x\}\subset \Omega</script><blockquote><p>当一个函数等于一个值的时候其实它就是一个方程了，因此上述就类似于 $f(x)=a$。</p></blockquote><p>上述两种形式的事件是典型事件。例如，要确定一个离散随机变量 $X$ 取值的概率，只要对其可能取值 $x_i$ 确定形如 “$X=x_i$” 事件的概率即可。而对一般随机变量 $X$ ，要确定它取值的概率，就要对任意实数 $x$ ，确定形如 “$X\le x$” 的事件概率，这类事件的概率 $P(X\le x)$ 是 $x$ 的函数，随 $x$ 变化而变化，将这个函数记为 $F(x)$ ，即为分布函数。</p><p><strong>定义2</strong>　设 $X$ 为一个随机变量，对任意实数 $x$ ，事件 $X\le x$ 的概率是 $x$ 的函数，记为</p><script type="math/tex; mode=display">F(x)=P(X\le x)</script><p>这个函数称为 $X$ 的累积概率分布函数，简称分布函数。</p><p>从分布函数定义可以得到它的一些基本性质。</p><ol><li>$0\le F(x)\le 1$。分布函数值是特定形式事件 “$X\le x$” 的概率，而概率总是在0和1之间。</li><li>$F(-\infty)=\lim\limits_{x\rightarrow -\infty}F(x)=0$。这是因为事件 “$X\le -\infty$” 是不可能事件。</li><li>$F(+\infty)=\lim\limits_{x\rightarrow +\infty}F(x)=1$。这是因为事件 “$X\le +\infty$” 是必然事件。</li><li>$F(x)$ 是非降函数，即对任意 $x_1&lt;x_2$， $F(x_1)\le F(x_2)$。这是因为事件“$X\le x_2$” 包含事件 “$X\le x_1$”。</li><li>$F(x)$ 是右连续函数，即 $F(x)=F(x+0)$，其中 $F(x+0)$ 是函数在 $x$ 处的右极限，对任意给定的 $x$，取一个下降数列 $\{x_n\}$，使其极限为 $x$ ，即<script type="math/tex; mode=display">x_1>x_2>\cdots>x_n>\cdots\rightarrow x</script></li></ol><script type="math/tex; mode=display">F(x+0)=\lim\limits_{x_n\rightarrow x}F(x)</script><p><strong>可列可加性公理</strong>　若 $A_1,A_2,\cdots$ 是一列互不相容事件，则有</p><script type="math/tex; mode=display">P(\bigcup\limits _{n=1}^\infty A_n)=\sum _{n=1}^\infty P(A_n)</script><p>此公理与非负性公理、正则性公理一起组成新的公理体系，它使可列个事件经运算后所得事件可谈及概率。</p><h2 id="2-离散随机变量"><a href="#2-离散随机变量" class="headerlink" title="2. 离散随机变量"></a>2. 离散随机变量</h2><p><strong>定义3</strong>　设 $X$ 是离散随机变量，它的所有可能取值时 $x_1,x_2,\cdots,x_n,\cdots$，假如 $X$ 取 $x_i$ 的概率为</p><script type="math/tex; mode=display">P(X=x_i)=p(x_i)\ge 0,i=1,2,\cdots,n,\cdots</script><p>且满足如下条件：</p><script type="math/tex; mode=display">\sum_{i=1}^\infty p(x_i)=1</script><p>则称这组概率 $\{p(x_i)\}$ 为该随机变量 $X$ 的<strong>分布列</strong>，或 $X$ 的<strong>概率分布</strong>，记为 $X\sim\{p(x_i) \}$，读成随机变量 $X$ 服从分布 $\{p(x_i)\}$ 。<br>若已知离散随机变量 $X$ 的分布列为 $\{p(x_i)\}$ ，容易写出 $X$ 的分布函数</p><script type="math/tex; mode=display">F(x)=\sum_{x_i\le x}p(x_i)</script><p>但在离散随机变量场合，使用分布列更为方便，故常用分布列表示离散随机变量的概率分布，非必要不使用分布函数。</p><p>分布列还有两种图表示方法：线条图与概率直方图。</p><h3 id="2-1-离散随机变量的数学期望"><a href="#2-1-离散随机变量的数学期望" class="headerlink" title="2.1 离散随机变量的数学期望"></a>2.1 离散随机变量的数学期望</h3><p>“期望”在日常生活中常指有根据的希望，或发生可能性较大的希望，例如一台冰箱期望的使用寿命是10年，并不是指到10年的时间就一定坏，而是或早或晚都有可能。“期望”就是指用概率分布算得的一种加权平均。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;随机变量及其概率分布&quot;&gt;&lt;a href=&quot;#随机变量及其概率分布&quot; class=&quot;headerlink&quot; title=&quot;随机变量及其概率分布&quot;&gt;&lt;/a&gt;随机变量及其概率分布&lt;/h1&gt;&lt;h2 id=&quot;1-随机变量&quot;&gt;&lt;a href=&quot;#1-随机变量&quot; class=&quot;</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://example.com/2022/01/12/ComputerScience/1.information%20layer/"/>
    <id>http://example.com/2022/01/12/ComputerScience/1.information%20layer/</id>
    <published>2022-01-12T00:33:57.350Z</published>
    <updated>2022-01-12T00:33:57.350Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-信息层-数据存储"><a href="#1-信息层-数据存储" class="headerlink" title="1 信息层-数据存储"></a>1 信息层-数据存储</h1><h2 id="1-1-二进制和记数系统"><a href="#1-1-二进制和记数系统" class="headerlink" title="1.1 二进制和记数系统"></a>1.1 二进制和记数系统</h2><p>基数（base）：记数系统的基本数值，规定了这个系统中使用的数字量和数位位置的值。</p><p>位置记数法（positional notation）：一种表达数字的系统，数位按顺序排列，每个数位又一个位置，数字的值是每个数位和位值的乘积之和，即用记数系统的基数的多项式表示值。</p><p>不同进制之间的相互转换</p><ul><li>二进制数字（binary digit）：二进制记数系统中的一位数字，可以是0或1，在计算机中每个存储位为高电平和低电平两种信号，每个存储单元即为一个二进制数字。</li><li>位（bit）：二进制数字的简称。</li><li>字节（byte）：8个二进制位。</li><li>字（word）：一个或多个字节，字中的位数称为计算机的字长</li></ul><h2 id="1-2-数据形式"><a href="#1-2-数据形式" class="headerlink" title="1.2  数据形式"></a>1.2  数据形式</h2><p>计算机可以处理各种各样的信息，可以存储、表示和帮助我们修改各种类型的数据</p><ul><li>数据（data）：基本值或事实，未组织过的，缺少context，</li><li>信息（information）：用有效的方式组织或处理过的数据，可以帮助我们回答问题。</li><li>多媒体（multimedia）：集中不同的媒体类型。</li><li>数据压缩（data compression）：减少存储一段数据所需的空间。</li><li>带宽（bandwidth）：在固定时间内从一个地点传输到另一个地点的最大位数或字节数。</li><li>压缩率（compression ratio）：压缩后的数据大小除以原始数据大小的值。</li><li>无损压缩（lossless compression）：不会丢失信息的数据压缩技术。</li><li>有损压缩（lossy compression）：会丢失信息的数据压缩技术。</li></ul><p>表示数据的方法有两种，模拟法和数字法。模拟数据是一种连续表示法，模拟它表示的真实信息，数字数据是一种离散表示法，把信息分割成独立的元素。模拟数据完全对应于我们周围无限连续的世界。因此，计算机不能很好的处理模拟数据，需要数字化数据，即把信息分割成片段并单独表示每个片段。<br>数字信号只能在两个极端之间跳跃，称为脉冲编码调制（PCM）。数字信号在信息丢失之前会降级相当多，因为大于某个阈值的电平值被看作高电平。数字信号会被周期性重新计时，以恢复到它的原始状态。</p><p>1.3  数字数据表示法<br>1.3.1  负数表示法<br>符号数值表示法 signed-magnitude representation，符号表示数所属的分类（正数或负数）、值表示数的量值的数字表示法。对带符号的整数执行加减法可以被描述为向一个方向或另一个方向移动一定数字单位。符号数值表示法存在问题，即表示0的方法有两种，+0或-0，在计算机中会引起不必要的麻烦。<br>十进制补码 （ten’s complement），一种负数表示法，负数I用10的k次幂减I表示。</p><p>二进制补码（ two’s complement），位模式最左边的二进制位指明了所表示的数值的符号，0为正，1为负（指明符号的同时，0变为1，也可以理解为在数的基础上加2^n，例如八位中，0加上了128，则用128表示-128，127加上128，则用255表示-1。因此，在二进制补码中11111111，不表示-127，而是表示-1）。假定数字只能用八位表示，七位表示数值，一位表示符号，则取值范围为-128～127。与十进制补码相类似：<br>-2 = 2^7 - 2 = 128 - 2 = 126<br>十进制数126用二进制表示为1111110，左边添加一位符号位变为11111110。<br>除了用公式计算，在二进制中还有更简单的方法计算二进制补码，当知道5的二进制数，如何得到-5的二进制数？<br>方法一：将每一位取反再加一<br>5:  00000101<br>取反：11111010<br>    +1：11111011 = 251<br>方法二：从右向左，直到第一个二进制1（包括），他们都是相同的。然后，以这个1为分界线，左面的位模式取反。<br>5:  00000101<br>-5:  11111011<br>二进制补码的加减法：二进制补码记数法的一个主要优点在于，减法可以转化为加法，从而可以使用相同的电路来实现。7-5与7+（-5）是一样的。因此，在计算机中执行7（0111）减去5（0101），将5转换为-5（1011），再执行0111+1011=0010=2。<br>溢出（overflow）：当给结果预留的位数存不下计算出的值的状况时。在二进制补码记数法中，如果两个正值相加的结果是负值，或者两个负值相加的结果为正，那么就发生了溢出问题。<br>127+3 ：01111111+00000011=10000010   在二进制补码中，第一位是符号位结果为-126。但如果表示的不是负数时，结果就是130将是正确的。<br>余码计数法（excess notation）<br>1.3.2  实数表示法<br>在计算中，非整数的值称为实值。实数具有整数部分和小数部分，每个部分都可能是0。位置记数法中，数字的位置表示数值，位值是由基数决定的。在十进制中，小数点左侧的位值有1、10、100等等，它们都是基数的幂，从小数点开始向左，每一位升高一次幂。小数点右侧的位值同样如此，只不过幂是负数。所以小数点右侧的位置是十分位（10^-1或十分之一）、百分位（10^-2或百分之一）。同理，在二进制中，小数点右侧的位置是二分位、四分位，以此类推。<br>一种基于科学计数法的存储方法，称为浮点记数法。任何实值可以由三个属性描述，即符号、指数和尾数，尾数由该数值中的数字构成，指数确定了小数点相对于尾数的位移。数字的个数是固定的，小数点却是浮动的，因此称为浮点记数法。在用浮点形式表示的数值中，正指数将把小数点右移，负指数将小数点左移。<br>假如一个字节由位模式01101011组成，符号为0，指数是110，尾数是1011。解码，尾数左边放置一个小数点，得到.1011，指数110是一个三位的二进制补码，表示整数2，因此小数点右移2位，可以得到10.11=1<em>2+0</em>1+1<em>1/2+1</em>1/4=2.75。<br>截断误差（truncation error）：由于尾数域空间不够大，而导致的存储部分数值丢失。二进制中的无穷展开式多于十进制，在二进制中无法精确表示1/10。<br>单精度浮点（single precision floating point）记数法，具有32位，其中1位表示符号位、8位表示指数、23位表示尾数。因此，单精度浮点最多有7位十进制有效数字，可以表示10^32到10^-37数量级之间的数，也就是说，可以精确地存储前七位十进制有效数字。<br>双精度浮点（double precision floating point）记数法，具有64位，最多有15位十进制有效数字。<br>科学计数法（scientific notation）是浮点表示法的一种形式，小数点总在最左边数字的右侧，也是说只有一位整数部分，12001.32708被写为1.200132708E+4。<br>1.4  文本表示法<br>1.4.1  ASCII字符集<br>字符集（character set）：字符和表示它们的代码的清单<br>ASCII字符集（American Standard Code for Information Interchange），用7位表示每个字符，可以表示128个字符，<br>0～31及127(共33个)是控制字符或通信专用字符（其余为可显示字符），如控制符：LF（换行）、CR（回车）、FF（换页）、DEL（删除）、BS（退格)、BEL（响铃）等；通信专用字符：SOH（文头）、EOT（文尾）、ACK（确认）等；ASCII值为8、9、10 和13 分别转换为退格、制表、换行和回车字符。它们并没有特定的图形显示，但会依不同的应用程序，而对文本显示有不同的影响 [1]  。<br>32～126(共95个)是字符(32是空格），其中48～57为0到9十个阿拉伯数字。<br>65～90为26个大写英文字母，97～122号为26个小写英文字母，其余为一些标点符号、运算符号等。<br>1.4.2 Unicode字符集<br>Unicode每个字符的编码为16位，但如果需要每个字符也可以使用更多空间，以便表示额外的字符。Unicode字符集的一个方便之处是把ASCii字符集作为一个子集，即前256个字符与扩展ASCii字符集中的完全一样。因此，即使底层系统采用Unicode字符集，采用ASCii值的程序也不会受到影响。<br>1.4.3 文本压缩<br>文本压缩主要有三种方式<br>关键字编码（keyword encoding）：用单个字符代替常用的单词。例如用^替代as，~替代the等等。这种方式有一些局限性。首先，用来编码的字符不能出现在原始文本中，否则会产生歧义；其次，The不会被编码，因为大小写是不同的字符；最后常用的单词都比较短，节省空间有限，而长的单词出现频率低也没有替换的必要。<br>行程长度编码（run-length encoding）：把一系列重复字符替换为它们重复出现的次数。将重复字符的序列替换为标志字符，后面加重复字符和说明字符重复次数的数字，例如AAAAAAA可以替换为<em>A7，</em>即为一种标志字符。因为用一个字符记录重复的次数，看似不能对重复次数大于9的序列编码，但实际上，在字符集中一个字符是由多个为表示的，因此可以将次数字符解释为一个二进制数，而不是ASCii数字。因此，能够编码的重复字符重复次数可以是0~255。<br>赫夫曼编码（Huffman encoding）：用变长的二进制串表示字符，使常用的字符具有较短的编码。不同的字符编码长度不同，例如常出现的A编码00，而出现比较少的D则为1011。由于编码是变长的，因此不知道每个字符对应多少位编码，看似很难将一个字符串解码。赫夫曼编码的一个重要特征是用于表示一个字符串的位串不会是另个一字符的位串的前缀。因此，从左到右扫描一个位串时，每当发现一个位串对应于一个字符，那么将唯一对应，不会发生歧义。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;1-信息层-数据存储&quot;&gt;&lt;a href=&quot;#1-信息层-数据存储&quot; class=&quot;headerlink&quot; title=&quot;1 信息层-数据存储&quot;&gt;&lt;/a&gt;1 信息层-数据存储&lt;/h1&gt;&lt;h2 id=&quot;1-1-二进制和记数系统&quot;&gt;&lt;a href=&quot;#1-1-二进制和记</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Hexo</title>
    <link href="http://example.com/2022/01/09/Command/Hexo/"/>
    <id>http://example.com/2022/01/09/Command/Hexo/</id>
    <published>2022-01-08T16:00:00.000Z</published>
    <updated>2022-01-12T00:33:57.347Z</updated>
    
    <content type="html"><![CDATA[<span id="more"></span><h1 id="Hexo"><a href="#Hexo" class="headerlink" title="Hexo"></a>Hexo</h1><h2 id="1-安装-NodeJs"><a href="#1-安装-NodeJs" class="headerlink" title="1. 安装 NodeJs"></a>1. 安装 NodeJs</h2><p>windows/mac 安装</p><p><a href="https://nodejs.org/en/download/">NodeJs官网</a></p><p>检查是否安装成功</p><pre><code class="lang-bash">node -v</code></pre><p>npm 换源</p><pre><code class="lang-shell">npm config set registry https://registry.npm.taobao.org</code></pre><p>检测是否修改成功</p><pre><code class="lang-shell">npm config get registry</code></pre><h2 id="2-安装-Hexo"><a href="#2-安装-Hexo" class="headerlink" title="2. 安装 Hexo"></a>2. 安装 Hexo</h2><p>安装命令</p><pre><code class="lang-shell">npm install hexo-cli -g</code></pre><p>检查是否安装成功</p><pre><code class="lang-shell">hexo -v</code></pre><p>初始化文件夹</p><pre><code class="lang-shell">hexo init blog</code></pre><h2 id="3-安装相关支持库"><a href="#3-安装相关支持库" class="headerlink" title="3. 安装相关支持库"></a>3. 安装相关支持库</h2><p>git 支持</p><pre><code class="lang-shell">npm install hexo-deployer-git --save</code></pre><p>search 支持</p><pre><code class="lang-shell">npm install hexo-generator-feed --save</code></pre><h2 id="4-公式相关依赖"><a href="#4-公式相关依赖" class="headerlink" title="4. 公式相关依赖"></a>4. 公式相关依赖</h2><pre><code class="lang-shell">npm uninstall hexo-renderer-marked --save</code></pre><pre><code class="lang-shell">npm install hexo-renderer-kramed --save</code></pre><p>打开node_modules/hexo-renderer-kramed/lib/renderer.js，将</p><pre><code class="lang-javascript">// Change inline math rulefunction formatText(text) &#123;    // Fit kramed&#39;s rule: $$ + \1 + $$    return text.replace(/`\$(.*?)\$`/g, &#39;$$$$$1$$$$&#39;);&#125;</code></pre><p>改为</p><pre><code class="lang-javascript">// Change inline math rulefunction formatText(text) &#123;    return text;&#125;</code></pre><pre><code class="lang-shell">npm uninstall hexo-math --save</code></pre><pre><code class="lang-shell">npm install hexo-renderer-mathjax --save</code></pre><p>打开node_modules/hexo-renderer-mathjax/mathjax.html，最后一行改为</p><pre><code class="lang-html">&lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML&quot;&gt;&lt;/script&gt;</code></pre><p>打开node_modules/kramed/lib/rules/inline.js:</p><p>将</p><pre><code class="lang-javascript">escape: /^\\([\\`*&#123;&#125;\[\]()#$+\-.!_&gt;])/,</code></pre><p>改为</p><pre><code class="lang-javascript">escape: /^\\([`*\[\]()# +\-.!_&gt;])/,</code></pre><p>将</p><pre><code class="lang-javascript">em: /^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/,</code></pre><p>改为</p><pre><code class="lang-javascript">em: /^\*((?:\*\*|[\s\S])+?)\*(?!\*)/,</code></pre><p>在home/_config.yml，中添加如下内容</p><pre><code class="lang-yml">mathjax:    enable: true</code></pre>]]></content>
    
    
      
      
    <summary type="html">&lt;span id=&quot;more&quot;&gt;&lt;/span&gt;
&lt;h1 id=&quot;Hexo&quot;&gt;&lt;a href=&quot;#Hexo&quot; class=&quot;headerlink&quot; title=&quot;Hexo&quot;&gt;&lt;/a&gt;Hexo&lt;/h1&gt;&lt;h2 id=&quot;1-安装-NodeJs&quot;&gt;&lt;a href=&quot;#1-安装-NodeJ</summary>
      
    
    
    
    <category term="Command" scheme="http://example.com/categories/Command/"/>
    
    
  </entry>
  
  <entry>
    <title>Linux</title>
    <link href="http://example.com/2022/01/09/Command/Linux/"/>
    <id>http://example.com/2022/01/09/Command/Linux/</id>
    <published>2022-01-08T16:00:00.000Z</published>
    <updated>2022-01-24T09:29:56.552Z</updated>
    
    <content type="html"><![CDATA[<span id="more"></span><h1 id="Linux"><a href="#Linux" class="headerlink" title="Linux"></a>Linux</h1><h2 id="网络"><a href="#网络" class="headerlink" title="网络"></a>网络</h2><h3 id="网络第一包慢"><a href="#网络第一包慢" class="headerlink" title="网络第一包慢"></a>网络第一包慢</h3><pre><code class="lang-bash">ping -n refclient.ext.here.com</code></pre><blockquote><p>PING refclient.ext.here.com (108.139.1.24) 56(84) bytes of data.<br>64 bytes from 108.139.1.24: icmp_seq=1 ttl=243 time=1.18 ms<br>64 bytes from 108.139.1.24: icmp_seq=2 ttl=243 time=1.18 ms<br>64 bytes from 108.139.1.24: icmp_seq=3 ttl=243 time=1.14 ms<br>64 bytes from 108.139.1.24: icmp_seq=4 ttl=243 time=1.15 ms<br>64 bytes from 108.139.1.24: icmp_seq=5 ttl=243 time=1.14 ms<br>64 bytes from 108.139.1.24: icmp_seq=6 ttl=243 time=1.14 ms</p></blockquote><pre><code class="lang-bash">time ping -n refclient.ext.here.com -c 1</code></pre><blockquote><p>1 packets transmitted, 1 received, 0% packet loss, time 0ms<br>rtt min/avg/max/mdev = 1.188/1.188/1.188/0.000 ms<br>real    0m5.011s<br>user    0m0.000s<br>sys     0m0.000s</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;span id=&quot;more&quot;&gt;&lt;/span&gt;
&lt;h1 id=&quot;Linux&quot;&gt;&lt;a href=&quot;#Linux&quot; class=&quot;headerlink&quot; title=&quot;Linux&quot;&gt;&lt;/a&gt;Linux&lt;/h1&gt;&lt;h2 id=&quot;网络&quot;&gt;&lt;a href=&quot;#网络&quot; class=&quot;head</summary>
      
    
    
    
    <category term="Command" scheme="http://example.com/categories/Command/"/>
    
    
  </entry>
  
  <entry>
    <title>5. Decision Tree</title>
    <link href="http://example.com/2021/12/22/MachineLearning/5.DecisionTree/"/>
    <id>http://example.com/2021/12/22/MachineLearning/5.DecisionTree/</id>
    <published>2021-12-21T16:00:00.000Z</published>
    <updated>2022-02-07T10:37:33.234Z</updated>
    
    <content type="html"><![CDATA[<span id="more"></span><h1 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h1><h2 id="1-历史背景"><a href="#1-历史背景" class="headerlink" title="1. 历史背景"></a>1. 历史背景</h2><p>关于树形结构的历史可以追溯到古巴比伦，在这里就不过多阐述。</p><p>1963：最早的发表在文献中的回归树算法是Automatic Interaction Detection(AID, Morgan &amp; Sonquist)。AID 从根节点开始递归的将数据拆分为两个子节点，选择划分特征的依据是通过类似于方差的公式 $\sum (X-\overline{X})^2$ ，称为不纯函数，计算划分子节点的误差平方和使其越小说明划分效果越好，因为 $\sum (X-\overline{X})^2=\sum X^2-N\overline{X}^2$，所以最终只要计算划分后 $\sum N_i\overline{X_i}^2$ 即可，选择最大的特征进行划分。当划分后的误差平方减少值小于 $0.02(\sum X^2-N\overline{X}^2)$ 则结束算法。</p><p>1972：THeta Automatic Interaction Detection (THAID, Messenger &amp; Mandell) 将上述思想应用到分类问题中，提出了第一个分类树，并用熵函数和基尼指数代替上述不纯函数。</p><p>1980：CHi-squared Automatic Interaction Detector (CHAID) 由 Kass 创建，通过 $\mathcal{X}^2$ 拟合优度检验来找到主要特征，假设有两个类别，默认分布概率都是 $1/2$，计算 $\mathcal{X}^2=\sum\sqrt{(n_i-np_i)/(np_i)}$，选取最大的特征进行分割。卡方拟合优度越大意味着观察频数与默认均匀分布的区别越大，也就是越趋于有序，当同一特征将样本均归于同一类时，卡方拟合优度达到最大，当两个类别各有一半时，卡方拟合优度则为0。</p><p>1984：Classification And Regression Trees (CART) 伯克利的统计学教授 Leo Breiman 和 Charles Stone 以及斯坦福的 Jerome Friedman 和 Richard Olshen 共同建立。它同样使用 AID 的贪婪搜索方法，但增加了新颖的改进。CART 不再使用停止规则，而是生成一颗大树，通过最低交叉验证对树结构进行剪枝。这解决了 AID 和 THAID 的欠拟合和过拟合问题。</p><p>1986：John Ross Quinlan 提出了一个新概念有多个答案的树，而CART 和所有其他算法对每个问题只有两个答案（称为二叉树）。Quinlan 使用称为增益比的杂质标准发明了  Iterative Dichotomiser 3（ID3）。</p><p>1993：C4.5 是 ID3 算法的扩展，C4.5 解决了其前身的缺点，在数据挖掘杰出论文的 Top 10 算法中排名第一（Springer LNCS，2008）。</p><h2 id="2-决策树结构"><a href="#2-决策树结构" class="headerlink" title="2.决策树结构"></a>2.决策树结构</h2><p>决策树模型是一种对实例进行分类或回归的树形结构。树结构的每个节点均代表输入特征空间的部分子区域，对于不再分割的节点称为终端节点（terminal node）也叫叶节点（leaf node），在图中由矩形框表示；除了叶节点以外的称为内部节点（internal node），在图中用圆形表示。</p><p>构建决策树的时候，从根节点开始，通过一定规则选定分割的特征维度和特征值，然后构建下一层的多个节点。特别地，如果每一个节点只通过一个特征的一个值分割为两部分，该决策树则称为二叉决策树。</p><p>对于决策树中的每一个节点的后代节点都是不相交的，也就是它们之间互斥，同时一个节点所有后代子集的并集即为该节点。例如图1中所示，$X_2$ 与 $X_3$ 互斥且 $X=X_2\bigcup X_3$。决策树中的每一层中的节点的并集构成整体输入特征空间。</p><p>终端节点是输入空间的一个子区域，所有终端节点互斥且完备，即所有终端节点的并集构成整体输入空间。在分类问题中，每个终端节点由一个类标签指定，可能有多个终端节点有相同的类标签。分类器最终分类划分是将同一类对应的所有终端子集放在一起得到的。例如，上图的划分结果为</p><script type="math/tex; mode=display">\begin{aligned}    A_1&=X_{15} \\    A_2&=X_{10}\bigcup X_{16}\\    A_3&=X_{11}\bigcup X_{14}\\    A_4&=X_{6}\bigcup X_{17}\\    A_5&=X_8 \\    A_6&=X_{12} \\\end{aligned}</script><p>综上所述，可以发现构建决策树需要解决三个问题：</p><ol><li>在每个中间节点选择拆分的方法</li><li>终止条件</li><li>每个终端叶子节点的输出结果</li></ol><p>进一步地，如果想要决策树模型不仅对训练数据有很好的拟合效果，同时对未知数据有很好的预测，也就是避免过拟合现象的发生，我们还可能需要对已生成的决策树进行自下而上的剪枝，使树结构变得更简单，从而使它具有更好的泛化能力，也就是</p><ol><li>决策树的剪枝 </li></ol><p>有关第一个问题，在这里先定义一个通用的方法和标准。</p><p>定义训练集的输入实例数为 $N$，每个类别的数量为 $N_j$，则每个节点 $t$ 中的某一类别的数量为 $N_j(t)$，落入节点的概率为 $p(t) = N(t)/N$，条件概率$p(t|j)=N_j(t)/N_j$，属于 $j$ 类且落入节点 $t$ 两事件同时发生的概率为 $p(j,t)=p(t|j)p(j)$，在这里我们先假设先验概率 $p(j)$ 就是训练集中的类别概率 $p(j)=N_j/N$。因此，根据贝叶斯公式我们可以得到后验概率</p><script type="math/tex; mode=display">\begin{aligned}    P(j|t)&=\frac{p(t|j)p(j)}{p(t)}\\    &=\frac{(N_j(t)/N_j) \ast (N_j/N)}{N(t)/N}\\    &=\frac{N_j(t)}{N(t)}\end{aligned}</script><blockquote><p>忙活半天，得到了一个很显而易见的结果 -_-!  但这个显而易见的比例实际上是条件概率。</p></blockquote><p>通常情况下先验概率 $p(j)$ 被视为训练集中的比例 $N_j/N$。但学习样本中的比例无法反映到现实中实际的比例，就例如评价道路的好坏，实际上得到的训练集都是通过 bad case 中的路线，实际上，可能差路线存在的比例会更低。</p><p>因此，我们由其他途径的得到先验概率 $\pi(j)$ 取代训练集中的概率 $p(j)$，联合概率即为 </p><script type="math/tex; mode=display">p(j,t)=\pi(j)N_j(t)/N_j</script><p>同时任何案例落入节点 $t$ 的概率重新带入估计 </p><script type="math/tex; mode=display">p(t)=\sum_jp(j,t)</script><p>最后再去计算条件概率 $p(j|t)$，可以发现，替换后的条件概率依旧满足 </p><script type="math/tex; mode=display">\sum_jp(j|t)=1</script><p>决策树划分的目的就是希望能通过特征将实例的类别进行区分出来。决策树的每个内部节点将分割为更多的子区域，每个子区域的条件是一样的，而所希望的就是某一类的条件概率 $P(j|t)$ 尽可能的大。换句话说，希望每个后代节点中的数据比父节点中的数据更“纯”（purer）。</p><p>例如，假设有个六分类问题，根节点的条件概率 $(p_1,p_2,p_3,p_4,p_5,p_6)$ 为 $(1/6,1/6,1/6,1/6,1/6,1/6)$，一个好的划分结果就是两个子节点的条件概率也就是类别比例为 $(1/3,1/3,1/3,0,0,0)$ 和 $(0,0,0,1/3,1/3,1/3)$ 。</p><p>因此我们需要寻找一个测度去衡量一个节点中子集的不纯度（impurity），定义一个非负函数，要求当所有类均匀混合在一起时它的不纯度最大，当节点中只包含一个类时，该值最小，现将这个函数定义为 $i(t)$。</p><p>对于任何中间节点 $t$，假设有一个节点的候选切分 $s$ 将节点分割为 $t_L$ 和 $t_R$，这样 $t$ 中的事件（case）分别进入 $t_L$ 和 $t_R$ 的比例为 $p_L$ 和 $p_R$，那么最终切分点 $s$ 的好坏定义为不纯度的好坏的减少</p><script type="math/tex; mode=display">\triangle i(s,t)=i(t)-p_Li(t_L)-p_Ri(t_R)</script><p>对于不是二叉树而是多切分节点的算法只需将上公式扩展为多个即可，但始终 $p_1+p_2+\cdots+p_n=1$。</p><p>我们对 $i(t)$ 函数的要求进行总结：</p><ol><li>当每个类别概率相等时，$i(t)$ 值最大。</li><li>当只有一个类别时，$i(t)$ 值最小</li><li>$i(t)$ 是轴对称函数</li></ol><p>至此，我们得到一个通用的标准去衡量切分的好坏程度，在不同的算法中以及回归和分类树中，所不同的只是不纯度函数 $i(t)$ 的不同。</p><h2 id="3-CART-算法"><a href="#3-CART-算法" class="headerlink" title="3. CART 算法"></a>3. CART 算法</h2><p>CART 算法由 Breiman 等人在 1984 年提出。CART 同样由特征选择、树的生成以及剪枝组成，既可用于分类也可以用于回归。</p><p>CART 是在给定输入随机变量 $X$ 条件下输出随机变量 $Y$ 的条件概率分布的学习方法。CART 是二叉树，递归的二分每个特征，分为“是”或“否”，将输入空间即特征空间划分为有限个单元，并在每个单元上确定预测的概率分布。</p><p>CART回归树实际是基于AID的决策树方法，但增加了最优切分点的寻找以及剪枝的过程。CART分类树使用到的基尼指数也不是第一次出现。</p><h3 id="3-1-CART-分类"><a href="#3-1-CART-分类" class="headerlink" title="3.1 CART 分类"></a>3.1 CART 分类</h3><p>我们从前述四个问题出发，先解决最直观的第三个问题</p><ol><li>每个终端叶子节点的输出结果</li></ol><p><strong>定义1</strong> 一个决策树的所有终端节点的集合为 $T$，每个终端节点 $t\in T$，所有分类标签的集合为 $j\in \{1,2,\cdots,J\}$，则某一节点所分配的类为 $j(t)$。</p><blockquote><p>$j(t)$ 是类分配结果同时也代表一种类分配规则</p></blockquote><p>对于任何一种类分配结果 $j(t)$，如果实例落入节点 $t$，则错误分类概率的重新替换估计（resubstitution estimate）为</p><script type="math/tex; mode=display">\sum_{j\ne j(t)}p(j|t)</script><p>我们需要最小化这个误分类估计的规则作为我们类分配规则 $j^*(t)$。</p><p>由于一个节点最终只能输出一个类别，一个直观的类分配规则就是将某一节点中占比最大的类分配给该节点，这样做的另一个原因就是使误分类的概率最小。</p><p><strong>定义2</strong> 类分配规则 $j^\ast(t)$：如果 $\displaystyle p(j|t)=\max_i p(i|t)$，则 $j^\ast(t)=j$，如果两个或多个不同类别达到最大值，则将 $j^\ast(t)$ 任意指定为任何一个最大化类别。</p><p>第三个问题得到解决。</p><p>进一步地，我们就可以得到节点 $t$ 的误分类概率的重新带入估计 $r(t)$ 为</p><script type="math/tex; mode=display">\begin{aligned}    r(t)&=\sum_{j\ne j^*(t)}p(j|t)\\    &=1-\max_j p(j|t)\end{aligned}</script><p>整体决策树误分类率的重新代入估计则为 </p><script type="math/tex; mode=display">R(T)=\sum_{t\in T}r(t)p(t)</script><p>$R(t)$ 的一个重要特性，以任何方式分割的越多，则 $R(T)$ 就会变得越小。一个节点 $t$ 分割成两部分 $t_L$ 和 $t_R$，则有</p><script type="math/tex; mode=display">R(t)\ge R(t_L)+R(t_R)</script><blockquote><p>以上公式的推导是在假定所有错误分类为 $i$ 类对象的成本或损失都是一样的，在某些问题中希望能将它们区分出来，例如某一类错误分类的代价将会更大，也就是希望尽可能减少其误分类的概率。因此，引入一组错误分类成本 $c(i|j)$ 代表将 $j$ 类对象误分类为 $i$ 类对象的代价，它应该满足</p><script type="math/tex; mode=display">{\displaystyle c(i|j)={\begin{cases}C_i(C_i\ge 0)&i\ne j\\0&i=j\end{cases}}}</script><p>随机一个实例落入节点 $t$ 并被分类为类别 $i$，则估计的预期误分类代价为 </p><script type="math/tex; mode=display">\sum_j c(i|j)p(j|t)</script><p>一个自然地节点分配规则是选择 $i$ 来最小化这个表达式，因此，<br>令 $j^*(t)=i_0$，当 $i_0$ 最小化 $\displaystyle \sum_j c(i|j)p(j|t)$，给定节点 $t$ 定义预期错误分类成本的重新替代估计 $r(t)$ 为</p><script type="math/tex; mode=display">r(t)=\min_i \sum_j c(i|j)p(j|t)</script><p>当 $c(i|j)=1,i\ne j$ 可以发现 </p><script type="math/tex; mode=display">\sum_j c(i|j)p(j|t)=1-p(i|t)</script><p>最小化结果与上面相同</p></blockquote><p>接下来，我们解决第一个问题</p><ol><li>在每个中间节点选择拆分的方法</li></ol><p>在第二节，我们已经定义了一个通用的切分准则，需要确定的只是 $i(t)$ 函数是什么。事实上，CART 分类树是通过基尼指数来作为切分函数的，但基尼指数并不是凭空出现的，也不是唯一可行的。通过实验发现，构建决策树的总体误分类率对分割规则的选择并不敏感，只要在合理的规则类别内，区别是不大的，同时原作者给出了其从开始到最终基尼指数的思考过程，在这里我们从头开始追根溯源。</p><p>从误分类率 $r(t)$ 出发，我们可以发现 $r(t)$ 是可以直接作为衡量节点不纯度的标准，当节点中只有一个类别时，$r(t)$ 为 $0$，当节点中均匀分布时，$r(t)$ 最大为 $1-\frac{1}{n}$。因此最好的分割方式就是最大化</p><script type="math/tex; mode=display">r(t)-P_Lr(t_L)-P_Rr(t_R)</script><p>用误分类率 $r(t)$ 作为不纯函数，存在两个严重的缺陷。</p><p>第一个缺陷，用上式对节点 $t$ 进行切分有可能所有拆分结果都为 $0$。证明如下：</p><script type="math/tex; mode=display">\begin{aligned}    r(t)&=\sum_{j}c(j^*(t)|j)p(j|t)\\    &=\sum_{j}c(j^*(t)|j)p(j,t)p(t)\\    &=\sum_{j}c(j^*(t)|j)[p(j,t_L)+p(j,t_R)]p(t)\end{aligned}</script><p>让 $p(t)=1$，因此</p><script type="math/tex; mode=display">\begin{aligned}    r(t)&-P_Lr(t_L)-P_Rr(t_R)\\    &=\sum_{j}c(j^*(t)|j)[p(j,t_L)+p(j,t_R)]-\min_i\sum_jc(i|j)p(j|t_L)P_L-\min_i\sum_jc(i|j)p(j|t_R)P_R\\    &=\sum_{j}c(j^*(t)|j)[p(j,t_L)+p(j,t_R)]-\min_i\sum_jc(i|j)p(j,t_L)-\min_i\sum_jc(i|j)p(j,t_R)\\    &=\sum_{j}c(j^*(t)|j)p(j,t_L)-\min_i\sum_jc(i|j)p(j,t_L) + \sum_{j}c(j^*(t)|j)p(j,t_R)-\min_i\sum_jc(i|j)p(j,t_R)\\    &=\sum_{j}c(j^*(t)|j)p(j,t_L)-\sum_jc(j^*(t_L)|j)p(j,t_L) + \sum_{j}c(j^*(t)|j)p(j,t_R)-\sum_jc(j^*(t_R)|j)p(j,t_R)\\\end{aligned}</script><p>等号的右边是大于 $0$ 的，并且仅当 $j^<em>(t)=j^</em>(t_L)=j^*(t_R)$ 时等号成立。</p><blockquote><p>这个式子可以这么理解，$j^\ast(t)$ 是使 $\displaystyle\sum_j c(i|j)p(j|t)$ 最小化的特征，当节点 $t$ 被切分为两部分后， $j^\ast(t_L)$ 是使 $\displaystyle\sum_j c(i|j)p(j|t_L)$ 最小化的特征，那么 $j^\ast(t)$ 无论是什么都不会使 $\displaystyle \sum_{j}c(j^<em>(t)|j)p(j,t_L)$ 比 $\displaystyle \sum_jc(j^</em>(t_L)|j)p(j,t_L)$ 更小。</p></blockquote><p>从上式中可以知道，当父节点与切分后的两个节点的最大占比的类别一样时，无论怎么切分 $r(t)-P_Lr(t_L)-P_Rr(t_R)$ 均为 $0$，即不存在单个或少量的最优拆分点。</p><p>第二个缺陷是很难对决策树进行准确的评价。换句话说，降低错误分类率似乎不是整个决策树生长过程的好的目标和标准。</p><p>举一个简单的例子，父节点 $t$ 的分布为 400（类别1）和 400（类别2），一个切分方式生成两个节点，一个节点分布为 300（类别1）、100（类别2），另一个节点分布为 100（类别1）、300（类别2），其中 200 个实例被分类错误，误分类率为 0.25。另一种切分方式为200（类别1）、400（类别2）和 200（类别1）、0（类别2），误分类率同样为 0.25。</p><p>误分类率对两种切分方式评价相同，不纯度值的减少分别为 $0.5-1/2<em>0.25-1/2</em>0.25=0.25$ 和 $0.5-6/8<em>2/6-2/8</em>0=0.25$。但对于决策树未来的生长来看，第二种切分方式更为可取，虽然一个节点的误分类率为 $2/6$ ，但另一个节点误分类率为 $0$，这个节点是终端，无需进一步切分。</p><p>综上所述，为了解决误分类率作为不纯度函数的缺陷，需要对不纯度函数进行改进。</p><p>从二分类问题出发，误分类率不纯度函数为</p><script type="math/tex; mode=display">\begin{aligned}    \varphi(p_1,p_2)&=1-\max (p_1,p_2)\\    &=\min (p_1,p_2)\\    &=\min (p_1,1-p_1)\\    &={    \begin{cases}        p_1&0\le p_1\le 0.5\\        1-p_1&0.5<p_1\le 1    \end{cases}    }\end{aligned}</script><p>从上述例子可以想到，为了将两种切分方式区分出来，使第二种切分结果得到的评价更高，就要充分奖励更纯的节点。假设 $p_1&gt;0.5$，那么 $\varphi(p_1)=1-p_1$ 是随着 $p_1$ 线性减少的，为了使纯节点的评价更好，需要使 $\varphi(p_1)$ 随着 $p_1$ 的增加而比线性下降更快。也就是当 $p’’_1&gt;p’_1$ 时，$\varphi’(p’’_1)&lt;\varphi’(p’_1)$，因此要求不纯度函数是严格凸函数。如果 $\varphi$ 在 $[0,1]$ 上有连续的二阶导数，则应该满足 $\varphi’’(p_1)&lt;0,0&lt;p_1&lt;1$。</p><p>将不纯函数需要满足的条件用公式表示</p><ol><li>$\varphi(0)=\varphi(1)=0$</li><li>$\varphi(p_1)=\varphi(1-p_1)$</li><li>$\varphi(1/2)=\text{maximum}$ </li><li>$\varphi’’(p_1)&lt;0,0&lt;p_1&lt;1$</li></ol><p>最简单的满足条件的函数就是二次多项式</p><script type="math/tex; mode=display">\varphi(x)=a+bx+cx^2</script><p>由 1 可以得到 $a=0,b+c=0$，因此</p><script type="math/tex; mode=display">\varphi(x)=b(x-x^2)</script><p>公式 4 要求 $b&gt;0$，不失一般性，取 $b=1$，因此得到不纯度函数</p><script type="math/tex; mode=display">i(t)=p(1|t)p(2|t)</script><p>这就是基尼指数的原型，公式简单且易于计算。一个直观的解释，假设节点 $t$ 中的所有 1 类对象被赋予数值 1，而 2 类对象被赋予数值 0，那么 $p(1|t)$ 和 $p(2|t)$ 是节点中两个类的比例，则节点中数值的样本方差就是 $p(1|t)p(2|t)$。</p><blockquote><p>信息熵公式 $i(t)=-p(1|t)\log p(1|t)-p(2|t)\log p(2|t)$ 同样满足上述条件。原作者说想不出任何内在的原因为什么同样一个满足条件的函数应该优于任何其他函数，并且测试表明两个函数给出了相似的结果，因此依据简单性原则选择基尼指数。</p></blockquote><p>将二分类问题扩展到多分类问题中，给定节点 $t$ 的类估计概率 $p(j|t),j=1,\cdots,J$，基尼指数定义为</p><script type="math/tex; mode=display">i(t)=\sum_j\sum_{i\ne j}p(j|t)p(i|t)</script><p>同样可以写成</p><script type="math/tex; mode=display">i(t)=\sum_jp(j|t)(1-p(j|t))=1-\sum_jp^2(j|t)</script><p>对于二分类问题则有</p><script type="math/tex; mode=display">\begin{aligned}    i(t)&=2p(1|t)p(2|t)\\    &=2p(1-p)\\\end{aligned}</script><blockquote><p>这里与上面的不纯度函数不同，具体在于多了 2，基尼指数是在误差分类率上推导出来的，按理说$\displaystyle i(t)=\min_j\sum_{i\ne j}p(j|t)p(i|t)$，但实际上是累加的，基尼指数的最大值是1，而误分类率最大值是0.5，有些不解。如果按下述所说的，那么实际难道是随机选择分类吗？</p></blockquote><p>基尼指数的一种解释方法是，不是使用多数选择的规则对节点 $t$ 中的对象进行分类，而是从节点中随机选择实例将该实例的类别分配给对象，因此将以概率 $p(i|t)$ 分配为类别 $i$，则误分类的概率就是 $\displaystyle \sum_{i\ne j}p(j|t)p(i|t)=p(j|t)(1-p(j|t))$，然后再对 $j$ 求和。因此</p><script type="math/tex; mode=display">i(t)=\sum_j\sum_{i\ne j}p(j|t)p(i|t)</script><p>另一种解释方法如前所说，在节点 $t$ 中，为所有 $j$ 类对象分配值 $1$，为所有其他对象分配值 $0$。那么这些值的样本方差为$p(j|t)(1-p(j|t))$。对所有 $J$ 类重复操作并求和，则结果就是</p><script type="math/tex; mode=display">i(t)=\sum_jp(j|t)(1-p(j|t))=1-\sum_jp^2(j|t)</script><h3 id="3-2-CART-回归"><a href="#3-2-CART-回归" class="headerlink" title="3.2 CART 回归"></a>3.2 CART 回归</h3><p>回归树同样需要解决上面三个问题</p><p>训练数据集：</p><script type="math/tex; mode=display">T=\{(\mathbf x_1,y_1),(\mathbf x_2,y_2),...,(\mathbf x_N,y_N)\}</script><p>第三个问题最直观也最好解决，假设输出值的函数为 $d$，总的均方误差为</p><script type="math/tex; mode=display">R(d)=\frac{1}{N}\sum_n^N(y_n-d(\mathbf x_n))^2</script><p>对于每个叶子节点，假设输入空间为 $t$，根据最小二乘法，为了使 $R(d)$ 最小，输出值应为</p><script type="math/tex; mode=display">d_t(\mathbf x_n)=\overline{y}_t=\frac{1}{N_t}\sum_{\mathbf x_n\in t}y_n</script><p><strong>即回归树建立完后，每个叶子节点的输出值为其分配样本的均值。</strong></p><p>进一步的，将预测值带入每个节点 $t$，并用 $R(T)$ 取代 $R(d)$</p><script type="math/tex; mode=display">R(T)=\frac{1}{N}\sum_{t\in T}\sum_{\mathbf x_n\in t}(y_n-\overline{y}_t)^2</script><p>定义</p><script type="math/tex; mode=display">R(t)=\frac{1}{N}\sum_{\mathbf x_n\in t}(y_n-\overline{y}_t)^2</script><blockquote><p>注意这里不是 $N_t$ </p></blockquote><p>因此，$R(T)$ 还可以写为</p><script type="math/tex; mode=display">R(T)=\sum_{t\in T}R(t)</script><p>对以上表达式一个简单的解释就是，对于每个节点 $t$，$\displaystyle \sum_{\mathbf x_n\in t}(y_n-\overline{y}_t)^2$ 是节点内的误差平方和，对 $t$ 求和给出所有节点的总平方和，再除以 $N$ 给出平均值。</p><p>假定分割前的均方误差为 $R(t)$，分割点为特征向量 $j$，分割值为 $s$，输入空间 $t$ 将被分为两个区域</p><script type="math/tex; mode=display">t_L=\{x|x^{(j)}\le s\},t_R=\{x|x^{(j)}\ge s\}</script><p>为了使分割更为有效，将使均方误差减少的尽可能多，因此目标函数为</p><script type="math/tex; mode=display">\triangle R(s,t)=R(t)-R(t_L)-R(t_R)</script><p>与 $s$ 相关的变量只有后两项，因此</p><script type="math/tex; mode=display">\max \triangle R(s,t)=\min [R(t_L)+R(t_R)]</script><p>令 $p(t)=\frac{N_t}{N}$ 表示随机选择输入变量落入节点 $t$ 的概率，定义节点内方差</p><script type="math/tex; mode=display">s^2(t)=\frac{1}{N_t}\sum_{\mathbf x_n\in t}(y_n-\overline{y}_t)^2</script><p>因此可得 $R(t)=s^2(t)p(t)$，并且</p><script type="math/tex; mode=display">R(T)=\sum_{t\in T}s^2(t)p(t)</script><p>$t$ 的最佳分割也就是使加权方差最小化</p><script type="math/tex; mode=display">\min[p(t_L)s^2(t_L)+p(t_R)s^2(t_R)]</script><p>实际上概率 $p$ 可以带入进去，再乘以 $N$，就得到李航书中的公式</p><script type="math/tex; mode=display">\min_{j,s} \left[\sum_{x_i \in t_L} (y_i-\overline{y}_{t_L})^2+ \sum_{x_i \in t_R} (y_i-\overline{y}_{t_R})^2\right]</script><p>依次选取切分变量 $j$，遍历空间中所有输入点确定最优切分值 $s$，找到最优切分变量和切分值使上式最小。</p><p>对每个区域依次重复上述过程，直到达到终止条件。与 AID 不同，CART 终止条件为节点中的样本个数小于特定阈值（通常为5），或者节点中样本均为一个类别，即该节点为纯节点。</p><p>至此回归的三个问题均已解决，归纳如下</p><p><a href="https://holypython.com/dt/decision-tree-history/">https://holypython.com/dt/decision-tree-history/</a><br><a href="https://www.explorium.ai/blog/the-complete-guide-to-decision-trees/">https://www.explorium.ai/blog/the-complete-guide-to-decision-trees/</a><br><a href="https://en.wikipedia.org/wiki/C4.5_algorithm">https://en.wikipedia.org/wiki/C4.5_algorithm</a><br><a href="https://www.analyticsvidhya.com/blog/2021/05/implement-of-decision-tree-using-chaid/">https://www.analyticsvidhya.com/blog/2021/05/implement-of-decision-tree-using-chaid/</a><br><a href="https://www.cnblogs.com/fushengweixie/p/8039991.html">https://www.cnblogs.com/fushengweixie/p/8039991.html</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;span id=&quot;more&quot;&gt;&lt;/span&gt;
&lt;h1 id=&quot;决策树&quot;&gt;&lt;a href=&quot;#决策树&quot; class=&quot;headerlink&quot; title=&quot;决策树&quot;&gt;&lt;/a&gt;决策树&lt;/h1&gt;&lt;h2 id=&quot;1-历史背景&quot;&gt;&lt;a href=&quot;#1-历史背景&quot; class=&quot;head</summary>
      
    
    
    
    <category term="Machine learning" scheme="http://example.com/categories/Machine-learning/"/>
    
    
  </entry>
  
  <entry>
    <title>4. Naive Bayes Classifier</title>
    <link href="http://example.com/2021/12/05/MachineLearning/4.NaiveBayesClassifier/"/>
    <id>http://example.com/2021/12/05/MachineLearning/4.NaiveBayesClassifier/</id>
    <published>2021-12-04T16:00:00.000Z</published>
    <updated>2022-01-14T08:09:28.329Z</updated>
    
    <content type="html"><![CDATA[<span id="more"></span><h1 id="朴素贝叶斯分类器"><a href="#朴素贝叶斯分类器" class="headerlink" title="朴素贝叶斯分类器"></a>朴素贝叶斯分类器</h1><p>朴素贝叶斯分类器是基于贝叶斯定理与特征条件独立假设的分类方法。</p><h2 id="1-历史背景"><a href="#1-历史背景" class="headerlink" title="1. 历史背景"></a>1. 历史背景</h2><p>朴素贝叶斯分类器最早的出现时间在 1964 年，由 Mosteller 和 Wallace 第一次应用在文本信息检索领域。</p><p>朴素贝叶斯分类器的核心就是贝叶斯定理，而贝叶斯定理最早由托马斯·贝叶斯所发现。</p><p>贝叶斯于 1701 年至 1761 年间居住在英国，他出生于赫特福德郡，1719 年至 1722 年间就读于爱丁堡大学，学习逻辑学和神学。</p><p>1734 年搬到肯特后，他继承了家族传统并一直担任西恩山教堂的牧师直到 1752 年。</p><p>贝叶斯在 30 多岁时只发表了两篇论文，一篇神学领域的，另一篇是数学领域的，并未产生多大影响。作为长老会牧师，他继续过着简朴的生活。</p><p>直到贝叶斯去世后，他的朋友理查德·普莱斯收集并发表了贝叶斯有关概率的研究内容。普莱斯非常重视贝叶斯的想法，并对遗留手稿进行编辑、纠正、阐述说明，在 1763 年发表文章“An Essay towards solving X Problem in the Doctine of Chances”。可以说，没有普莱斯的工作，贝叶斯理论将不会闻名于世。</p><p>但将贝叶斯理论推广开来，最终成为贝叶斯定理的是法国人拉普拉斯。</p><p>补充一点，理论（theory）和定理（theorem）是不同的。前者是一种可验证但尚待证明（或目前在文明的段时间跨度内无法证明）的解释或陈述，后者是由公理证明的结果（被普遍接受为真理的数学逻辑或定律）。</p><h2 id="2-基本原理"><a href="#2-基本原理" class="headerlink" title="2. 基本原理"></a>2. 基本原理</h2><p>假设训练数据集：</p><script type="math/tex; mode=display">T=\{(\mathbf x_1,y_1),(\mathbf x_2,y_2),...,(\mathbf x_N,y_N)\}</script><p>其中，${\displaystyle \mathbf x_i\in \mathcal{X}\subseteq R^n}$ 表示实例的特征向量，$y\in \mathcal{Y}=\{c_1,c_2,\cdots,c_k\}$ 表示实例的类别，也就是标记（label）。给定实例特征向量$\mathbf x$，输出所属的类 $y$ 。</p><p>从概率的角度来看，$X$ 是定义在输入空间 $\mathcal{X}$ 上的随机向量，$Y$ 是定义在输出空间 $\mathcal{Y}$ 上的随机变量。$P(X,Y)$ 是 $X$ 和 $Y$ 的联合概率分布。因此，训练数据集就是由 $P(X,Y)$ 独立同分布产生的。</p><p>我们再简单回顾一下贝叶斯定理。贝叶斯定理实际是由条件概率和全概率公式得到的。</p><p>从两个事件的条件概率开始，在 $y$ 事件已经发生的情况下，事件 $x$ 发生的概率：</p><script type="math/tex; mode=display">P(x|y)=\frac{P(xy)}{P(y)}</script><p>而在事件 $x$ 已经发生，而 $y$ 事件发生的概率：</p><script type="math/tex; mode=display">P(y|x)=\frac{P(xy)}{P(x)}</script><p>联立两个公式就可以得到：</p><script type="math/tex; mode=display">P(y|x)P(x) = P(x|y)P(y)</script><script type="math/tex; mode=display">P(y|x) = \frac{P(x|y)P(y)}{P(x)}</script><p>在机器学习的场景中，$P(x|y)$ 就是当标签是 $y$ 的情况下，特征是 $x$ 的概率。在现实情况中，$P(x|y)$、$P(y)$、$P(x)$ 是可以求得的，这样就可以计算在某个特征的情况下，属于哪个类标签的概率 $P(y|x)$。这就是贝叶斯方程。</p><p>$P(x)$ 可以通过全概率公式进一步分解</p><script type="math/tex; mode=display">\begin{aligned}    P(x)&= P(xy)+P(x\overline{y})\\    &= P(x|y)P(y)+P(x\overline{y})P(\overline{y})\\\end{aligned}</script><p>如果标签的空间不仅仅分为 $y$ 和 $\overline{y}$，而是多个分割，那就可以得到更一般的贝叶斯公式</p><p><strong>（贝叶斯公式）</strong>　设 $y_1,y_2,\cdots,y_n$ 是基本空间 $\Omega$ 的一个分割，且它们各自概率 $P(y_1),P(y_2),\cdots,P(y_n)$ 皆已知且为正，又设 $x$ 是 $\Omega$ 中的一个事件，$P(x)&gt;0$，且在诸 $y_i$ 下事件 $x$ 的条件概率 $P(x|y_1),P(x|y_2),\cdots,P(x|y_n)$ 可通过试验等手段获得，则在 $x$ 给定下，事件 $y_k$ 的条件概率为</p><script type="math/tex; mode=display">P(y_k|x)=\frac{P(x|y_k)P(y_k)}{\sum\limits^n_{i=1}P(x|y_i)P(y_i)},k=1,2,\cdots,n</script><p>上述公式是在一个特征 $x$ 的情况下，如果是多个特征，则需要求这多个特征的条件概率分布。</p><script type="math/tex; mode=display">P(X=\mathbf{x}|Y=c_k)=P(X^{(1)}=x^{(1)},\cdots,X^{(n)}=x^{(n)}|Y=c_k),k=1,2,\cdots,K</script><p>条件联合概率分布 $P(X=\mathbf{x}|Y=c_k)$ 有指数级数量的参数，假设 $x^{(j)}$ 可取值有 $N_j$ 个，$j=1,2,\cdots,n$，Y可取值有 $K$ 个，那么参数个数为 $K\prod \limits_{j=1}^nN_j$。如果一个特征空间和类别都是二元的，维度是10，那么需要估计的参数就有2048个，平均每个参数需要100个观测值的话，那就需要20万组数据，随着特征维度的增加，这个值还会随指数增长。</p><p>除此之外，如果数据量不够大的情况下，由于数据的稀疏性，很容易统计到某个参数估计为0的情况，这也是不对的。</p><blockquote><p>也就是不同特征组合情况很多，可能很多组合下都没有样本实例，不能简单认为这种可能性就不存在。</p></blockquote><p>朴素贝叶斯在求解条件概率分布时做了一个非常强的假设，条件独立性（conditional independence）。它的意思是在给定的类别下，不同维度特征的取值之间是相互独立的。朴素贝叶斯也因此得名。</p><script type="math/tex; mode=display">\begin{aligned}    P(X=\mathbf{x}|Y=c_k)&=P(X^{(1)}=x^{(1)},\cdots,X^{(n)}=x^{(n)}|Y=c_k),k=1,2,\cdots,K\\    &= \prod \limits_{j=1}^n P(X^{(j)}=x^{(j)}|Y=c_k)\\\end{aligned}</script><p>朴素贝叶斯法实际上学习到生成数据的机制，所以属于生成模型。条件独立假设等于是说用于分类的特征在类确定的条件下都是条件独立的。这一假设使朴素贝叶斯法变得简单，但有时会牺牲一定的分类准确率。</p><p>将上式带入贝叶斯公式，可以得到后验概率分布 $P(Y=c_k|X=\mathbf{x})$</p><script type="math/tex; mode=display">P(Y=c_k|X=\mathbf{x})=\frac{P(Y=c_k)\prod \limits_{j=1}^n P(X^{(j)}=x^{(j)}|Y=c_k)}{\sum\limits_k P(Y=c_k)\prod \limits_{j=1}^n P(X^{(j)}=x^{(j)}|Y=c_k)},k=1,2,\cdots,n</script><p>朴素贝叶斯实际是一个概率分类器，求出在不同类别下某一特征的概率分布，找到最大后验概率的类别即可。同时上式中的分母实际就是 $P(X=\mathbf{x})$，可以直接求得，并且由于同一特征分母均相同，可以省略分母，就是只比较分子的大小，选出最大的即可。</p><script type="math/tex; mode=display">\begin{aligned}    \hat{y}&=\arg\max_{c_k}P(Y=c_k|X=\mathbf{x})\\    &= \arg\max_{c_k} \frac{P(Y=c_k)\prod \limits_{j=1}^n P(X^{(j)}=x^{(j)}|Y=c_k)}{\sum\limits_k P(Y=c_k)\prod \limits_{j=1}^n P(X^{(j)}=x^{(j)}|Y=c_k)}\\    &= \arg\max_{c_k}  P(Y=c_k)\prod \limits_{j=1}^n P(X^{(j)}=x^{(j)}|Y=c_k)\end{aligned}</script><h2 id="3-最大化含义"><a href="#3-最大化含义" class="headerlink" title="3. 最大化含义"></a>3. 最大化含义</h2><p>朴素贝叶斯法将实例分到后验概率最大的类。这等价于期望风险最小化。假设损失函数：</p><script type="math/tex; mode=display">{\displaystyle L(Y,f(X))={    \begin{cases}        +1&Y\ne f(X)\\        -1&Y=f(X)    \end{cases}    }}</script><p>式中 $f(X)$ 是分类决策函数。这时，期望风险函数为</p><script type="math/tex; mode=display">R_{\exp}(f)=E[L(Y,f(X))]</script><p>期望是对联合分布 $P(X,Y)$ 取的。由此取条件期望</p><script type="math/tex; mode=display">R_{\exp}(f)=E_X\sum^{K}_{k=1}[L(c_k,f(X))]P(c_k|X)</script><p>为了使期望风险最小化，需要对 $X=x$ 逐个极小化，由此得到：</p><script type="math/tex; mode=display">\begin{aligned}    f(x)&=\arg\min_{y\in \mathcal Y}\sum^K_{k=1}L(c_k,y)P(c_k|X=x)\\    &=\arg\min_{y\in \mathcal Y}\sum^K_{k=1}P(y\not ={c_k}|X=x)\\    &=\arg\min_{y\in \mathcal Y}(1-P(y=c_k|X=x))\\    &=\arg\max_{y\in \mathcal Y}P(y=c_k|X=x)\end{aligned}</script><p>这样一来，根据期望风险最小化准则就得到了后验概率最大化准则：</p><script type="math/tex; mode=display">f(x)=\arg\max_{c_k}P(c_k|X=x)</script><h2 id="4-参数估计"><a href="#4-参数估计" class="headerlink" title="4. 参数估计"></a>4. 参数估计</h2><p>以上可以知道要对哪些参数需要估计，那么具体如何对参数进行估计？可以用到极大似然估计或者贝叶斯估计。</p><p>需要估计的参数有先验概率 $P(Y=c_k)$ 、条件概率 $P(X^{(j)}=a_{jl}|Y=c_k)$。</p><p>先验概率 $P(Y=c_k)$ 的极大似然估计：</p><script type="math/tex; mode=display">P(Y=c_k)=\frac{\sum\limits^N_{i=1}I(y_i=c_k)}{N},k=1,2,\cdots,K</script><p>条件概率 $P(X^{(j)}=a_{jl}|Y=c_k)$ 的极大似然估计：</p><script type="math/tex; mode=display">P(X^{(j)}=a_{jl}|Y=c_k)=\frac{\sum\limits^N_{i=1}I(x_i^{(j)}=a_{jl},y_i=c_k)}{\sum\limits^N_{i=1}I(y_i=c_k)}</script><p>式中，$x_i^{(j)}$ 是样本的第 $j$ 个特征，$a_{jl}$ 是 $x^{(j)}$ 可能的取的第 $l$ 个值，$I$ 为指示函数，也就是计数。</p><p>用极大似然估计可能会出现所要估计的概率值为 $0$ 的情况，这会影响到后验概率的计算结果，使分类产生偏差。解决方法是采用贝叶斯估计：</p><script type="math/tex; mode=display">P(X^{(j)}=a_{jl}|Y=c_k)=\frac{\sum\limits^N_{i=1}I(x_i^{(j)}=a_{jl},y_i=c_k)+\lambda}{\sum\limits^N_{i=1}I(y_i=c_k)+S_j\lambda}</script><p>式中，$\lambda \ge 0$ ， $S_j$ 是 $l$ 的最大取值，即 第 $j$ 个特征的可选分类数量。</p><p>上式相当于在随机变量各个取值的频数上赋予一个正数 $\lambda&gt;0$ 。当 $\lambda=0$ 时就是极大似然估计。当 $\lambda=1$ 时，称为拉普拉斯平滑（Laplacian smoothing）。显然，对于任何 $l=1,2,\cdots,S_j$ ， $k=1,2,\cdots,K$ ，都满足</p><script type="math/tex; mode=display">P_\lambda(X^{(j)}=a_{jl}|Y=c_k)>0</script><script type="math/tex; mode=display">\sum^{S_j}_{l=1}P(X^{(j)}=a_{jl}|Y=c_k)=1</script><p>满足概率的公理化定义，表明条件概率 $P_\lambda(X^{(j)}=a_{jl}|Y=c_k)$ 是一种概率分布。</p><p>同样，先验概率的贝叶斯估计是</p><script type="math/tex; mode=display">P_\lambda(Y=c_k)=\frac{\sum\limits^N_{i=1}I(y_i=c_k)+\lambda}{N+K\lambda},k=1,2,\cdots,K</script><h2 id="5-算法流程"><a href="#5-算法流程" class="headerlink" title="5. 算法流程"></a>5. 算法流程</h2><p>以下给出朴素贝叶斯的完整算法。有些时候朴素贝叶斯计算会把后验概率映射到对数空间进行计算，这样做的好处是避免值太小以及加快计算速度（avoid underflow and increase speed）。</p><p><strong>算法输入：</strong> 训练数据集 $T$ 和实例 $\mathbf x$。</p><script type="math/tex; mode=display">T=\{(\mathbf x_1,y_1),(\mathbf x_2,y_2),...,(\mathbf x_N,y_N)\}</script><p>其中 $\mathbf x_i=(x_i^{(1)},x_i^{(2)},\cdots,x_i^{(n)})^T$，$x_i^{(j)}$ 是第 $i$ 个样本的第 $j$ 个特征，$x_i^{(j)}\in\{a_{j1},a_{j2},\cdots,a_{jS_j}\}$，$a_{jl}$ 是第 $j$ 个特征可能取的第 $l$ 个值，$j=1,2,\cdots,n$，$l=1,2,\cdots,S_j$，$y_i\in\{c_1,c_2,\cdots,c_K\}$</p><p><strong>算法输出：</strong> 实例 $\mathbf x$ 的分类。</p><p><strong>算法流程：</strong></p><ol><li>计算先验概率及条件概率<script type="math/tex; mode=display">P(Y=c_k)=\frac{\sum\limits^N_{i=1}I(y_i=c_k)}{N},k=1,2,\cdots,K</script><script type="math/tex; mode=display">P(X^{(j)}=a_{jl}|Y=c_k)=\frac{\sum\limits^N_{i=1}I(x_i^{(j)}=a_{jl},y_i=c_k)}{\sum\limits^N_{i=1}I(y_i=c_k)}</script></li><li>对于给定的实例 $\mathbf{x}=(x^{(1)},x^{(2)},\cdots,x^{(n)})^T$，计算映射到对数空间的后验概率<script type="math/tex; mode=display">P(Y=c_k|X=\mathbf{x})=\log P(Y=c_k)+\sum_{j=1}^n\log P(X^{(j)}=x^{(j)}|Y=c_k),k=1,2,\cdots,K</script></li><li>确定实例 $\mathbf{x}$ 的类别<script type="math/tex; mode=display">y=\arg\max_{c_k} P(Y=c_k|X=\mathbf{x})</script></li></ol><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><blockquote><p><a href="https://www.cnblogs.com/bonheur/p/12469873.html">1. 机器学习之朴素贝叶斯详解-cnblog</a><br><a href="https://holypython.com/nbc/naive-bayes-classifier-history/">2. Naive Bayes Classifier History-holy python</a><br><a href="https://www.zhihu.com/question/20138060">3. 朴素贝叶斯分类器和一般的贝叶斯分类器有什么区别？-zhihu</a><br><a href="https://web.stanford.edu/~jurafsky/slp3/4.pdf">4. Naive Bayes and Sentiment Classification - Stanford University</a></p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;span id=&quot;more&quot;&gt;&lt;/span&gt;
&lt;h1 id=&quot;朴素贝叶斯分类器&quot;&gt;&lt;a href=&quot;#朴素贝叶斯分类器&quot; class=&quot;headerlink&quot; title=&quot;朴素贝叶斯分类器&quot;&gt;&lt;/a&gt;朴素贝叶斯分类器&lt;/h1&gt;&lt;p&gt;朴素贝叶斯分类器是基于贝叶斯定理与特征条件独</summary>
      
    
    
    
    <category term="Machine learning" scheme="http://example.com/categories/Machine-learning/"/>
    
    
  </entry>
  
  <entry>
    <title>3. K-Nearest Neighbors</title>
    <link href="http://example.com/2021/11/09/MachineLearning/3.K-NearestNeighbors/"/>
    <id>http://example.com/2021/11/09/MachineLearning/3.K-NearestNeighbors/</id>
    <published>2021-11-08T16:00:00.000Z</published>
    <updated>2022-01-14T07:12:50.159Z</updated>
    
    <content type="html"><![CDATA[<span id="more"></span><h1 id="k近邻算法"><a href="#k近邻算法" class="headerlink" title="k近邻算法"></a>k近邻算法</h1><p>k近邻算法是“最简单”的监督机器学习算法之一，并且在上个世纪在模式识别领域得到了广泛的研究。虽然现在 kNN 不像以前那样流行，但在实践中仍然被广泛使用。kNN 算法在分类项目中可以作为更复杂模型的预测性能基准。</p><h2 id="1-历史背景"><a href="#1-历史背景" class="headerlink" title="1. 历史背景"></a>1. 历史背景</h2><p>Evelyn Fix(1904-1965) 是一位数学家/统计学家，在伯克利攻读博士学位并继续在那里教授统计学。</p><p>Joseph Lawson Hodges Jr.(1922-2000) 也是伯克利的一名统计学家，并从1944年开始参与与美国空军第二十空军（Twentieth Air Force of USAF）的统计合作。</p><p>这两位天才在1951年为美国空军制作的一份技术分析报告中相遇，在那里他们引入了一种非参数分类方法（判别分析）。他们从未正式发表过这篇论文，可能是因为所涉及的工作性质和保密性，特别是考虑到二战后不久的全球气氛。</p><p>接下来，Thomas Cover 和 Peter Hart 在1967年证明了 kNN 分类的上限错误率<a href="#refer-anchor-1"><sup>[11]</sup></a>。</p><h2 id="2-算法模型"><a href="#2-算法模型" class="headerlink" title="2. 算法模型"></a>2. 算法模型</h2><p>kNN 算法在某些条件下是一个通用的函数逼近器，但潜在的概念相对简单。kNN 是一种监督学习算法，它在训练阶段简单地存储标记的训练样本。因此，kNN 也被称为惰性学习算法，它对训练样本的处理推迟到做预测的时候才进行。  </p><p>假设训练数据集：</p><script type="math/tex; mode=display">T=\{(\mathbf x_,y_1),(\mathbf x_2,y_2),...,(\mathbf x_N,y_N)\}</script><p>其中，${\displaystyle \mathbf x_i\in X\subseteq R^n}$ 表示实例的特征向量，$y\in Y$ 表示实例的类别。给定实例特征向量$\mathbf x$，输出所属的类 $y$ 。</p><p>具体过程：</p><ol><li>通过给定的距离度量，在训练集 $T$ 中找出与 $\mathbf x$ 最近邻的 $k$ 个点，涵盖这 $k$ 个点的 $\mathbf x$ 的领域记作 $N_k(\mathbf x)$</li><li>随后在 $N_k(\mathbf x)$ 中根据分类决策规则决定 $\mathbf x$ 的类别 $y$。</li></ol><p>可以看出，kNN 算法的主要三个要素分别为距离度量、$k$ 值和分类决策规则。</p><h3 id="2-1-距离度量"><a href="#2-1-距离度量" class="headerlink" title="2.1 距离度量"></a>2.1 距离度量</h3><p>距离度量有曼哈顿距离、欧式距离或更一般的闵式距离。</p><p>假设两个特征向量 $\mathbf{x}_i=(x_i^{(1)},x_i^{(2)},…,x_i^{(n)})^T$，$\mathbf{x}_j=(x_j^{(1)},x_j^{(2)},…,x_j^{(n)})^T$，$\mathbf{x}_i,\mathbf{x}_j$ 的 $L_p$距离定义为：</p><script type="math/tex; mode=display">L_p(\mathbf{x}_i,\mathbf{x}_j)=\left(\sum_{l=1}^{n} |x_i^{(l)} - x_j^{(l)}|^p \right)^{\frac{1}{p}}</script><p>当 $p=1$ 时，称为曼哈顿距离，即</p><script type="math/tex; mode=display">L_p(\mathbf{x}_i,\mathbf{x}_j)=\sum_{l=1}^{n} |x_i^{(l)} - x_j^{(l)}|</script><p>当 $p=2$ 时，称为欧式距离，即</p><script type="math/tex; mode=display">L_p(\mathbf{x}_i,\mathbf{x}_j)=\left(\sum_{l=1}^{n} |x_i^{(l)} - x_j^{(l)}|^2 \right)^{\frac{1}{2}}</script><p>当 $p=\infty$，它是各个坐标距离的最大值，即</p><script type="math/tex; mode=display">L_p(\mathbf{x}_i,\mathbf{x}_j)=\max_{l} |x_i^{(l)} - x_j^{(l)}|</script><p>下图给出了二维空间中不同 $p$ 值情况下，与原点距离为 $1$ 的图形。<br><img src="https://raw.githubusercontent.com/He1o/Cyrus_NoteBook/main/MachineLearning/3.K-NearestNeighbor/img/distance.svg" alt="Minkowski distance"></p><p><center>Minkowski distance</center></p><blockquote><p>范数即为特征向量到原点的距离，表征自身的长度</p></blockquote><h3 id="2-2-k-值的选择"><a href="#2-2-k-值的选择" class="headerlink" title="2.2 $k$ 值的选择"></a>2.2 $k$ 值的选择</h3><p>如果选择较小的 $k$ 值，相当于用较小的领域中的训练数据进行预测，近似误差（approximation error）会减少，只有与输入实例较近的训练数据才会对预测结果产生影响。但缺点是估计误差（estimation error）会增大，预测结果对近邻的实例点非常敏感，容易发生过拟合。当 $k$ 为1时，称为最近邻算法，对于输入实例，将与其最近的数据点的类作为预测结果。</p><p>相反的，当使用较大的 $k$ 值时，意味着距离输入实例较远的训练数据也会对预测结果产生影响，使预测产生错误，容易发生欠拟合。当 $k$ 为 $N$ 时，无论输入实例是什么，预测结果都将是训练数据中存在最多的类。</p><p>在应用中，$k$ 一般选取一个比较小的值，采用交叉验证法来选取最优的 $k$ 值。</p><h3 id="2-3-决策规则"><a href="#2-3-决策规则" class="headerlink" title="2.3 决策规则"></a>2.3 决策规则</h3><p>kNN算法在分类问题中决策规则往往是“多数表决”，即由输入实例的 $k$ 个近邻的训练实例中的多数类决定输入实例的类。事实上，多数表决（Majority vote）分为简单多数表决和特定多数表决，是要求满足一半数量以上或者特定数量，而不是占比最多的（Plurality vote），在二分类问题中两者没有区别，而在多分类问题中，不需要某一类投票数过半，超过 $N$ 分之一（$N$ 是分类总数）就可以预测了。</p><p>在回归问题中，可使用“平均法”，即将这 $k$ 个样本的实值输出标记的平均值作为预测结果，还可基于距离远近进行加权平均或加权投票，距离越近的样本权重越大。</p><h2 id="3-模型优化"><a href="#3-模型优化" class="headerlink" title="3. 模型优化"></a>3. 模型优化</h2><p>在考虑 kNN 算法时间复杂度之前，先看一下特征维度对算法的影响。</p><p>假设我们有 $100$ 个训练实例均匀分布在 $(0,1]$ 区间， 它们的间隔为 $0.01$ 单元。假设 $k$ 值为 $3$，就是找到查询点的三个最近邻，期望覆盖特征轴 $0.03$ 的范围。当我们增加一个维度的时候，总体分布在 $1\times 1$ 的区域中，为了覆盖相同的领域，需要 $0.03^{1/2}\approx 0.17$ 范围的坐标轴区域。当维度为 $10$ 的时候，这一数值 $0.03^{1/10}\approx 0.704=70.4\%$。可以发现，在高维中我们需要考虑很大的超体积来找到 $k$ 个近邻样本，这些点与查询点的距离相对较远，变得越来越不“相似”。</p><p>这种现象在机器学习中被称为维度灾难，指的是训练样本大小固定但维度的数量和每个维度的特征值范围不断增加的场景。</p><p>kNN 算法的时间复杂度是 $O(k\ast N\ast m)$ ，$N$ 是训练样本的数量，$m$ 是训练数据集的特征维度，由于 $N\gg m$，时间复杂度简化为 $O(k\ast N)$，可以看出时间复杂度较高，通过数据结构的方法可以将时间进行优化。</p><h3 id="3-1-堆优化"><a href="#3-1-堆优化" class="headerlink" title="3.1 堆优化"></a>3.1 堆优化</h3><p>最暴力的解法是重复 $k$ 次找到 $k$ 个最近邻的训练实例，通过堆优化可以将时间复杂度降到 $O(n\log(k))$。</p><p>随机选取 $k$ 个训练数据集的点来初始化查询点的堆。通过维护一个堆来保存距离查询点最近的 $k$ 个点。通过遍历数据集，如果该点到查询点的距离比堆中保存的最大的距离还要小，则从堆中剔除最远的点并插入当前点。一旦完成了训练数据集的一次迭代，我们就有了一组 $k$ 个最近邻的点。</p><h3 id="3-2-桶优化"><a href="#3-2-桶优化" class="headerlink" title="3.2 桶优化"></a>3.2 桶优化</h3><p>上面的优化方法在每次查询时都还是要对训练数据集进行遍历，如果要快速查询还是需要对数据的存储方式进行优化。</p><p>最简单的方法就是分桶（bucketing），我们将搜索空间划分为相同大小的单元格，类似于网格。</p><blockquote><p>突然发现，工作中要把热力点匹配到距离最近的道路点上，同事就是用的种方法，取热力点所在单元格周围的九个格子中的link，再将取到的道路点和热力点进行匹配。</p></blockquote><h3 id="3-3-KD-树优化"><a href="#3-3-KD-树优化" class="headerlink" title="3.3 KD-树优化"></a>3.3 KD-树优化</h3><p>找到 $k$ 个最近邻的点，本质还是搜索，而提高搜索效率很自然的就会想到二叉树，KD-树就是二叉搜索树（BST）的一种推广。KD-树的时间复杂度平均为 $O(\log(N))$ ，它在笛卡尔坐标系中垂直于特征轴划分搜索空间，这在较低的维度上表现较好，随着特征轴的增多，KD-树将变得低效。</p><p><strong>KD-树构建</strong></p><p>构造KD-树相当于不断用垂直于坐标轴的超平面将k维空间切分，构造一系列的k维超矩形区域。KD-树的每个结点存储一个训练实例点，每一颗子树对应于一个k维超矩形空间。</p><p>二叉搜索树是KD-树在一维空间的特例，任何一个节点相当于一个分割点，左边的比它小，右边的所有节点比它大。在二维空间，一个节点就相当于一条分割线，以此类推，三维空间就是一个面。这种分割方式意味着垂直于一条坐标轴进行分割。分割维度的方式是通过二叉树的深度，第一层根结点分割x轴，第二层分割y轴，以此类推，如果维度遍历完了，下一层又回到x轴，不断重复。</p><p>KD-树构建步骤：</p><ol><li>初始化数据集 $T=\{x_1, x_2,…,x_N\}$，KD树深度 $j=1$，数据集的维度为 $k$。</li><li>选择第 $l=j\bmod k + 1$ 维进行分割，找到数据集中所有实例的第 $l$ 维坐标的中位数，把该点作为切分点。</li><li>（可选）不按照固定顺序选择切割维度，而是选取方差最大特征作为分割特征。</li><li>把切分点记录到KD-树节点上，把数据集中该特征值小于中位数的传递给左子树，把大于中位数的传递给右子树。</li><li>递归执行步骤2-4，直到所有数据都被建立到KD-树节点上。</li></ol><p><strong>KD-树搜索</strong></p><p>KD-树构建完成之后，每次进行查找最近邻时就可以通过KD-树进行查找，类似于二分查找，但由于存在每一层只有一个节点的树形情况，时间复杂度介于 $O(\log(N))$、$O(N)$。相较于二叉搜索树，KD-树多了一个回溯的过程。</p><p>KD-树搜索步骤：</p><ol><li>从根节点起始，根据当前节点的分割维度查看查询点相应的特征值，与节点同一维度的特征值对比，根据大小相应的向左或向右移动。</li><li>当移动到叶子结点时，记录当前节点为“当前最近点”。</li><li>回溯，再从叶子结点返回根节点。</li><li>如果当前节点比当前最近点距离查询点更近，则记录当前节点为“当前最近点”。</li><li>判断查询点到当前节点的父节点所在的将数据集分割为两部分的超平面的距离，如果该距离比到“当前最近点”的距离要小，意味着兄弟节点所在的子树中可能包含更近的点。进入到兄弟节点，重复进行1-4步骤。</li><li>当回溯到根节点时，结束搜索。</li></ol><p>用查询点到“当前最近点”的距离可以形成一个圆或者一个超球体，如果到超平面的距离更小，意味着超球体与另一半空间相交，那么另一半空间可能存在比当前最近点更近的点，否则的话，另一半空间一定不存在更近的点，可以略去搜索。</p><h2 id="4-KD-树实现"><a href="#4-KD-树实现" class="headerlink" title="4. KD-树实现"></a>4. KD-树实现</h2><p><strong>KD-树的构建</strong></p><pre><code class="lang-python">import numpy as npclass Node():    def __init__(self, feature):        self.father = None        self.feature = feature        self.left = None        self.right = None        self.divide = None    @property    def brother(self):        if self == self.father.left:            return self.father.right        else:            return self.father.left    def __str__(self):        return &#39;feature: &#123;&#125;&#39;.format(self.feature)class KDTree():    def __init__(self, points):        self.root = self.build_tree(points)    def build_tree(self, points, dim = 0, father = None):        if not points:            return None        points = sorted(points, key = lambda x: x[dim])        mid = len(points) // 2        curNode = Node(points[mid])        curNode.divide = dim        curNode.father = father        curNode.left  = self.build_tree(points[:mid], (dim + 1) % len(points[0]), curNode)        curNode.right = self.build_tree(points[mid + 1:], (dim + 1) % len(points[0]), curNode)        return curNode    def __str__(self):        def inorder(root, depth = 0):            if not root:                return            ret.append(&#39;depth: &#123;&#125;, &#123;&#125;&#39;.format(str(depth), str(root)))            inorder(root.left, depth + 1)            inorder(root.right, depth + 1)        ret = []        inorder(self.root)        return &#39;\n&#39;.join(ret)pnts = [[2,3], [5,4], [9,6], [4,7], [8,1], [7,2]]tree = KDTree(pnts)print(tree)</code></pre><blockquote><p>depth: 0, feature: [7, 2]<br>depth: 1, feature: [5, 4]<br>depth: 2, feature: [2, 3]<br>depth: 2, feature: [4, 7]<br>depth: 1, feature: [9, 6]<br>depth: 2, feature: [8, 1]</p></blockquote><p><strong>KD-树的搜索</strong></p><pre><code class="lang-python">    def _search(self, root, target):        if not root:            return None        if target[root.divide] &lt; root.feature[root.divide]:            res = self._search(root.left, target)        else:            res = self._search(root.right, target)        return res if res else root    def _get_distance(self, x, y):        return np.sqrt(np.sum((np.array(x) - np.array(y)) ** 2))    def _get_hyper_plane_distance(self, node, target):        return abs(target[node.divide] - node.feature[node.divide])    def nearest_neighbour_search(self, target, root):        nearest_node = self._search(root, target)        nearest_distance = self._get_distance(nearest_node.feature, target)        currnode = nearest_node        while currnode != root:            tempnode = currnode            currnode = currnode.father  #如果currnode没有父节点，它一定等于root，就不会进入这一层            if self._get_distance(currnode.feature, target) &lt; nearest_distance:                nearest_node = currnode                nearest_distance = self._get_distance(currnode.feature, target)            if self._get_hyper_plane_distance(currnode, target) &lt; nearest_distance:                bro_distance, bro_nearest_node = self.nearest_neighbour_search(target, tempnode.brother)                if bro_distance &lt; nearest_distance:                    nearest_node = bro_nearest_node                    nearest_distance = bro_distance        return nearest_distance, nearest_nodepnts = [[2,3], [5,4], [9,6], [4,7], [8,1], [7,2]]tree = KDTree(pnts)dis, node = tree.nearest_neighbour_search([6,2], tree.root)print(dis, node)</code></pre><blockquote><p>1.0 feature: [7, 2]</p></blockquote><h2 id="5-算例"><a href="#5-算例" class="headerlink" title="5. 算例"></a>5. 算例</h2><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><blockquote><p><a href="http://www.atyun.com/37601.html">1. K近邻算法KNN的简述</a><br><a href="https://zhuanlan.zhihu.com/p/110066200">2. KNN（K近邻算法）基本介绍</a><br><a href="https://blog.csdn.net/sinat_30353259/article/details/80901746">3. 机器学习之KNN（k近邻）算法详解</a><br><a href="https://zh.wikipedia.org/wiki/K-%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95">4. K-近邻算法-维基百科</a><br><a href="http://www.scholarpedia.org/article/K-nearest_neighbor">5. K-nearest neighbor</a><br><a href="https://zhuanlan.zhihu.com/p/45346117">6. KD Tree的原理及Python实现-李小文</a><br><a href="https://sebastianraschka.com/pdf/lecture-notes/stat479fs18/02_knn_notes.pdf">7. STAT 479: Machine Learning Lecture Notes</a><br><a href="https://zhuanlan.zhihu.com/p/23966698">8. 【数学】kd 树算法之详细篇</a><br><a href="https://zhuanlan.zhihu.com/p/53826008">9. KD树简介</a><br><a href="https://www.cnblogs.com/eyeszjwang/articles/2429382.html">10. k-d tree算法</a><br><a href="https://isl.stanford.edu/~cover/papers/transIT/0021cove.pdf">11. Nearest Neighbor Pattern Classification</a> <div id="refer-anchor-1"></div></p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;span id=&quot;more&quot;&gt;&lt;/span&gt;
&lt;h1 id=&quot;k近邻算法&quot;&gt;&lt;a href=&quot;#k近邻算法&quot; class=&quot;headerlink&quot; title=&quot;k近邻算法&quot;&gt;&lt;/a&gt;k近邻算法&lt;/h1&gt;&lt;p&gt;k近邻算法是“最简单”的监督机器学习算法之一，并且在上个世纪在模式识</summary>
      
    
    
    
    <category term="Machine learning" scheme="http://example.com/categories/Machine-learning/"/>
    
    
  </entry>
  
  <entry>
    <title>2. Perceptron</title>
    <link href="http://example.com/2021/10/10/MachineLearning/2.Perceptron/"/>
    <id>http://example.com/2021/10/10/MachineLearning/2.Perceptron/</id>
    <published>2021-10-09T16:00:00.000Z</published>
    <updated>2022-01-14T06:38:01.757Z</updated>
    
    <content type="html"><![CDATA[<span id="more"></span><h1 id="感知机"><a href="#感知机" class="headerlink" title="感知机"></a>感知机</h1><p>感知机（英语：Perceptron）是弗兰克·罗森布拉特（Frank Rosenblatt）在1957年就职于康奈尔航空实验室（Cornell Aeronautical Laboratory）时所发明的一种人工神经网络。在人工神经网络领域中，感知机也被指为单层的人工神经网络，以区别于较复杂的多层感知机（Multilayer Perceptron）。</p><p>感知机作为一种二分分类的线性分类模型，其输入为实例的特征向量，输出为实例的类别，取 $+1$ 或 $-1$ 二值。感知机学习旨在求出将训练数据进行线性划分的分离超平面，因此，导入基于误分类的损失函数，利用梯度下降法对损失函数进行极小化，求得感知机模型。（单层）感知机可说是最简单的前向人工神经网络形式。尽管结构简单，感知机能够学习并解决相当复杂的问题。感知机主要的本质缺陷是它不能处理线性不可分问题。</p><p>感知机是生物神经细胞的简单抽象。神经细胞结构大致可分为：树突、突触、细胞体及轴突。单个神经细胞可被视为一种只有两种状态的机器——激活时为“是”，而未激活时为“否”。神经细胞的状态取决于从其它的神经细胞收到的输入信号量，及突触的强度（抑制或加强）。当信号量总和超过了某个阈值时，细胞体就会激活，产生电脉冲。电脉冲沿着轴突并通过突触传递到其它神经元。为了模拟神经细胞行为，与之对应的感知机基础概念被提出，如权量（突触）、偏置（阈值）及激活函数（细胞体）。</p><h2 id="1-历史背景"><a href="#1-历史背景" class="headerlink" title="1. 历史背景"></a>1. 历史背景</h2><p>1943年，心理学家沃伦·麦卡洛克和数理逻辑学家沃尔特·皮茨在合作的《A logical calculus of the ideas immanent in nervous activity》论文中提出并给出了人工神经网络的概念及人工神经元的数学模型，从而开创了人工神经网络研究的时代。</p><p>1949年，心理学家唐纳德·赫布在《The Organization of Behavior》论文中描述了神经元学习法则——赫布型学习。</p><p>人工神经网络更进一步被美国神经学家弗兰克·罗森布拉特所发展。他提出了可以模拟人类感知能力的机器，并称之为“感知机”。1957年，在 Cornell 航空实验室中，他成功在 IBM 704 机上完成了感知机的仿真。两年后，他又成功实现了能够识别一些英文字母、基于感知机的神经计算机——Mark1，并于1960年6月23日，展示与众。</p><p>为了“教导”感知机识别图像，弗兰克·罗森布拉特在 Hebb 学习法则的基础上，发展了一种迭代、试错、类似于人类学习过程的学习算法——感知机学习。除了能够识别出现较多次的字母，感知机也能对不同书写方式的字母图像进行概括和归纳。但是，由于本身的局限，感知机除了那些包含在训练集里的图像以外，不能对受干扰（半遮蔽、不同大小、平移、旋转）的字母图像进行可靠的识别。</p><p>首个有关感知机的成果，由弗兰克·罗森布拉特于1958年发表在《The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain》的文章里。1962年，他又出版了《Principles of Neurodynamics: Perceptrons and the theory of brain mechanisms》一书，向大众深入解释感知机的理论知识及背景假设。此书介绍了一些重要的概念及定理证明，例如感知机收敛定理。</p><p>虽然最初被认为有着良好的发展潜能，但感知机最终被证明不能处理诸多的模式识别问题。1969年，马文·明斯基和西摩尔·派普特在《Perceptrons》书中，仔细分析了以感知机为代表的单层神经网络系统的功能及局限，证明感知机不能解决简单的异或（XOR）等线性不可分问题，但弗兰克·罗森布拉特和马文·明斯基和西摩尔·派普特等人在当时已经了解到多层神经网络能够解决线性不可分的问题。</p><p>由于弗兰克·罗森布拉特等人没能够及时推广感知机学习算法到多层神经网络上，又由于《Perceptrons》在研究领域中的巨大影响，及人们对书中论点的误解，造成了人工神经领域发展的长年停滞及低潮，直到人们认识到多层感知机没有单层感知机固有的缺陷及反向传播算法在80年代的提出，才有所恢复。1987年，书中的错误得到了校正，并更名再版为《Perceptrons - Expanded Edition》。</p><p>近年，在Freund及Schapire（1998）使用核技巧改进感知机学习算法之后，愈来愈多的人对感知机学习算法产生兴趣。后来的研究表明除了二元分类，感知机也能应用在较复杂、被称为structured learning类型的任务上（Collins, 2002），又或使用在分布式计算环境中的大规模机器学习问题上（McDonald, Hall and Mann, 2011）。</p><h2 id="2-感知机模型"><a href="#2-感知机模型" class="headerlink" title="2. 感知机模型"></a>2. 感知机模型</h2><p>假设输入空间（特征空间）是 ${\displaystyle X\subseteq R^n}$ （代表n维的实数空间），输出空间是 $Y=\{+1, -1\}$ 。输入 $\mathbf x\in X$ 表示实例的特征向量，对应于输入空间（特征空间）的点；输出 $\mathbf y\in Y$ 表示实例的类别。把矩阵上的输入 $x$ 映射到输出值$f(x)$上。</p><script type="math/tex; mode=display">{\displaystyle f(x)=\text{sign}(\mathbf w\cdot \mathbf x+b)}</script><p>$\mathbf w$ 是实数的表示权重的向量，与 $\mathbf x$ 维度相同，$\mathbf {w\cdot x}$ 是点积，$b$ 是偏置，一个不依赖于任何输入值的常数。偏置可以认为是激励函数的偏移量，或者给神经元一个基础活跃等级。sign是符号函数：</p><script type="math/tex; mode=display">{\displaystyle \text{sign}(n)={    \begin{cases}        +1&n\geq 0\\        -1&n<0    \end{cases}    }}</script><p>映射函数同样可以写成如下的表述形式：</p><script type="math/tex; mode=display">{\displaystyle y=\text{sign}(\sum_{i=1}^{n}{ {w}_{i}{x}_{i}+b})=\text{sign}( {W}^{T} {X})}</script><h2 id="3-感知机学习算法"><a href="#3-感知机学习算法" class="headerlink" title="3. 感知机学习算法"></a>3. 感知机学习算法</h2><p>假设训练数据是线性可分的，感知机的学习目标是寻找模型参数$w,b$来将数据集正负实例点通过超平面进行区分，因此需要一个损失函数并得到损失函数的极小值。</p><blockquote><p>我们很自然的想到能否还用最小二乘法中的损失函数呢？</p><script type="math/tex; mode=display">Q=min{||\text{sign}(\mathbf w\cdot \mathbf x+b)-y||}^2</script><p>感知机与最小二乘法的不同之处在于 $y$ 即映射函数 $f(x)$ 的不同。最小二乘法是线性函数，用于线性回归；而感知机是sign函数，用于分类。很明显sign是个阶跃的不连续函数，这就导致损失函数同样是不连续的，就无法通过微分来得到极值点。</p></blockquote><p>损失函数还有一个自然选择是误分类点的总数，但同样这种损失函数也不是参数 $\mathbf w,b$ 的连续可导函数。损失函数的另一个选择是误分类点到超平面的距离，通过距离公式可以得到任一点 $x_0$ 到超平面 $S$ 的距离：</p><script type="math/tex; mode=display">\frac{1}{\left\|\mathbf w\right\|}|\mathbf w\cdot x_0+b|</script><blockquote><p>在一维空间，一个线性函数如 $Ax+By+C=0$ 的法向量就是 $(A,B)$，点到直线距离相当于该点到法向量投影的长度，假设点 $Q$ 坐标 $(x_0,y_0)$，直线上任意一点坐标 $P(x,y)$</p><script type="math/tex; mode=display">d=|PQ|\cdot \cos\theta</script><script type="math/tex; mode=display">d=\frac{|n|\cdot|PQ|\cdot \cos\theta}{|n|}</script><script type="math/tex; mode=display">d=\frac{\vec{n}\cdot \vec{PQ}}{|n|}</script><p>同理可以推广到多维空间</p><p><strong>记住一点：</strong></p><p><strong>权重向量是超平面的法线</strong></p></blockquote><p>对于误分类的数据$(\mathbf x_i,y_i)$，当$\mathbf w\cdot \mathbf x_i+b&gt;0$，$y_i=-1$；反之，$y_i=+1$，因此任一点$x_i$到超平面$S$的距离是</p><script type="math/tex; mode=display">-\frac{1}{\left\|\mathbf w\right\|}y_i(\mathbf w\cdot \mathbf x_i+b)</script><p>不考虑$\displaystyle \frac{1}{\left|\mathbf w\right|}$， 感知机学习的损失函数定义为</p><script type="math/tex; mode=display">L(\mathbf w,b)=-\sum_{\mathbf x_i\in M}y_i(\mathbf w\cdot \mathbf x_i+b)</script><p>$M$ 是误分类点的集合。</p><p>显然，损失函数是非负的。如果没有误分类点，损失函数值是0。而且，误分类点越少，误分类点离超平面越近，损失函数值就越小。注意，在分类正确时，损失函数为0。因此，给定训练数据$T$，损失函数$L(\mathbf w,b)$ 是$\mathbf w,b$ 的连续可导函数。</p><p>感知机学习算法采用随机梯度下降法。首先选取一个超平面$\mathbf w_0,b_0$，然后用梯度下降法不断地极小化目标函数。极小化过程不是一次使$M$中所有误分类点的梯度下降，而是一次随机选取一个误分类点使其梯度下降。</p><p>损失函数 $L(\mathbf w,b)$ 的梯度：</p><script type="math/tex; mode=display">\bigtriangledown_\mathbf w L(\mathbf w,b)=-\sum_{\mathbf x_i\in M}y_i\mathbf x_i</script><script type="math/tex; mode=display">\bigtriangledown_b L(\mathbf w,b)=-\sum_{x_i\in M}y_i</script><p>随机选取一个误分类点$(x_i,y_i)$，对 $\mathbf w,b$ 进行更新：</p><script type="math/tex; mode=display">\mathbf w_{t+1}\leftarrow \mathbf w_{t}+ \eta y_i\mathbf x_i</script><script type="math/tex; mode=display">b_{t+1}\leftarrow b_{t}+ \eta y_i</script><p>式中$\eta$是步长，在统计学习中称为学习率。通过迭代可以期待损失函数不断减少。</p><p>为了便于推导，可以将偏置 $b$ 并入权重向量 $\mathbf w$，将 $d$ 维的输入转为 $(d+1)$ 维的输入，同时传入到对应的 $(d+1)$ 维感知机当中。</p><script type="math/tex; mode=display">\mathbf w\cdot x+b=[\mathbf w,b]\cdot [x,1]=\mathbf w\cdot \mathbf x</script><p>总结算法流程如下:</p><ol><li>初始化参数，迭代次数$t=1$并且$\mathbf w_1$为全是0的权重向量。</li><li>对每一个样本 $\mathbf x_t$ 预测，$f(x_t)=+1$ if $\mathbf w_t\cdot \mathbf x_t\ge 0$ else $-1$</li><li>对一个分类错误的样本 $y_i(\mathbf w_t \cdot \mathbf x_i) \le 0$ 进行学习，$\mathbf w_{t+1}\leftarrow \mathbf w_{t}+ \eta y_i\mathbf x_i$ </li><li>$t\leftarrow t + 1$</li></ol><h2 id="4-算法收敛性"><a href="#4-算法收敛性" class="headerlink" title="4. 算法收敛性"></a>4. 算法收敛性</h2><p>在证明感知机算法收敛性之前，先证明两个假设。</p><p><strong>定理1(线性可分性).</strong> 对于线性可分的数据集，存在 $\mathbf w^\ast$ 满足 $\left|\mathbf w^\ast \right|=1$ ,使得超平面 $\mathbf w^\ast \cdot \mathbf x=0$ 能够将所有训练数据完全正确分开，且存在 $\gamma &gt; 0$，对于所有 $i\in \{1,2,…,n\}$，</p><script type="math/tex; mode=display">y_i(\mathbf w^\ast \cdot \mathbf x_i) > \gamma</script><p>证明：由于训练数据集是线性可分的，则存在超平面将数据集完全分开，取此超平面为 $\mathbf w^\ast \cdot \mathbf x=0$，通过向量单位化使 $\left|\mathbf w^*\ast \right|=1$ 。由于对于有限的数据集 $i\in \{1,2,…,n\}$，均有</p><script type="math/tex; mode=display">y_i(\mathbf w^\ast \cdot \mathbf x_i) > 0</script><p>所以存在</p><script type="math/tex; mode=display">y_i(\mathbf w^\ast \cdot \mathbf x_i) \ge \min_i(y_i(\mathbf w^\ast \cdot \mathbf x_i)) > \gamma</script><p><strong>定理2(有界性).</strong> 存在 ${R \in R^n}$ 对于有限的数据集 $i\in \{1,2,…,n\}$，均有</p><script type="math/tex; mode=display">\left\|\mathbf x_i \right\| \le R</script><p><strong>定理3(收敛性).</strong> 感知机学习算法最多迭代次数（之后得到一个分离超平面），满足不等式：</p><script type="math/tex; mode=display">k\le \left( \frac{R}{\gamma}\right) ^2</script><p>证明：我们需要推导出 $k$ 的上边界是上式，策略是根据 $k$ 推导出 $\mathbf w_{k+1}$ 长度的上下限并将它们关联起来。</p><p>注意到 $\mathbf w_{1}=0$，对于 $k\ge1$ ，如果 $\mathbf{x}_j$ 是迭代 $k$ 期间的误分类点，由定理1，我们有:</p><script type="math/tex; mode=display">\begin{aligned}    \mathbf w_{k+1} \cdot \mathbf{w}^*&=(\mathbf{w}_k + \eta y_j\mathbf{x}_j)\cdot \mathbf{w}^* \\    &=\mathbf w_{k} \cdot \mathbf{w}^* + \eta y_j(\mathbf{x}_j \cdot \mathbf{w}^*) \\    &>\mathbf w_{k} \cdot \mathbf{w}^* + \eta \gamma \\    &>\mathbf w_{k-1} \cdot \mathbf{w}^* + 2\eta \gamma \\    &>k\eta \gamma\end{aligned}</script><p>由于</p><script type="math/tex; mode=display">\begin{aligned}\mathbf w_{k+1} \cdot \mathbf{w}^* &= \left\|\mathbf w_{k+1} \right\|\left\|\mathbf w^* \right\| \cos \theta \\&\le \left\|\mathbf w_{k+1} \right\| \left\|\mathbf w^* \right\| = \left\|\mathbf w_{k+1} \right\|\end{aligned}</script><p>我们有：</p><script type="math/tex; mode=display">\left\|\mathbf w_{k+1} \right\| >k\eta \gamma</script><p>至此我们得到了$\left|\mathbf w_{k+1} \right|$的下界，为了得到上界，我们推断：</p><script type="math/tex; mode=display">\begin{aligned}\left\|\mathbf w_{k+1} \right\|^2 &=\left\|\mathbf w_{k}+\eta y_j\mathbf{x}_j \right\|^2 \\&=\left\|\mathbf w_{k}\right\|^2 + \left\| \eta y_j\mathbf{x}_j \right\|^2 + 2(\mathbf w_{k}\cdot \mathbf x_{j})\eta y_j \\&=\left\|\mathbf w_{k}\right\|^2 + \left\| \eta \mathbf{x}_j \right\|^2 + 2(\mathbf w_{k}\cdot \mathbf x_{j})\eta y_j \\&\le \left\|\mathbf w_{k}\right\|^2 + \left\| \eta \mathbf{x}_j \right\|^2 \\&\le \left\|\mathbf w_{k}\right\|^2 + \eta ^2R^2 \\&\le \left\|\mathbf w_{k-1}\right\|^2 + 2\eta ^2R^2 \\&\le k\eta ^2R^2 \\\end{aligned}</script><p>联立$\left|\mathbf w_{k+1} \right|$的上下界，我们得到不等式：</p><script type="math/tex; mode=display">(k\eta \gamma)^2<\left\|\mathbf w_{k+1} \right\|\le k\eta ^2R^2</script><p>最终得到：</p><script type="math/tex; mode=display">k\le \left( \frac{R}{\gamma}\right) ^2</script><h2 id="5-算例"><a href="#5-算例" class="headerlink" title="5. 算例"></a>5. 算例</h2><pre><code class="lang-python">import numpy as npimport matplotlib.pyplot as pltclass Perceptron(object):    def __init__(self, train_data, lable):        self.data_pos = train_data        self.train_data = np.array(train_data)        self.lable = np.array(lable)        self.length = len(train_data)        self.train_data = np.append(self.train_data, np.ones((self.length, 1)), axis = 1)        self.weights = np.zeros(len(train_data[0]) + 1)        self.eta = 0.2    def sign(self, x):        return 1 if x &gt;= 0 else -1    def predict(self, input_data):        return self.sign(np.sum(self.weights * input_data))    def train(self):        for idx in range(100):            flag = True            for i, inpute_data in enumerate(self.train_data):                if self.lable[i] * self.predict(inpute_data) &lt;= 0:                    self.vectorA = self.eta * self.lable[i] * inpute_data                    Y = lambda x,y: (- y[2] - (x) * y[0]) / y[1]                    plt.cla()                    plt.figure().set_size_inches(6, 6)                    plt.style.use(&#39;Solarize_Light2&#39;)                    plt.subplots_adjust(top = 0.93, bottom = 0.07, right = 0.93, left = 0.07, hspace = 0, wspace = 0)                    plt.ylim((-5, 10))                    plt.xlim((-5, 10))                                   plt.scatter([x[0] for x in self.data_pos[:5]], [x[1] for x in self.data_pos[:5]], c=&#39;r&#39;)                    plt.scatter([x[0] for x in self.data_pos[5:]], [x[1] for x in self.data_pos[5:]], c=&#39;b&#39;)                    plt.scatter(inpute_data[0], inpute_data[1], c=&#39;k&#39;)                    plt.scatter(0, -self.weights[2] / self.weights[1], c=&#39;k&#39;)                    plt.plot([0, self.vectorA[0]], [Y(0, self.weights), Y(0, self.weights) + self.vectorA[1]])                    plt.plot([0, self.weights[0]], [Y(0, self.weights), Y(0, self.weights) + self.weights[1]])                    plt.plot([-5,10], [Y(-5, self.weights), Y(10, self.weights)])                    self.vectorB = self.weights + self.vectorA                    plt.plot([0, self.vectorB[0]], [Y(0, self.weights), Y(0, self.weights) + self.vectorB[1]])                    plt.plot([-5,10], [Y(-5, self.vectorB), Y(10, self.vectorB)])                    print(self.weights, self.eta * self.lable[i] * inpute_data, self.predict(inpute_data), self.lable[i])                    plt.savefig(&#39;img_&#123;&#125;_&#123;&#125;.jpg&#39;.format(idx,i))                    flag = False                    self.weights = self.vectorB                    # plt.pause(0.5)            if flag:                return self.weightstrain_data = [[2, 3], [1, 4], [3, 5], [2, 6], [4, 5], [3, 1], [4, 3], [6, 2], [2, 1]]lable = [1, 1, 1, 1, 1, -1, -1, -1 ,-1]plt.ion()ptron = Perceptron(train_data, lable)ptron.train()</code></pre><p><img src="https://raw.githubusercontent.com/He1o/Cyrus_NoteBook/main/MachineLearning/2.Perceptron/img/perceptron_iteration.gif" alt="感知机迭代过程图"></p><center>感知机迭代过程图</center><h2 id="5-思考"><a href="#5-思考" class="headerlink" title="5. 思考"></a>5. 思考</h2><p><img src="https://raw.githubusercontent.com/He1o/Cyrus_NoteBook/main/MachineLearning/2.Perceptron/img/img_1_0.jpg" alt="感知机迭代过程图1"></p><center>感知机迭代过程图1</center><p><img src="https://raw.githubusercontent.com/He1o/Cyrus_NoteBook/main/MachineLearning/2.Perceptron/img/img_1_6.jpg" alt="感知机迭代过程图2"></p><center>感知机迭代过程图2</center><p>对两个过程图进行进一步分析，以下为自己理解：</p><ol><li>不同的系数可能绘制出同一条直线或同一个平面，例如 $[1,1,1]$ 和 $[-1,-1,-1]$ 都可以表示直线 $x+y+1=0$，但权重代表的法向量的方向相反。如上述图中垂直于直线的即为法向量也就是 $w$，当实例点与法向量点乘之后再加上位移量 $b$，就可以用来判断该点在直线的哪一侧。原理有很多中解释，其中一种就是$\mathbf w\cdot\mathbf x$ 的正负决定了它们之间的夹角大小是否超过 $90$ 度，确定相对角度之后还不足以判断该点相对于直线的位置，</li><li>误分类的点改变的是直线法向量的方向和大小以及 $b$ 即直线的位置，误分类点也可以理解为一个向量，乘以衰减系数后作用在法向量上，作用可能是吸引或者排斥。当分类为正时为吸引作用，如图1，反之则为排斥作用如图2。前提都是错误的分类数据，即正分类点不会和法向量处在同一侧，因此正分类点希望法向量“转过来”，同理，负分类点会与法向量处于同一侧，它希望法向量“转过去”。</li></ol><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><blockquote><p><a href="https://zh.wikipedia.org/wiki/%E6%84%9F%E7%9F%A5%E5%99%A8">1. 感知器-维基百科</a><br><a href="https://www.cnblogs.com/graphics/archive/2010/07/10/1774809.html">2. 点到平面的距离公式</a><br><a href="https://blog.csdn.net/song430/article/details/88718602">3. 用Python实现单层感知机</a><br><a href="https://www.cnblogs.com/xym4869/p/11282469.html">4. 深度学习基础——感知机</a><br><a href="https://www.cnblogs.com/huangyc/p/9706575.html">5. 感知机原理</a><br><a href="https://github.com/SmallVagetable/machine_learning_python">6. machine_learning_python</a></p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;span id=&quot;more&quot;&gt;&lt;/span&gt;
&lt;h1 id=&quot;感知机&quot;&gt;&lt;a href=&quot;#感知机&quot; class=&quot;headerlink&quot; title=&quot;感知机&quot;&gt;&lt;/a&gt;感知机&lt;/h1&gt;&lt;p&gt;感知机（英语：Perceptron）是弗兰克·罗森布拉特（Frank Rosenbl</summary>
      
    
    
    
    <category term="Machine learning" scheme="http://example.com/categories/Machine-learning/"/>
    
    
  </entry>
  
  <entry>
    <title>1. Least Sqaure</title>
    <link href="http://example.com/2021/09/18/MachineLearning/1.LeastSqaure/"/>
    <id>http://example.com/2021/09/18/MachineLearning/1.LeastSqaure/</id>
    <published>2021-09-17T16:00:00.000Z</published>
    <updated>2022-01-14T06:31:41.712Z</updated>
    
    <content type="html"><![CDATA[<span id="more"></span><h1 id="最小二乘法"><a href="#最小二乘法" class="headerlink" title="最小二乘法"></a>最小二乘法</h1><p>最小二乘法又称为最小平方法，是一种数学优化建模方法。它通过最小化误差的平方和寻找数据的最佳函数匹配。最小二乘法是对线性方程组，即方程个数比未知数更多的方程组，以回归分析求得近似解的标准方法。在这个解决方案中，最小二乘法演算为每一方程的结果中，将残差平方和的总和最小化。</p><p>最小二乘法分为线性和非线性，取决于所有未知数中的残差是否为线性。线性的最小二乘问题发生在统计回归分析中，具有一个封闭形式的解决方案。非线性的问题通常由迭代细致化来解决，每次迭代中，系统由线性近似，因此这两种情况下核心演算是相同的。</p><h2 id="1-历史背景"><a href="#1-历史背景" class="headerlink" title="1. 历史背景"></a>1. 历史背景</h2><p>最小二乘法发展于天文学和大地测量学领域，科学家和数学家尝试为大航海探索时期的海洋航行挑战提供解决方案。准确描述天体的行为是船舰在大海洋上航行的关键，水手不能再依靠陆上目标导航作航行。</p><p>这个方法是在十八世纪期间一些进步的集大成：</p><ul><li>不同观测值的组合是真实值的最佳估计；多次观测会减少误差而不是增加，也许在1722年由Roger Cotes首先阐明。</li><li>在相同条件下采取的不同观察结果，与只尝试记录一次最精确的观察结果是对立的。这个方法被称为平均值方法。托马斯·马耶尔（Tobias Mayer）在1750年研究月球的天平动时，特别使用这种方法，而拉普拉斯（Pierre-Simon Laplace）在1788年他的工作成果中以此解释木星和土星的运动差异。</li><li>在不同条件下进行的不同观测值组合。该方法被称为最小绝对偏差法，出现在Roger Joseph Boscovich在1757年他对地球形体的著名作品，而拉普拉斯在1799年也表示了同样的问题。</li><li>评定对误差达到最小的解决方案标准，拉普拉斯指明了误差的概率密度的数学形式，并定义了误差最小化的估计方法。为此，拉普拉斯使用了一双边对称的指数分布，现在称为拉普拉斯分布作为误差分布的模型，并将绝对偏差之和作为估计误差。他认为这是他最简单的假设，他期待得出算术平均值而成为最佳的估计。可相反地，他的估计是后验中位数。</li></ul><p>1801年，意大利天文学家朱塞普·皮亚齐发现了第一颗小行星谷神星。经过40天的追踪观测后，由于谷神星运行至太阳背后，使得皮亚齐失去了谷神星的位置。随后全世界的科学家利用皮亚齐的观测数据开始寻找谷神星，但是根据大多数人计算的结果来寻找谷神星都没有结果。当年24岁的高斯也计算了谷神星的轨道。奥地利天文学家海因里希·奥伯斯根据高斯计算出来的轨道重新发现了谷神星。</p><p>高斯使用的最小二乘法的方法发表于1809年他的著作《天体运动论》中，而法国科学家勒让德于1806年独立发现“最小二乘法”，但因不为世人所知而没没无闻。两人曾为谁最早创立最小二乘法原理发生争执。</p><p>1829年，高斯提供了最小二乘法的优化效果强于其他方法的证明，见高斯-马尔可夫定理。</p><h2 id="2-问题引入"><a href="#2-问题引入" class="headerlink" title="2. 问题引入"></a>2. 问题引入</h2><p>如果对同一目标例如手机宽度通过尺子测量长度，通过同一尺子不同测量员或者不同尺子同一测量员得到的结果都有可能不同，那么如何通过所有的测量结果来得到准确的真值呢？</p><p>假设测量结果分别为72.5mm、72.2mm、72.9mm、72.4mm、72.5mm。</p><p>只要做过初中物理实验的都知道，通常都会对同一实验进行多次重复操作，把得到的结果进行平均求和，最后的结果作为实验的准确结果</p><script type="math/tex; mode=display">\overline{x}=\frac{72.5+72.2+72.9+72.4+72.5}{5}=72.5</script><p>直觉告诉我们取平均值是正确的，那么这么做得依据是什么呢？</p><p>再比如，我们知道营业税税收总额与社会商品零售总额有关，假设收集了如下数据</p><div class="table-container"><table><thead><tr><th style="text-align:center">序号</th><th style="text-align:center">社会商品零售总额</th><th style="text-align:center">营业税税收总额</th></tr></thead><tbody><tr><td style="text-align:center">1</td><td style="text-align:center">142.08</td><td style="text-align:center">3.93</td></tr><tr><td style="text-align:center">2</td><td style="text-align:center">177.30</td><td style="text-align:center">5.96</td></tr><tr><td style="text-align:center">3</td><td style="text-align:center">204.68</td><td style="text-align:center">7.85</td></tr><tr><td style="text-align:center">4</td><td style="text-align:center">242.88</td><td style="text-align:center">9.82</td></tr><tr><td style="text-align:center">5</td><td style="text-align:center">316.24</td><td style="text-align:center">12.50</td></tr><tr><td style="text-align:center">6</td><td style="text-align:center">341.99</td><td style="text-align:center">15.55</td></tr><tr><td style="text-align:center">7</td><td style="text-align:center">332.69</td><td style="text-align:center">15.79</td></tr><tr><td style="text-align:center">8</td><td style="text-align:center">389.29</td><td style="text-align:center">16.39</td></tr><tr><td style="text-align:center">9</td><td style="text-align:center">453.40</td><td style="text-align:center">18.45</td></tr></tbody></table></div><p>如何通过这些已有数据在给定社会商品零售总额的情况下预测营业税税收总额呢？</p><h2 id="3-问题求解"><a href="#3-问题求解" class="headerlink" title="3. 问题求解"></a>3. 问题求解</h2><p>我们将第一个问题用公式进行描述，假设要猜测的真实值是$y$，测量值则为$y_i$，上述求平均公式则为：</p><script type="math/tex; mode=display">\displaystyle{\frac{\sum y_i}{n}}=y</script><p>公式可以转化为：</p><script type="math/tex; mode=display">\sum (y_i-y)=0</script><p>满足上式时，误差平方和取得最小值，因为导数为0</p><script type="math/tex; mode=display">\frac{d}{dy}S_{\epsilon^2}=\frac{d}{dy}\sum (y_i-y)^2=0</script><p>至此，我们明白求平均的实质是希望预测值与测量值的误差平方和最小，最小二乘法也就是最小平方法，二乘实际上也是日语中平方的意思。</p><p>第二个问题中，社会商品零售总额$x$是自变量，其值是可以控制或精确测量的，是非随机变量；营业税税收总额$y$是因变量，取值不能事先确定，是随机变量。假定它们具有线性相关关系，$y_i$的表达式：</p><script type="math/tex; mode=display">{\displaystyle{y_i=\beta_0+\beta_1x_i+\epsilon_i}}</script><p>其中$\beta_0,\beta_1$称为回归系数，由于它们未知，因此需要从收集到的数据出发进行估计，记为$\hat{\beta_0},\hat{\beta_1}$。</p><p>通过最小二乘法估计回归系数，目标是希望偏差平方和$Q$达到最小：</p><script type="math/tex; mode=display">Q(\beta_0,\beta_1)=\sum_{i=1}^n (y_i-\beta_0-\beta_1x_i)^2</script><p>由于$Q$是一个非负二次型，因此可通过令$Q$对$\beta_0,\beta_1$的偏导为零来求：</p><script type="math/tex; mode=display">\begin{cases}    {\displaystyle \frac{\partial Q}{\partial \beta_0}=-2\sum(y_i-\beta_0-\beta_1x_i)} \\    \\    {\displaystyle \frac{\partial Q}{\partial \beta_1}=-2\sum(y_i-\beta_0-\beta_1x_i)x_i}\end{cases}</script><p>经整理有</p><script type="math/tex; mode=display">\begin{cases}    {\displaystyle n\beta_0 + n\overline{x}\cdot\beta_1}=n\overline{y} \\    \\    {\displaystyle n\overline{x}\beta_0+\sum{x_i^2\beta_1}=\sum{x_iy_i}}\end{cases}</script><p>两式合并后</p><script type="math/tex; mode=display">\begin{aligned}    \left( \sum{x_i^2} -n\overline{x}^2 \right)\beta_1&=\sum{x_iy_i}-n\overline{x}\overline{y}\\    \left( \sum{x_i^2} + \sum{\overline{x}^2}-2n\overline{x}^2 \right)\beta_1&=\sum{x_iy_i}-\sum{x_i}\overline{y}\\    \left( \sum{x_i^2} + \sum{\overline{x}^2}-2\sum{x_i}\overline{x} \right)\beta_1&=\sum{x_iy_i}-\sum{x_i}\overline{y}+n\overline{x}\overline{y}-n\overline{x}\overline{y}\\    \left( \sum{(x_i -\overline{x})^2} \right)\beta_1&=\sum{x_iy_i}-\sum{x_i}\overline{y}+\sum{\overline{x}y_i}-\sum\overline{x}\overline{y}\\    \left( \sum{(x_i -\overline{x})^2} \right)\beta_1&=\sum{(x_i-\overline{x})(y_i-\overline{y})}\\    \beta_1&=\frac{cov(x,y)}{Var(x)}\end{aligned}</script><p>实际上 $\beta_1$ 是 $x$ 与 $y$ 的协方差与 $x$ 方差的商</p><p>通过以上的方法可以推导出更多特征的求解方法，通过高斯消元法可以求解多元线性方程组，求得解析解。</p><p>同时也可以通过矩阵法求解，将多个方程看做一个整体进行求解。</p><script type="math/tex; mode=display">{\displaystyle     {\begin{pmatrix}    1& x_{11}& \cdots & x_{1j}\cdots & x_{1q}\\    1& x_{21}& \cdots & x_{2j}\cdots & x_{2q}\\    \vdots \\    1& x_{i1}& \cdots & x_{ij}\cdots & x_{iq}\\    \vdots \\    1& x_{n1}& \cdots & x_{nj}\cdots & x_{nq}    \end{pmatrix}}     \cdot     {\begin{pmatrix}\beta_{0}\\\beta_{1}\\\beta_{2}\\\vdots \\\beta_{j}\\\vdots \\\beta_{q}\end{pmatrix}}=    {\begin{pmatrix}y_{1}\\y_{2}\\\vdots \\y_{i}\\\vdots \\y_{n}\end{pmatrix}}}</script><p>矩阵表达式为：</p><script type="math/tex; mode=display">    Q=min{||Xw-y||}^2</script><p>求 $w$ 的最小二乘估计，即求 $\frac{\partial Q}{\partial w}$ 的零点。其中 $y$ 是 $m\times 1$ 列向量，$X$ 是 $m\times n$ 矩阵，$w$是$n\times 1$列向量，$Q$是标量。</p><p>将向量模平方改写成向量与自身的内积：</p><script type="math/tex; mode=display">Q=(Xw-y)^T(Xw-y)</script><p>求微分：</p><script type="math/tex; mode=display">\begin{aligned}    dQ&=(Xdw)^T(Xw-y)+(Xw-y)^T(Xdw)\\    &=2(Xw-y)^T(Xdw)\end{aligned}</script><p>这里是因为两个向量的内积满足$u^Tv=v^Tu$。</p><p>导数与微分的关系式</p><script type="math/tex; mode=display">dQ={\frac{\partial Q}{\partial w}}^Tdw</script><p>得到</p><script type="math/tex; mode=display">{\frac{\partial Q}{\partial w}}=(2(Xw-y)^TX)^T=2X^T(Xw-y)=0</script><p>求解可得</p><script type="math/tex; mode=display">\begin{aligned}    X^TXw&=X^Ty\\    w&=(X^TX)^{-1}X^Ty\end{aligned}</script><h2 id="4-思考"><a href="#4-思考" class="headerlink" title="4. 思考"></a>4. 思考</h2><p>为什么损失函数不用误差的绝对值？</p><script type="math/tex; mode=display">Q(\beta_0,\beta_1)=\sum_{i=1}^n |y_i-\beta_0-\beta_1x_i|</script><p>网上有人说是太麻烦，但我觉得主要原因是<strong>绝对值函数连续但不可导</strong>，无法通过求导数的为零的极值点，也无法通过梯度下降法进行求解。</p><h2 id="5-算例"><a href="#5-算例" class="headerlink" title="5. 算例"></a>5. 算例</h2><pre><code class="lang-python">import matplotlib.pyplot as pltimport numpy as np%matplotlib inlinex = np.array([142.08, 177.30, 204.68, 242.88, 316.24, 332.69, 341.99, 389.29, 453.40])y = np.array([3.93,   5.96,   7.85,   9.82,   12.50,  15.79,  15.55,  16.39,  18.45])# 通过代数方法求解# numpy的协方差默认是样本方差，无偏的，自由度n-1，因此要加bias = Truebeta_0 = np.cov(x, y, bias = True)[0,1] / np.var(x)  beta_1 = np.sum(y) / 9 - np.sum(x) / 9 * beta_0print(beta_0, beta_1)# 通过公式计算，与上面相同# a = np.sum(np.multiply(x, y)) - np.sum(x) * np.sum(y) / 9# b = np.sum(np.multiply(x, x)) - np.sum(x) * np.sum(x) / 9</code></pre><blockquote><p>计算结果<br>0.04867773628668675 -2.2609874555936926</p></blockquote><pre><code class="lang-python"># 通过矩阵法实现最小二乘法def least_sqaure(X, Y):    return (X.T * X).I * X.T * Y# 生成多项式def polynomial(x, n):    X = np.mat(x)    X = np.append(np.ones((1, 9)), X, axis = 0)    for i in range(1, n):        X = np.append(X, np.mat(x**(i + 1)), axis = 0)    return X.TY = np.mat(y).T# 线性拟合X = polynomial(x, 1)beta = np.array(least_sqaure(X, Y)).flatten()[::-1]print(&#39;beta:&#39;, beta)plt.subplot(221)plt.plot(x, y, &#39;bo&#39;, label=&#39;noise&#39;)plt.plot(x, np.poly1d(beta)(x), label=&#39;fitted curve&#39;)# 二次拟合X = polynomial(x, 2)beta = np.array(least_sqaure(X, Y)).flatten()[::-1]print(&#39;beta:&#39;, beta)plt.subplot(222)plt.plot(x, y, &#39;bo&#39;, label=&#39;noise&#39;)plt.plot(x, np.poly1d(beta)(x), label=&#39;fitted curve&#39;)# 三次拟合X = polynomial(x, 3)beta = np.array(least_sqaure(X, Y)).flatten()[::-1]print(&#39;beta:&#39;, beta)plt.subplot(223)plt.plot(x, y, &#39;bo&#39;, label=&#39;noise&#39;)plt.plot(x, np.poly1d(beta)(x), label=&#39;fitted curve&#39;)# 六次拟合X = polynomial(x, 6)beta = np.array(least_sqaure(X, Y)).flatten()[::-1]print(&#39;beta:&#39;, beta)plt.subplot(224)plt.plot(x, y, &#39;bo&#39;, label=&#39;noise&#39;)plt.plot(x, np.poly1d(beta)(x), label=&#39;fitted curve&#39;)plt.show()</code></pre><p><img src="https://raw.githubusercontent.com/He1o/Cyrus_NoteBook/main/MachineLearning/1.LeastSquare/img/least_square.png" alt="最小二乘法拟合图"></p><center>最小二乘法拟合图</center><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><blockquote><p><a href="https://www.zhihu.com/question/37031188">1. 最小二乘法的本质是什么？</a><br><a href="https://www.cnblogs.com/pinard/p/5976811.html">2. 最小二乘法小结-刘建平Pinard</a><br><a href="https://zh.wikipedia.org/wiki/%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95">3. 最小二乘法-维基百科</a><br><a href="https://zhuanlan.zhihu.com/p/84133777">4. 最小二乘法详细推导 - 知乎</a><br><a href="https://zhuanlan.zhihu.com/p/89373759">5. 最小二乘法 - 知乎</a><br><a href="https://zhuanlan.zhihu.com/p/87582571">6. 矩阵形式下的最小二乘法推导 - 知乎</a><br><a href="https://zhuanlan.zhihu.com/p/24709748">7. 矩阵求导术（上）- 知乎</a></p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;span id=&quot;more&quot;&gt;&lt;/span&gt;
&lt;h1 id=&quot;最小二乘法&quot;&gt;&lt;a href=&quot;#最小二乘法&quot; class=&quot;headerlink&quot; title=&quot;最小二乘法&quot;&gt;&lt;/a&gt;最小二乘法&lt;/h1&gt;&lt;p&gt;最小二乘法又称为最小平方法，是一种数学优化建模方法。它通过最小化误差</summary>
      
    
    
    
    <category term="Machine learning" scheme="http://example.com/categories/Machine-learning/"/>
    
    
  </entry>
  
</feed>
