<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>He1o</title>
  
  
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2022-01-08T10:30:39.045Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>He1o</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>2. Perceptron</title>
    <link href="http://example.com/2021/10/10/MachineLearning/2.Perceptron/"/>
    <id>http://example.com/2021/10/10/MachineLearning/2.Perceptron/</id>
    <published>2021-10-09T16:00:00.000Z</published>
    <updated>2022-01-08T10:30:39.045Z</updated>
    
    <content type="html"><![CDATA[<span id="more"></span><h1 id="感知机"><a href="#感知机" class="headerlink" title="感知机"></a>感知机</h1><p>感知机（英语：Perceptron）是弗兰克·罗森布拉特（Frank Rosenblatt）在1957年就职于康奈尔航空实验室（Cornell Aeronautical Laboratory）时所发明的一种人工神经网络。在人工神经网络领域中，感知机也被指为单层的人工神经网络，以区别于较复杂的多层感知机（Multilayer Perceptron）。</p><p>感知机作为一种二分分类的线性分类模型，其输入为实例的特征向量，输出为实例的类别，取 $+1$ 或 $-1$ 二值。感知机学习旨在求出将训练数据进行线性划分的分离超平面，因此，导入基于误分类的损失函数，利用梯度下降法对损失函数进行极小化，求得感知机模型。（单层）感知机可说是最简单的前向人工神经网络形式。尽管结构简单，感知机能够学习并解决相当复杂的问题。感知机主要的本质缺陷是它不能处理线性不可分问题。</p><p>感知机是生物神经细胞的简单抽象。神经细胞结构大致可分为：树突、突触、细胞体及轴突。单个神经细胞可被视为一种只有两种状态的机器——激活时为“是”，而未激活时为“否”。神经细胞的状态取决于从其它的神经细胞收到的输入信号量，及突触的强度（抑制或加强）。当信号量总和超过了某个阈值时，细胞体就会激活，产生电脉冲。电脉冲沿着轴突并通过突触传递到其它神经元。为了模拟神经细胞行为，与之对应的感知机基础概念被提出，如权量（突触）、偏置（阈值）及激活函数（细胞体）。</p><h2 id="1-历史背景"><a href="#1-历史背景" class="headerlink" title="1. 历史背景"></a>1. 历史背景</h2><p>1943年，心理学家沃伦·麦卡洛克和数理逻辑学家沃尔特·皮茨在合作的《A logical calculus of the ideas immanent in nervous activity》论文中提出并给出了人工神经网络的概念及人工神经元的数学模型，从而开创了人工神经网络研究的时代。</p><p>1949年，心理学家唐纳德·赫布在《The Organization of Behavior》论文中描述了神经元学习法则——赫布型学习。</p><p>人工神经网络更进一步被美国神经学家弗兰克·罗森布拉特所发展。他提出了可以模拟人类感知能力的机器，并称之为“感知机”。1957年，在 Cornell 航空实验室中，他成功在 IBM 704 机上完成了感知机的仿真。两年后，他又成功实现了能够识别一些英文字母、基于感知机的神经计算机——Mark1，并于1960年6月23日，展示与众。</p><p>为了“教导”感知机识别图像，弗兰克·罗森布拉特在 Hebb 学习法则的基础上，发展了一种迭代、试错、类似于人类学习过程的学习算法——感知机学习。除了能够识别出现较多次的字母，感知机也能对不同书写方式的字母图像进行概括和归纳。但是，由于本身的局限，感知机除了那些包含在训练集里的图像以外，不能对受干扰（半遮蔽、不同大小、平移、旋转）的字母图像进行可靠的识别。</p><p>首个有关感知机的成果，由弗兰克·罗森布拉特于1958年发表在《The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain》的文章里。1962年，他又出版了《Principles of Neurodynamics: Perceptrons and the theory of brain mechanisms》一书，向大众深入解释感知机的理论知识及背景假设。此书介绍了一些重要的概念及定理证明，例如感知机收敛定理。</p><p>虽然最初被认为有着良好的发展潜能，但感知机最终被证明不能处理诸多的模式识别问题。1969年，马文·明斯基和西摩尔·派普特在《Perceptrons》书中，仔细分析了以感知机为代表的单层神经网络系统的功能及局限，证明感知机不能解决简单的异或（XOR）等线性不可分问题，但弗兰克·罗森布拉特和马文·明斯基和西摩尔·派普特等人在当时已经了解到多层神经网络能够解决线性不可分的问题。</p><p>由于弗兰克·罗森布拉特等人没能够及时推广感知机学习算法到多层神经网络上，又由于《Perceptrons》在研究领域中的巨大影响，及人们对书中论点的误解，造成了人工神经领域发展的长年停滞及低潮，直到人们认识到多层感知机没有单层感知机固有的缺陷及反向传播算法在80年代的提出，才有所恢复。1987年，书中的错误得到了校正，并更名再版为《Perceptrons - Expanded Edition》。</p><p>近年，在Freund及Schapire（1998）使用核技巧改进感知机学习算法之后，愈来愈多的人对感知机学习算法产生兴趣。后来的研究表明除了二元分类，感知机也能应用在较复杂、被称为structured learning类型的任务上（Collins, 2002），又或使用在分布式计算环境中的大规模机器学习问题上（McDonald, Hall and Mann, 2011）。</p><h2 id="2-感知机模型"><a href="#2-感知机模型" class="headerlink" title="2. 感知机模型"></a>2. 感知机模型</h2><p>假设输入空间（特征空间）是 ${\displaystyle X\subseteq R^n}$ （代表n维的实数空间），输出空间是 $Y=\{+1, -1\}$ 。输入 $\mathbf x\in X$ 表示实例的特征向量，对应于输入空间（特征空间）的点；输出 $\mathbf y\in Y$ 表示实例的类别。把矩阵上的输入 $x$ 映射到输出值$f(x)$上。</p><script type="math/tex; mode=display">{\displaystyle f(x)=\text{sign}(\mathbf w\cdot \mathbf x+b)}</script><p>$\mathbf w$ 是实数的表示权重的向量，与 $\mathbf x$ 维度相同，$\mathbf {w\cdot x}$ 是点积，$b$ 是偏置，一个不依赖于任何输入值的常数。偏置可以认为是激励函数的偏移量，或者给神经元一个基础活跃等级。sign是符号函数：</p><script type="math/tex; mode=display">{\displaystyle \text{sign}(n)={    \begin{cases}        +1&n\geq 0\\        -1&n<0    \end{cases}    }}</script><p>映射函数同样可以写成如下的表述形式：</p><script type="math/tex; mode=display">{\displaystyle y=\text{sign}(\sum_{i=1}^{n}{ {w}_{i}{x}_{i}+b})=\text{sign}( {W}^{T} {X})}</script><h2 id="3-感知机学习算法"><a href="#3-感知机学习算法" class="headerlink" title="3. 感知机学习算法"></a>3. 感知机学习算法</h2><p>假设训练数据是线性可分的，感知机的学习目标是寻找模型参数$w,b$来将数据集正负实例点通过超平面进行区分，因此需要一个损失函数并得到损失函数的极小值。</p><blockquote><p>我们很自然的想到能否还用最小二乘法中的损失函数呢？</p><script type="math/tex; mode=display">Q=min{||\text{sign}(\mathbf w\cdot \mathbf x+b)-y||}^2</script><p>感知机与最小二乘法的不同之处在于 $y$ 即映射函数 $f(x)$ 的不同。最小二乘法是线性函数，用于线性回归；而感知机是sign函数，用于分类。很明显sign是个阶跃的不连续函数，这就导致损失函数同样是不连续的，就无法通过微分来得到极值点。</p></blockquote><p>损失函数还有一个自然选择是误分类点的总数，但同样这种损失函数也不是参数 $\mathbf w,b$ 的连续可导函数。损失函数的另一个选择是误分类点到超平面的距离，通过距离公式可以得到任一点 $x_0$ 到超平面 $S$ 的距离：</p><script type="math/tex; mode=display">\frac{1}{\left\|\mathbf w\right\|}|\mathbf w\cdot x_0+b|</script><blockquote><p>在一维空间，一个线性函数如 $Ax+By+C=0$ 的法向量就是 $(A,B)$，点到直线距离相当于该点到法向量投影的长度，假设点 $Q$ 坐标 $(x_0,y_0)$，直线上任意一点坐标 $P(x,y)$</p><script type="math/tex; mode=display">d=|PQ|\cdot \cos\theta</script><script type="math/tex; mode=display">d=\frac{|n|\cdot|PQ|\cdot \cos\theta}{|n|}</script><script type="math/tex; mode=display">d=\frac{\vec{n}\cdot \vec{PQ}}{|n|}</script><p>同理可以推广到多维空间</p><p><strong>记住一点：</strong></p><p><strong>权重向量是超平面的法线</strong></p></blockquote><p>对于误分类的数据$(\mathbf x_i,y_i)$，当$\mathbf w\cdot \mathbf x_i+b&gt;0$，$y_i=-1$；反之，$y_i=+1$，因此任一点$x_i$到超平面$S$的距离是</p><script type="math/tex; mode=display">-\frac{1}{\left\|\mathbf w\right\|}y_i(\mathbf w\cdot \mathbf x_i+b)</script><p>不考虑$\displaystyle \frac{1}{\left|\mathbf w\right|}$， 感知机学习的损失函数定义为</p><script type="math/tex; mode=display">L(\mathbf w,b)=-\sum_{\mathbf x_i\in M}y_i(\mathbf w\cdot \mathbf x_i+b)</script><p>$M$ 是误分类点的集合。</p><p>显然，损失函数是非负的。如果没有误分类点，损失函数值是0。而且，误分类点越少，误分类点离超平面越近，损失函数值就越小。注意，在分类正确时，损失函数为0。因此，给定训练数据$T$，损失函数$L(\mathbf w,b)$ 是$\mathbf w,b$ 的连续可导函数。</p><p>感知机学习算法采用随机梯度下降法。首先选取一个超平面$\mathbf w_0,b_0$，然后用梯度下降法不断地极小化目标函数。极小化过程不是一次使$M$中所有误分类点的梯度下降，而是一次随机选取一个误分类点使其梯度下降。</p><p>损失函数 $L(\mathbf w,b)$ 的梯度：</p><script type="math/tex; mode=display">\bigtriangledown_\mathbf w L(\mathbf w,b)=-\sum_{\mathbf x_i\in M}y_i\mathbf x_i</script><script type="math/tex; mode=display">\bigtriangledown_b L(\mathbf w,b)=-\sum_{x_i\in M}y_i</script><p>随机选取一个误分类点$(x_i,y_i)$，对 $\mathbf w,b$ 进行更新：</p><script type="math/tex; mode=display">\mathbf w_{t+1}\leftarrow \mathbf w_{t}+ \eta y_i\mathbf x_i</script><script type="math/tex; mode=display">b_{t+1}\leftarrow b_{t}+ \eta y_i</script><p>式中$\eta$是步长，在统计学习中称为学习率。通过迭代可以期待损失函数不断减少。</p><p>为了便于推导，可以将偏置 $b$ 并入权重向量 $\mathbf w$，将 $d$ 维的输入转为 $(d+1)$ 维的输入，同时传入到对应的 $(d+1)$ 维感知机当中。</p><script type="math/tex; mode=display">\mathbf w\cdot x+b=[\mathbf w,b]\cdot [x,1]=\mathbf w\cdot \mathbf x</script><p>总结算法流程如下:</p><ol><li>初始化参数，迭代次数$t=1$并且$\mathbf w_1$为全是0的权重向量。</li><li>对每一个样本 $\mathbf x_t$ 预测，$f(x_t)=+1$ if $\mathbf w_t\cdot \mathbf x_t\ge 0$ else $-1$</li><li>对一个分类错误的样本 $y_i(\mathbf w_t \cdot \mathbf x_i) \le 0$ 进行学习，$\mathbf w_{t+1}\leftarrow \mathbf w_{t}+ \eta y_i\mathbf x_i$ </li><li>$t\leftarrow t + 1$</li></ol><h2 id="4-算法收敛性"><a href="#4-算法收敛性" class="headerlink" title="4. 算法收敛性"></a>4. 算法收敛性</h2><p>在证明感知机算法收敛性之前，先证明两个假设。</p><p><strong>定理1(线性可分性).</strong> 对于线性可分的数据集，存在 $\mathbf w^\ast$ 满足 $\left|\mathbf w^\ast \right|=1$ ,使得超平面 $\mathbf w^\ast \cdot \mathbf x=0$ 能够将所有训练数据完全正确分开，且存在 $\gamma &gt; 0$，对于所有 $i\in \{1,2,…,n\}$，</p><script type="math/tex; mode=display">y_i(\mathbf w^\ast \cdot \mathbf x_i) > \gamma</script><p>证明：由于训练数据集是线性可分的，则存在超平面将数据集完全分开，取此超平面为 $\mathbf w^\ast \cdot \mathbf x=0$，通过向量单位化使 $\left|\mathbf w^*\ast \right|=1$ 。由于对于有限的数据集 $i\in \{1,2,…,n\}$，均有</p><script type="math/tex; mode=display">y_i(\mathbf w^\ast \cdot \mathbf x_i) > 0</script><p>所以存在</p><script type="math/tex; mode=display">y_i(\mathbf w^\ast \cdot \mathbf x_i) \ge \min_i(y_i(\mathbf w^\ast \cdot \mathbf x_i)) > \gamma</script><p><strong>定理2(有界性).</strong> 存在 ${R \in R^n}$ 对于有限的数据集 $i\in \{1,2,…,n\}$，均有</p><script type="math/tex; mode=display">\left\|\mathbf x_i \right\| \le R</script><p><strong>定理3(收敛性).</strong> 感知机学习算法最多迭代次数（之后得到一个分离超平面），满足不等式：</p><script type="math/tex; mode=display">k\le \left( \frac{R}{\gamma}\right) ^2</script><p>证明：我们需要推导出 $k$ 的上边界是上式，策略是根据 $k$ 推导出 $\mathbf w_{k+1}$ 长度的上下限并将它们关联起来。</p><p>注意到 $\mathbf w_{1}=0$，对于 $k\ge1$ ，如果 $\mathbf{x}_j$ 是迭代 $k$ 期间的误分类点，由定理1，我们有:</p><script type="math/tex; mode=display">\begin{aligned}    \mathbf w_{k+1} \cdot \mathbf{w}^*&=(\mathbf{w}_k + \eta y_j\mathbf{x}_j)\cdot \mathbf{w}^* \\    &=\mathbf w_{k} \cdot \mathbf{w}^* + \eta y_j(\mathbf{x}_j \cdot \mathbf{w}^*) \\    &>\mathbf w_{k} \cdot \mathbf{w}^* + \eta \gamma \\    &>\mathbf w_{k-1} \cdot \mathbf{w}^* + 2\eta \gamma \\    &>k\eta \gamma\end{aligned}</script><p>由于</p><script type="math/tex; mode=display">\begin{aligned}\mathbf w_{k+1} \cdot \mathbf{w}^* &= \left\|\mathbf w_{k+1} \right\|\left\|\mathbf w^* \right\| \cos \theta \\&\le \left\|\mathbf w_{k+1} \right\| \left\|\mathbf w^* \right\| = \left\|\mathbf w_{k+1} \right\|\end{aligned}</script><p>我们有：</p><script type="math/tex; mode=display">\left\|\mathbf w_{k+1} \right\| >k\eta \gamma</script><p>至此我们得到了$\left|\mathbf w_{k+1} \right|$的下界，为了得到上界，我们推断：</p><script type="math/tex; mode=display">\begin{aligned}\left\|\mathbf w_{k+1} \right\|^2 &=\left\|\mathbf w_{k}+\eta y_j\mathbf{x}_j \right\|^2 \\&=\left\|\mathbf w_{k}\right\|^2 + \left\| \eta y_j\mathbf{x}_j \right\|^2 + 2(\mathbf w_{k}\cdot \mathbf x_{j})\eta y_j \\&=\left\|\mathbf w_{k}\right\|^2 + \left\| \eta \mathbf{x}_j \right\|^2 + 2(\mathbf w_{k}\cdot \mathbf x_{j})\eta y_j \\&\le \left\|\mathbf w_{k}\right\|^2 + \left\| \eta \mathbf{x}_j \right\|^2 \\&\le \left\|\mathbf w_{k}\right\|^2 + \eta ^2R^2 \\&\le \left\|\mathbf w_{k-1}\right\|^2 + 2\eta ^2R^2 \\&\le k\eta ^2R^2 \\\end{aligned}</script><p>联立$\left|\mathbf w_{k+1} \right|$的上下界，我们得到不等式：</p><script type="math/tex; mode=display">(k\eta \gamma)^2<\left\|\mathbf w_{k+1} \right\|\le k\eta ^2R^2</script><p>最终得到：</p><script type="math/tex; mode=display">k\le \left( \frac{R}{\gamma}\right) ^2</script><h2 id="5-算例"><a href="#5-算例" class="headerlink" title="5. 算例"></a>5. 算例</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Perceptron</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, train_data, lable</span>):</span></span><br><span class="line">        self.train_data = np.array(train_data)</span><br><span class="line">        self.lable = np.array(lable)</span><br><span class="line">        self.length = <span class="built_in">len</span>(train_data)</span><br><span class="line">        self.train_data = np.append(self.train_data, np.ones((self.length, <span class="number">1</span>)), axis = <span class="number">1</span>)</span><br><span class="line">        self.weights = np.zeros(<span class="built_in">len</span>(train_data[<span class="number">0</span>]) + <span class="number">1</span>)</span><br><span class="line">        self.eta = <span class="number">0.2</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sign</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span> <span class="keyword">if</span> x &gt;= <span class="number">0</span> <span class="keyword">else</span> -<span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span>(<span class="params">self, input_data</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.sign(np.<span class="built_in">sum</span>(self.weights * input_data))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">for</span> idx <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">            flag = <span class="literal">True</span></span><br><span class="line">            <span class="keyword">for</span> i, inpute_data <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.train_data):</span><br><span class="line">                <span class="keyword">if</span> self.lable[i] * self.predict(inpute_data) &lt;= <span class="number">0</span>:</span><br><span class="line">                    vectorA = self.eta * self.lable[i] * inpute_data</span><br><span class="line">                    Y = <span class="keyword">lambda</span> x,y: (- y[<span class="number">2</span>] - (x) * y[<span class="number">0</span>]) / y[<span class="number">1</span>]</span><br><span class="line">                    plt.style.use(<span class="string">&#x27;Solarize_Light2&#x27;</span>)</span><br><span class="line">                    plt.figure().add_subplot(<span class="number">111</span>).set_aspect(<span class="string">&#x27;equal&#x27;</span>)</span><br><span class="line">                    plt.cla()</span><br><span class="line">                    scat()</span><br><span class="line">                    <span class="comment"># plt.scatter(0, -self.weights[2] / self.weights[1], c=&#x27;k&#x27;)</span></span><br><span class="line">                    plt.ylim((-<span class="number">5</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">                    plt.xlim((-<span class="number">5</span>, <span class="number">10</span>))</span><br><span class="line">                    plt.plot([<span class="number">0</span>, vectorA[<span class="number">0</span>]], [Y(<span class="number">0</span>, self.weights), Y(<span class="number">0</span>, self.weights) + vectorA[<span class="number">1</span>]])</span><br><span class="line">                    plt.plot([<span class="number">0</span>, self.weights[<span class="number">0</span>]], [Y(<span class="number">0</span>, self.weights), Y(<span class="number">0</span>, self.weights) + self.weights[<span class="number">1</span>]])</span><br><span class="line"></span><br><span class="line">                    vectorB = self.weights + vectorA</span><br><span class="line">                    plt.plot([-<span class="number">5</span>,<span class="number">10</span>], [Y(-<span class="number">5</span>, self.weights), Y(<span class="number">10</span>, self.weights)])</span><br><span class="line">                    plt.plot([<span class="number">0</span>, vectorB[<span class="number">0</span>]], [Y(<span class="number">0</span>, self.weights), Y(<span class="number">0</span>, self.weights) + vectorB[<span class="number">1</span>]])</span><br><span class="line">                    plt.plot([-<span class="number">5</span>,<span class="number">10</span>], [Y(-<span class="number">5</span>, vectorB), Y(<span class="number">10</span>, vectorB)])</span><br><span class="line">                    <span class="built_in">print</span>(self.weights, self.eta * self.lable[i] * inpute_data, self.predict(inpute_data), self.lable[i])</span><br><span class="line">                    plt.scatter(inpute_data[<span class="number">0</span>], inpute_data[<span class="number">1</span>], c=<span class="string">&#x27;k&#x27;</span>)</span><br><span class="line">                    plt.savefig(<span class="string">&#x27;testblueline&#123;&#125;-&#123;&#125;.jpg&#x27;</span>.<span class="built_in">format</span>(idx,i))</span><br><span class="line">                    self.weights += vectorA</span><br><span class="line">                    </span><br><span class="line">                    flag = <span class="literal">False</span></span><br><span class="line">                    <span class="comment"># plt.pause(0.5)</span></span><br><span class="line">            <span class="keyword">if</span> flag:</span><br><span class="line">                <span class="keyword">return</span> self.weights</span><br><span class="line"></span><br><span class="line">train_data = [[<span class="number">2</span>, <span class="number">3</span>], [<span class="number">1</span>, <span class="number">4</span>], [<span class="number">3</span>, <span class="number">5</span>], [<span class="number">2</span>, <span class="number">6</span>], [<span class="number">4</span>, <span class="number">5</span>], [<span class="number">3</span>, <span class="number">1</span>], [<span class="number">4</span>, <span class="number">3</span>], [<span class="number">6</span>, <span class="number">2</span>], [<span class="number">2</span>, <span class="number">1</span>]]</span><br><span class="line">lable = [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span> ,-<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">plt.ion()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">scat</span>():</span></span><br><span class="line">    plt.scatter([x[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> train_data[:<span class="number">5</span>]], [x[<span class="number">1</span>] <span class="keyword">for</span> x <span class="keyword">in</span> train_data[:<span class="number">5</span>]], c=<span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">    plt.scatter([x[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> train_data[<span class="number">5</span>:]], [x[<span class="number">1</span>] <span class="keyword">for</span> x <span class="keyword">in</span> train_data[<span class="number">5</span>:]], c=<span class="string">&#x27;b&#x27;</span>)</span><br><span class="line"></span><br><span class="line">ptron = Perceptron(train_data, lable)</span><br><span class="line">ptron.train()</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/He1o/Cyrus_NoteBook/main/MachineLearning/2.Perceptron/source/img/perceptron_iteration.gif" alt="感知机迭代过程图"></p><center>感知机迭代过程图</center><h2 id="5-思考"><a href="#5-思考" class="headerlink" title="5. 思考"></a>5. 思考</h2><p><img src="https://raw.githubusercontent.com/He1o/Cyrus_NoteBook/main/MachineLearning/2.Perceptron/source/img/img_1_0.jpg" alt="感知机迭代过程图1"></p><center>感知机迭代过程图1</center><p><img src="https://raw.githubusercontent.com/He1o/Cyrus_NoteBook/main/MachineLearning/2.Perceptron/source/img/img_1_6.jpg" alt="感知机迭代过程图2"></p><center>感知机迭代过程图2</center><p>对两个过程图进行进一步分析，以下为自己理解：</p><ol><li>不同的系数可能绘制出同一条直线或同一个平面，例如 [1,1,1] 和 [-1,-1,-1] 都可以表示直线 $x+y+1=0$，但权重代表的法向量的方向相反。如上述图中垂直于直线的即为法向量也就是 $w$，当实例点与法向量点乘之后再加上位移量 $b$，就可以用来判断该点在直线的哪一侧。原理有很多中解释，其中一种就是$\mathbf w\cdot\mathbf x$ 的正负决定了它们之间的夹角大小是否超过 $90$ 度，确定相对角度之后还不足以判断该点相对于直线的位置，</li><li>误分类的点改变的是直线法向量的方向和大小以及 $b$ 即直线的位置，误分类点也可以理解为一个向量，乘以衰减系数后作用在法向量上，作用可能是吸引或者排斥。当分类为正时为吸引作用，如图1，反之则为排斥作用如图2。前提都是错误的分类数据，即正分类点不会和法向量处在同一侧，因此正分类点希望法向量“转过来”，同理，负分类点会与法向量处于同一侧，它希望法向量“转过去”。</li></ol><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><blockquote><p><a href="https://zh.wikipedia.org/wiki/%E6%84%9F%E7%9F%A5%E5%99%A8">1. 感知器-维基百科</a><br><a href="https://www.cnblogs.com/graphics/archive/2010/07/10/1774809.html">2. 点到平面的距离公式</a><br><a href="https://blog.csdn.net/song430/article/details/88718602">3. 用Python实现单层感知机</a><br><a href="https://www.cnblogs.com/xym4869/p/11282469.html">4. 深度学习基础——感知机</a><br><a href="https://www.cnblogs.com/huangyc/p/9706575.html">5. 感知机原理</a><br><a href="https://github.com/SmallVagetable/machine_learning_python">6. machine_learning_python</a></p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;span id=&quot;more&quot;&gt;&lt;/span&gt;
&lt;h1 id=&quot;感知机&quot;&gt;&lt;a href=&quot;#感知机&quot; class=&quot;headerlink&quot; title=&quot;感知机&quot;&gt;&lt;/a&gt;感知机&lt;/h1&gt;&lt;p&gt;感知机（英语：Perceptron）是弗兰克·罗森布拉特（Frank Rosenbl</summary>
      
    
    
    
    <category term="Machine learning" scheme="http://example.com/categories/Machine-learning/"/>
    
    
  </entry>
  
  <entry>
    <title>1. LeastSqaure</title>
    <link href="http://example.com/2021/09/18/MachineLearning/1.LeastSqaure/"/>
    <id>http://example.com/2021/09/18/MachineLearning/1.LeastSqaure/</id>
    <published>2021-09-17T16:00:00.000Z</published>
    <updated>2022-01-08T10:29:20.809Z</updated>
    
    <content type="html"><![CDATA[<span id="more"></span><h1 id="最小二乘法"><a href="#最小二乘法" class="headerlink" title="最小二乘法"></a>最小二乘法</h1><p>最小二乘法又称为最小平方法，是一种数学优化建模方法。它通过最小化误差的平方和寻找数据的最佳函数匹配。最小二乘法是对线性方程组，即方程个数比未知数更多的方程组，以回归分析求得近似解的标准方法。在这个解决方案中，最小二乘法演算为每一方程的结果中，将残差平方和的总和最小化。</p><p>最小二乘法分为线性和非线性，取决于所有未知数中的残差是否为线性。线性的最小二乘问题发生在统计回归分析中，具有一个封闭形式的解决方案。非线性的问题通常由迭代细致化来解决，每次迭代中，系统由线性近似，因此这两种情况下核心演算是相同的。</p><h2 id="1-历史背景"><a href="#1-历史背景" class="headerlink" title="1. 历史背景"></a>1. 历史背景</h2><p>最小二乘法发展于天文学和大地测量学领域，科学家和数学家尝试为大航海探索时期的海洋航行挑战提供解决方案。准确描述天体的行为是船舰在大海洋上航行的关键，水手不能再依靠陆上目标导航作航行。</p><p>这个方法是在十八世纪期间一些进步的集大成：</p><ul><li>不同观测值的组合是真实值的最佳估计；多次观测会减少误差而不是增加，也许在1722年由Roger Cotes首先阐明。</li><li>在相同条件下采取的不同观察结果，与只尝试记录一次最精确的观察结果是对立的。这个方法被称为平均值方法。托马斯·马耶尔（Tobias Mayer）在1750年研究月球的天平动时，特别使用这种方法，而拉普拉斯（Pierre-Simon Laplace）在1788年他的工作成果中以此解释木星和土星的运动差异。</li><li>在不同条件下进行的不同观测值组合。该方法被称为最小绝对偏差法，出现在Roger Joseph Boscovich在1757年他对地球形体的著名作品，而拉普拉斯在1799年也表示了同样的问题。</li><li>评定对误差达到最小的解决方案标准，拉普拉斯指明了误差的概率密度的数学形式，并定义了误差最小化的估计方法。为此，拉普拉斯使用了一双边对称的指数分布，现在称为拉普拉斯分布作为误差分布的模型，并将绝对偏差之和作为估计误差。他认为这是他最简单的假设，他期待得出算术平均值而成为最佳的估计。可相反地，他的估计是后验中位数。</li></ul><p>1801年，意大利天文学家朱塞普·皮亚齐发现了第一颗小行星谷神星。经过40天的追踪观测后，由于谷神星运行至太阳背后，使得皮亚齐失去了谷神星的位置。随后全世界的科学家利用皮亚齐的观测数据开始寻找谷神星，但是根据大多数人计算的结果来寻找谷神星都没有结果。当年24岁的高斯也计算了谷神星的轨道。奥地利天文学家海因里希·奥伯斯根据高斯计算出来的轨道重新发现了谷神星。</p><p>高斯使用的最小二乘法的方法发表于1809年他的著作《天体运动论》中，而法国科学家勒让德于1806年独立发现“最小二乘法”，但因不为世人所知而没没无闻。两人曾为谁最早创立最小二乘法原理发生争执。</p><p>1829年，高斯提供了最小二乘法的优化效果强于其他方法的证明，见高斯-马尔可夫定理。</p><h2 id="2-问题引入"><a href="#2-问题引入" class="headerlink" title="2. 问题引入"></a>2. 问题引入</h2><p>如果对同一目标例如手机宽度通过尺子测量长度，通过同一尺子不同测量员或者不同尺子同一测量员得到的结果都有可能不同，那么如何通过所有的测量结果来得到准确的真值呢？</p><p>假设测量结果分别为72.5mm、72.2mm、72.9mm、72.4mm、72.5mm。</p><p>只要做过初中物理实验的都知道，通常都会对同一实验进行多次重复操作，把得到的结果进行平均求和，最后的结果作为实验的准确结果</p><script type="math/tex; mode=display">\overline{x}=\frac{72.5+72.2+72.9+72.4+72.5}{5}=72.5</script><p>直觉告诉我们取平均值是正确的，那么这么做得依据是什么呢？</p><p>再比如，我们知道营业税税收总额与社会商品零售总额有关，假设收集了如下数据</p><div class="table-container"><table><thead><tr><th style="text-align:center">序号</th><th style="text-align:center">社会商品零售总额</th><th style="text-align:center">营业税税收总额</th></tr></thead><tbody><tr><td style="text-align:center">1</td><td style="text-align:center">142.08</td><td style="text-align:center">3.93</td></tr><tr><td style="text-align:center">2</td><td style="text-align:center">177.30</td><td style="text-align:center">5.96</td></tr><tr><td style="text-align:center">3</td><td style="text-align:center">204.68</td><td style="text-align:center">7.85</td></tr><tr><td style="text-align:center">4</td><td style="text-align:center">242.88</td><td style="text-align:center">9.82</td></tr><tr><td style="text-align:center">5</td><td style="text-align:center">316.24</td><td style="text-align:center">12.50</td></tr><tr><td style="text-align:center">6</td><td style="text-align:center">341.99</td><td style="text-align:center">15.55</td></tr><tr><td style="text-align:center">7</td><td style="text-align:center">332.69</td><td style="text-align:center">15.79</td></tr><tr><td style="text-align:center">8</td><td style="text-align:center">389.29</td><td style="text-align:center">16.39</td></tr><tr><td style="text-align:center">9</td><td style="text-align:center">453.40</td><td style="text-align:center">18.45</td></tr></tbody></table></div><p>如何通过这些已有数据在给定社会商品零售总额的情况下预测营业税税收总额呢？</p><h2 id="3-问题求解"><a href="#3-问题求解" class="headerlink" title="3. 问题求解"></a>3. 问题求解</h2><p>我们将第一个问题用公式进行描述，假设要猜测的真实值是$y$，测量值则为$y_i$，上述求平均公式则为：</p><script type="math/tex; mode=display">\displaystyle{\frac{\sum y_i}{n}}=y</script><p>公式可以转化为：</p><script type="math/tex; mode=display">\sum (y_i-y)=0</script><p>满足上式时，误差平方和取得最小值，因为导数为0</p><script type="math/tex; mode=display">\frac{d}{dy}S_{\epsilon^2}=\frac{d}{dy}\sum (y_i-y)^2=0</script><p>至此，我们明白求平均的实质是希望预测值与测量值的误差平方和最小，最小二乘法也就是最小平方法，二乘实际上也是日语中平方的意思。</p><p>第二个问题中，社会商品零售总额$x$是自变量，其值是可以控制或精确测量的，是非随机变量；营业税税收总额$y$是因变量，取值不能事先确定，是随机变量。假定它们具有线性相关关系，$y_i$的表达式：</p><script type="math/tex; mode=display">{\displaystyle{y_i=\beta_0+\beta_1x_i+\epsilon_i}}</script><p>其中$\beta_0,\beta_1$称为回归系数，由于它们未知，因此需要从收集到的数据出发进行估计，记为$\hat{\beta_0},\hat{\beta_1}$。</p><p>通过最小二乘法估计回归系数，目标是希望偏差平方和$Q$达到最小：</p><script type="math/tex; mode=display">Q(\beta_0,\beta_1)=\sum_{i=1}^n (y_i-\beta_0-\beta_1x_i)^2</script><p>由于$Q$是一个非负二次型，因此可通过令$Q$对$\beta_0,\beta_1$的偏导为零来求：</p><script type="math/tex; mode=display">\begin{cases}    {\displaystyle \frac{\partial Q}{\partial \beta_0}=-2\sum(y_i-\beta_0-\beta_1x_i)} \\    \\    {\displaystyle \frac{\partial Q}{\partial \beta_1}=-2\sum(y_i-\beta_0-\beta_1x_i)x_i}\end{cases}</script><p>经整理有</p><script type="math/tex; mode=display">\begin{cases}    {\displaystyle n\beta_0 + n\overline{x}\cdot\beta_1}=n\overline{y} \\    \\    {\displaystyle n\overline{x}\beta_0+\sum{x_i^2\beta_1}=\sum{x_iy_i}}\end{cases}</script><p>两式合并后</p><script type="math/tex; mode=display">\begin{aligned}    \left( \sum{x_i^2} -n\overline{x}^2 \right)\beta_1&=\sum{x_iy_i}-n\overline{x}\overline{y}\\    \left( \sum{x_i^2} + \sum{\overline{x}^2}-2n\overline{x}^2 \right)\beta_1&=\sum{x_iy_i}-\sum{x_i}\overline{y}\\    \left( \sum{x_i^2} + \sum{\overline{x}^2}-2\sum{x_i}\overline{x} \right)\beta_1&=\sum{x_iy_i}-\sum{x_i}\overline{y}+n\overline{x}\overline{y}-n\overline{x}\overline{y}\\    \left( \sum{(x_i -\overline{x})^2} \right)\beta_1&=\sum{x_iy_i}-\sum{x_i}\overline{y}+\sum{\overline{x}y_i}-\sum\overline{x}\overline{y}\\    \left( \sum{(x_i -\overline{x})^2} \right)\beta_1&=\sum{(x_i-\overline{x})(y_i-\overline{y})}\\    \beta_1&=\frac{cov(x,y)}{Var(x)}\end{aligned}</script><p>实际上 $\beta_1$ 是 $x$ 与 $y$ 的协方差与 $x$ 方差的商</p><p>通过以上的方法可以推导出更多特征的求解方法，通过高斯消元法可以求解多元线性方程组，求得解析解。</p><p>同时也可以通过矩阵法求解，将多个方程看做一个整体进行求解。</p><script type="math/tex; mode=display">{\displaystyle     {\begin{pmatrix}    1& x_{11}& \cdots & x_{1j}\cdots & x_{1q}\\    1& x_{21}& \cdots & x_{2j}\cdots & x_{2q}\\    \vdots \\    1& x_{i1}& \cdots & x_{ij}\cdots & x_{iq}\\    \vdots \\    1& x_{n1}& \cdots & x_{nj}\cdots & x_{nq}    \end{pmatrix}}     \cdot     {\begin{pmatrix}\beta_{0}\\\beta_{1}\\\beta_{2}\\\vdots \\\beta_{j}\\\vdots \\\beta_{q}\end{pmatrix}}=    {\begin{pmatrix}y_{1}\\y_{2}\\\vdots \\y_{i}\\\vdots \\y_{n}\end{pmatrix}}}</script><p>矩阵表达式为：</p><script type="math/tex; mode=display">    Q=min{||Xw-y||}^2</script><p>求 $w$ 的最小二乘估计，即求 $\frac{\partial Q}{\partial w}$ 的零点。其中 $y$ 是 $m\times 1$ 列向量，$X$ 是 $m\times n$ 矩阵，$w$是$n\times 1$列向量，$Q$是标量。</p><p>将向量模平方改写成向量与自身的内积：</p><script type="math/tex; mode=display">Q=(Xw-y)^T(Xw-y)</script><p>求微分：</p><script type="math/tex; mode=display">\begin{aligned}    dQ&=(Xdw)^T(Xw-y)+(Xw-y)^T(Xdw)\\    &=2(Xw-y)^T(Xdw)\end{aligned}</script><p>这里是因为两个向量的内积满足$u^Tv=v^Tu$。</p><p>导数与微分的关系式</p><script type="math/tex; mode=display">dQ={\frac{\partial Q}{\partial w}}^Tdw</script><p>得到</p><script type="math/tex; mode=display">{\frac{\partial Q}{\partial w}}=(2(Xw-y)^TX)^T=2X^T(Xw-y)=0</script><p>求解可得</p><script type="math/tex; mode=display">\begin{aligned}    X^TXw&=X^Ty\\    w&=(X^TX)^{-1}X^Ty\end{aligned}</script><h2 id="4-思考"><a href="#4-思考" class="headerlink" title="4. 思考"></a>4. 思考</h2><p>为什么损失函数不用误差的绝对值？</p><script type="math/tex; mode=display">Q(\beta_0,\beta_1)=\sum_{i=1}^n |y_i-\beta_0-\beta_1x_i|</script><p>网上有人说是太麻烦，但我觉得主要原因是<strong>绝对值函数连续但不可导</strong>，无法通过求导数的为零的极值点，也无法通过梯度下降法进行求解。</p><h2 id="5-算例"><a href="#5-算例" class="headerlink" title="5. 算例"></a>5. 算例</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line">x = np.array([<span class="number">142.08</span>, <span class="number">177.30</span>, <span class="number">204.68</span>, <span class="number">242.88</span>, <span class="number">316.24</span>, <span class="number">332.69</span>, <span class="number">341.99</span>, <span class="number">389.29</span>, <span class="number">453.40</span>])</span><br><span class="line">y = np.array([<span class="number">3.93</span>,   <span class="number">5.96</span>,   <span class="number">7.85</span>,   <span class="number">9.82</span>,   <span class="number">12.50</span>,  <span class="number">15.79</span>,  <span class="number">15.55</span>,  <span class="number">16.39</span>,  <span class="number">18.45</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过代数方法求解</span></span><br><span class="line"><span class="comment"># numpy的协方差默认是样本方差，无偏的，自由度n-1，因此要加bias = True</span></span><br><span class="line">beta_0 = np.cov(x, y, bias = <span class="literal">True</span>)[<span class="number">0</span>,<span class="number">1</span>] / np.var(x)  </span><br><span class="line">beta_1 = np.<span class="built_in">sum</span>(y) / <span class="number">9</span> - np.<span class="built_in">sum</span>(x) / <span class="number">9</span> * beta_0</span><br><span class="line"><span class="built_in">print</span>(beta_0, beta_1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过公式计算，与上面相同</span></span><br><span class="line"><span class="comment"># a = np.sum(np.multiply(x, y)) - np.sum(x) * np.sum(y) / 9</span></span><br><span class="line"><span class="comment"># b = np.sum(np.multiply(x, x)) - np.sum(x) * np.sum(x) / 9</span></span><br></pre></td></tr></table></figure><blockquote><p>计算结果<br>0.04867773628668675 -2.2609874555936926</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过矩阵法实现最小二乘法</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">least_sqaure</span>(<span class="params">X, Y</span>):</span></span><br><span class="line">    <span class="keyword">return</span> (X.T * X).I * X.T * Y</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成多项式</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">polynomial</span>(<span class="params">x, n</span>):</span></span><br><span class="line">    X = np.mat(x)</span><br><span class="line">    X = np.append(np.ones((<span class="number">1</span>, <span class="number">9</span>)), X, axis = <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n):</span><br><span class="line">        X = np.append(X, np.mat(x**(i + <span class="number">1</span>)), axis = <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> X.T</span><br><span class="line"></span><br><span class="line">Y = np.mat(y).T</span><br><span class="line"></span><br><span class="line"><span class="comment"># 线性拟合</span></span><br><span class="line">X = polynomial(x, <span class="number">1</span>)</span><br><span class="line">beta = np.array(least_sqaure(X, Y)).flatten()[::-<span class="number">1</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;beta:&#x27;</span>, beta)</span><br><span class="line">plt.subplot(<span class="number">221</span>)</span><br><span class="line">plt.plot(x, y, <span class="string">&#x27;bo&#x27;</span>, label=<span class="string">&#x27;noise&#x27;</span>)</span><br><span class="line">plt.plot(x, np.poly1d(beta)(x), label=<span class="string">&#x27;fitted curve&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 二次拟合</span></span><br><span class="line">X = polynomial(x, <span class="number">2</span>)</span><br><span class="line">beta = np.array(least_sqaure(X, Y)).flatten()[::-<span class="number">1</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;beta:&#x27;</span>, beta)</span><br><span class="line">plt.subplot(<span class="number">222</span>)</span><br><span class="line">plt.plot(x, y, <span class="string">&#x27;bo&#x27;</span>, label=<span class="string">&#x27;noise&#x27;</span>)</span><br><span class="line">plt.plot(x, np.poly1d(beta)(x), label=<span class="string">&#x27;fitted curve&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 三次拟合</span></span><br><span class="line">X = polynomial(x, <span class="number">3</span>)</span><br><span class="line">beta = np.array(least_sqaure(X, Y)).flatten()[::-<span class="number">1</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;beta:&#x27;</span>, beta)</span><br><span class="line">plt.subplot(<span class="number">223</span>)</span><br><span class="line">plt.plot(x, y, <span class="string">&#x27;bo&#x27;</span>, label=<span class="string">&#x27;noise&#x27;</span>)</span><br><span class="line">plt.plot(x, np.poly1d(beta)(x), label=<span class="string">&#x27;fitted curve&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 六次拟合</span></span><br><span class="line">X = polynomial(x, <span class="number">6</span>)</span><br><span class="line">beta = np.array(least_sqaure(X, Y)).flatten()[::-<span class="number">1</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;beta:&#x27;</span>, beta)</span><br><span class="line">plt.subplot(<span class="number">224</span>)</span><br><span class="line">plt.plot(x, y, <span class="string">&#x27;bo&#x27;</span>, label=<span class="string">&#x27;noise&#x27;</span>)</span><br><span class="line">plt.plot(x, np.poly1d(beta)(x), label=<span class="string">&#x27;fitted curve&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/He1o/Cyrus_NoteBook/main/MachineLearning/1.LeastSquare/source/img/Figure_1.png" alt="最小二乘法拟合图"></p><center>最小二乘法拟合图</center><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><blockquote><p><a href="https://www.zhihu.com/question/37031188">1. 最小二乘法的本质是什么？</a><br><a href="https://www.cnblogs.com/pinard/p/5976811.html">2. 最小二乘法小结-刘建平Pinard</a><br><a href="https://zh.wikipedia.org/wiki/%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95">3. 最小二乘法-维基百科</a><br><a href="https://zhuanlan.zhihu.com/p/84133777">4. 最小二乘法详细推导 - 知乎</a><br><a href="https://zhuanlan.zhihu.com/p/89373759">5. 最小二乘法 - 知乎</a><br><a href="https://zhuanlan.zhihu.com/p/87582571">6. 矩阵形式下的最小二乘法推导 - 知乎</a><br><a href="https://zhuanlan.zhihu.com/p/24709748">7. 矩阵求导术（上）- 知乎</a></p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;span id=&quot;more&quot;&gt;&lt;/span&gt;
&lt;h1 id=&quot;最小二乘法&quot;&gt;&lt;a href=&quot;#最小二乘法&quot; class=&quot;headerlink&quot; title=&quot;最小二乘法&quot;&gt;&lt;/a&gt;最小二乘法&lt;/h1&gt;&lt;p&gt;最小二乘法又称为最小平方法，是一种数学优化建模方法。它通过最小化误差</summary>
      
    
    
    
    <category term="Machine learning" scheme="http://example.com/categories/Machine-learning/"/>
    
    
  </entry>
  
  <entry>
    <title>1.random events and their probabilities</title>
    <link href="http://example.com/2021/09/18/ProbabilityTheory/1.random%20events%20and%20their%20probabilities/"/>
    <id>http://example.com/2021/09/18/ProbabilityTheory/1.random%20events%20and%20their%20probabilities/</id>
    <published>2021-09-17T16:00:00.000Z</published>
    <updated>2022-01-01T02:36:18.000Z</updated>
    
    
    
    
    <category term="Probability Theory and Statistics" scheme="http://example.com/categories/Probability-Theory-and-Statistics/"/>
    
    
  </entry>
  
</feed>
