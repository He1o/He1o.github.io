<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>He1o</title>
  
  
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2022-01-01T02:36:18.000Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>He1o</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>K-NearestNeighbor</title>
    <link href="http://example.com/2021/09/18/Machine%20Learning/K-NearestNeighbor/"/>
    <id>http://example.com/2021/09/18/Machine%20Learning/K-NearestNeighbor/</id>
    <published>2021-09-17T16:00:00.000Z</published>
    <updated>2022-01-01T02:36:18.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="k近邻算法"><a href="#k近邻算法" class="headerlink" title="k近邻算法"></a>k近邻算法</h1><p>k近邻算法是一种基本分类与回归方法</p><h2 id="1、历史背景"><a href="#1、历史背景" class="headerlink" title="1、历史背景"></a>1、历史背景</h2><p>Evelyn Fix(1904-1965) 是一位数学家/统计学家，在伯克利攻读博士学位并继续在那里教授统计学。</p><p>Joseph Lawson Hodges Jr.(1922-2000)也是伯克利的一名统计学家，并从1944年开始参与与美国空军第二十空军（Twentieth Air Force of USAF）的统计合作。</p><p>这两位天才在1951年为美国空军制作的一份技术分析报告中相遇，在那里他们引入了一种非参数分类方法（判别分析）。他们从未正式发表过这篇论文，可能是因为所涉及的工作性质和保密性，特别是考虑到二战后不久的全球气氛。</p><p>接下来，Thomas Cover和Peter Hart在1967年证明了kNN分类的上限错误率</p><h2 id="2、算法模型"><a href="#2、算法模型" class="headerlink" title="2、算法模型"></a>2、算法模型</h2><p>kNN算法在某些条件下是一个通用的函数逼近器，但潜在的概念相对简单。kNN是一种监督学习算法，它在训练阶段简单地存储标记的训练样本。因此，kNN也被称为惰性学习算法，它对训练样本的处理推迟到做预测的时候才进行。<br>假设训练数据集：</p><script type="math/tex; mode=display">T=\{(\mathbf x_,y_1),(\mathbf x_2,y_2),...,(\mathbf x_N,y_N)\}</script><p>其中，${\displaystyle \mathbf x_i\in X\subseteq R^n}$ 表示实例的特征向量，$y\in Y$ 表示实例的类别。给定实例特征向量$\mathbf x$，输出所属的类 $y$ 。</p><p>具体过程：</p><ol><li>通过给定的距离度量，在训练集 $T$ 中找出与 $\mathbf x$ 最近邻的 $k$ 个点，涵盖这 $k$ 个点的 $\mathbf x$ 的领域 记作 $N_k(\mathbf x)$</li><li>随后在 $N_k(\mathbf x)$ 中根据分类决策规则决定 $\mathbf x$ 的类别 $y$。</li></ol><p>可以看出，kNN算法的主要三个要素分别为距离度量、$k$ 值和分类决策规则。</p><h3 id="2-1-距离度量"><a href="#2-1-距离度量" class="headerlink" title="2.1 距离度量"></a>2.1 距离度量</h3><p>距离度量有曼哈顿距离、欧式距离或更一般的闵式距离。</p><p>假设两个特征向量 $\mathbf{x}_i=(x_i^{(1)},x_i^{(2)},…,x_i^{(n)})^T$，$\mathbf{x}_j=(x_j^{(1)},x_j^{(2)},…,x_j^{(n)})^T$，$\mathbf{x}_i,\mathbf{x}_j$ 的 $L_p$距离定义为：</p><script type="math/tex; mode=display">L_p(\mathbf{x}_i,\mathbf{x}_j)=\left(\sum_{l=1}^{n} |x_i^{(l)} - x_j^{(l)}|^p \right)^{\frac{1}{p}}</script><p>当 $p=1$ 时，称为曼哈顿距离，即</p><script type="math/tex; mode=display">L_p(\mathbf{x}_i,\mathbf{x}_j)=\sum_{l=1}^{n} |x_i^{(l)} - x_j^{(l)}|</script><p>当 $p=2$ 时，称为欧式距离，即</p><script type="math/tex; mode=display">L_p(\mathbf{x}_i,\mathbf{x}_j)=\left(\sum_{l=1}^{n} |x_i^{(l)} - x_j^{(l)}|^2 \right)^{\frac{1}{2}}</script><p>当 $p=\infty$，它是各个坐标距离的最大值，即</p><script type="math/tex; mode=display">L_p(\mathbf{x}_i,\mathbf{x}_j)=\max_{l} |x_i^{(l)} - x_j^{(l)}|</script><blockquote><p>范数即为特征向量到原点的距离，表征自身的长度</p><h3 id="2-2-k-值的选择"><a href="#2-2-k-值的选择" class="headerlink" title="2.2 $k$ 值的选择"></a>2.2 $k$ 值的选择</h3><p>如果选择较小的 $k$ 值，相当于用较小的领域中的训练数据进行预测，近似误差（approximation error）会减少，只有与输入实例较近的训练数据才会对预测结果产生影响。但缺点是估计误差（estimation error）会增大，预测结果对近邻的实例点非常敏感，容易发生过拟合。当 $k$ 为1时，称为最近邻算法，对于输入实例，将与其最近的数据点的类作为预测结果。</p></blockquote><p>相反的，当使用较大的 $k$ 值时，意味着距离输入实例较远的训练数据也会对预测结果产生影响，使预测产生错误，容易发生欠拟合。当 $k$ 为N时，无论输入实例是什么，预测结果都将是训练数据中存在最多的类。</p><p>在应用中，$k$ 一般选取一个比较小的值，采用交叉验证法来选取最优的 $k$ 值。</p><h3 id="2-3-决策规则"><a href="#2-3-决策规则" class="headerlink" title="2.3 决策规则"></a>2.3 决策规则</h3><p>kNN算法在分类问题中决策规则往往是“多数表决”，即由输入实例的 $k$ 个近邻的训练实例中的多数类决定输入实例的类。事实上，多数表决（Majority vote）分为简单多数表决和特定多数表决，是要求满足一半数量以上或者特定数量，而不是占比最多的（Plurality vote），在二分类问题中两者没有区别，而在多分类问题中，不需要某一类投票数过半，超过N分之一就可以预测了。</p><p>在回归问题中，可使用“平均法”，即将这k个样本的实值输出标记的平均值作为预测结果，还可基于距离远近进行加权平均或加权投票，距离越近的样本权重越大。</p><h2 id="3、模型优化"><a href="#3、模型优化" class="headerlink" title="3、模型优化"></a>3、模型优化</h2><p>在考虑kNN算法时间复杂度之前，先看一下特征维度对算法的影响。</p><p>假设我们有100个训练实例均匀分布在 $(0,1]$ 区间， 它们的间隔为0.01单元。假设k值为3，就是找到查询点的三个最近邻，期望覆盖特征轴0.03的范围。当我们增加一个维度的时候，总体分布在 $1\times 1$ 的区域中，为了覆盖相同的领域，需要 $0.03^{1/2}\approx 0.17$ 范围的坐标轴区域。当维度为10的时候，这一数值 $0.03^{1/10}\approx 0.704=70.4\%$。可以发现，在高维中我们需要考虑很大的超体积来找到k个近邻样本，这些点与查询点的距离相对较远，变得越来越不“相似”。</p><p>这种现象在机器学习中被称为维度灾难，指的是训练样本大小固定但维度的数量和每个维度的特征值范围不断增加的场景</p><p>kNN算法的时间复杂度是 $O(k<em>N</em>m)$ ，$N$ 是训练样本的数量， $m$ 是训练数据集的特征维度，由于 $N\ll m$ ，时间复杂度简化为 $O(k*N)$，可以看出时间复杂度较高，通过数据结构的方法可以将时间进行优化</p><h3 id="3-1-堆优化"><a href="#3-1-堆优化" class="headerlink" title="3.1 堆优化"></a>3.1 堆优化</h3><p>最暴力的解法是重复k次找到k个最近邻的训练实例，通过堆优化可以将时间复杂度降到 $O(n\log(k))$。</p><p>随机选取k个训练数据集的点来初始化查询点的堆。通过维护一个堆来保存距离查询点最近的k个点。通过遍历数据集，如果该点到查询点的距离比堆中保存的最大的距离还要小，则从堆中剔除最远的点并插入当前点。一旦完成了训练数据集的一次迭代，我们就有了一组k个最近邻的点。</p><h3 id="3-2-桶优化"><a href="#3-2-桶优化" class="headerlink" title="3.2 桶优化"></a>3.2 桶优化</h3><p>上面的优化方法在每次查询时都还是要对训练数据集进行遍历，如果要快速查询还是需要对数据的存储方式进行优化。</p><p>最简单的方法就是分桶（bucketing），我们将搜索空间划分为相同大小的单元格，类似于网格。</p><blockquote><p>突然发现，工作中要把热力点匹配到距离最近的道路点上，同事就是用的种方法，取热力点所在单元格周围的九个格子中的link，再将取到的道路点和热力点进行匹配。</p></blockquote><h3 id="3-3-KD-树优化"><a href="#3-3-KD-树优化" class="headerlink" title="3.3 KD-树优化"></a>3.3 KD-树优化</h3><p>找到k个最近邻的点，本质还是搜索，而提高搜索效率很自然的就会想到二叉树，KD-树就是二叉搜索树（BST）的一种推广。KD-树的时间复杂度平均为 $O(\log(N))$ ，它在笛卡尔坐标系中垂直于特征轴划分搜索空间，这在较低的维度上表现较好，随着特征轴的增多，KD-树将变得低效。</p><p><strong>KD-树构建</strong></p><p>构造KD-树相当于不断用垂直于坐标轴的超平面将k维空间切分，构造一系列的k维超矩形区域。KD-树的每个结点存储一个训练实例点，每一颗子树对应于一个k维超矩形空间。</p><p>二叉搜索树是KD-树在一维空间的特例，任何一个节点相当于一个分割点，左边的比它小，右边的所有节点比它大。在二维空间，一个节点就相当于一条分割线，以此类推，三维空间就是一个面。这种分割方式意味着垂直于一条坐标轴进行分割。分割维度的方式是通过二叉树的深度，第一层根结点分割x轴，第二层分割y轴，以此类推，如果维度遍历完了，下一层又回到x轴，不断重复。</p><p>具体步骤：</p><ol><li>初始化数据集 $T=\{x_1, x_2,…,x_N\}$，KD树深度 $j=1$，数据集的维度为 $k$。</li><li>选择第 $l=j\bmod k + 1$ 维进行分割，找到数据集中所有实例的第 $l$ 维坐标的中位数，把该点作为切分点。</li><li>（可选）不按照固定顺序选择切割维度，而是选取方差最大特征作为分割特征。</li><li>把切分点记录到KD-树节点上，把数据集中该特征值小于中位数的传递给左子树，把大于中位数的传递给右子树。</li><li>递归执行步骤2-4，直到所有数据都被建立到KD-树节点上。</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Node</span>():</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, feature</span>):</span></span><br><span class="line">        self.father = <span class="literal">None</span></span><br><span class="line">        self.feature = feature</span><br><span class="line">        self.left = <span class="literal">None</span></span><br><span class="line">        self.right = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">brother</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">if</span> self == self.father.left:</span><br><span class="line">            <span class="keyword">return</span> self.father.right</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> self.father.left</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__str__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;feature: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(self.feature)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">KDTree</span>():</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, points</span>):</span></span><br><span class="line">        self.root = self.build_tree(points)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build_tree</span>(<span class="params">self, points, dim = <span class="number">0</span>, father = <span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> points:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">        </span><br><span class="line">        points = <span class="built_in">sorted</span>(points, key = <span class="keyword">lambda</span> x: x[dim])</span><br><span class="line">        mid = <span class="built_in">len</span>(points) // <span class="number">2</span></span><br><span class="line">        curNode = Node(points[mid])</span><br><span class="line">        curNode.father = father</span><br><span class="line">        curNode.left  = self.build_tree(points[:mid], (dim + <span class="number">1</span>) % <span class="built_in">len</span>(points[<span class="number">0</span>]), curNode)</span><br><span class="line">        curNode.right = self.build_tree(points[mid + <span class="number">1</span>:], (dim + <span class="number">1</span>) % <span class="built_in">len</span>(points[<span class="number">0</span>]), curNode)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> curNode</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__str__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">inorder</span>(<span class="params">root, depth = <span class="number">0</span></span>):</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">                <span class="keyword">return</span></span><br><span class="line">            ret.append(<span class="string">&#x27;depth: &#123;&#125;, &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(<span class="built_in">str</span>(depth), <span class="built_in">str</span>(root)))</span><br><span class="line">            inorder(root.left, depth + <span class="number">1</span>)</span><br><span class="line">            inorder(root.right, depth + <span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        ret = []</span><br><span class="line">        inorder(self.root)</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;\n&#x27;</span>.join(ret)</span><br><span class="line"></span><br><span class="line">pnts = [[<span class="number">2</span>,<span class="number">3</span>], [<span class="number">5</span>,<span class="number">4</span>], [<span class="number">9</span>,<span class="number">6</span>], [<span class="number">4</span>,<span class="number">7</span>], [<span class="number">8</span>,<span class="number">1</span>], [<span class="number">7</span>,<span class="number">2</span>]]</span><br><span class="line">tree = KDTree(pnts)</span><br><span class="line"><span class="built_in">print</span>(tree)</span><br></pre></td></tr></table></figure><blockquote><p>depth: 0, feature: [7, 2] &lt;/br&gt;<br>depth: 1, feature: [5, 4]<br>depth: 2, feature: [2, 3]<br>depth: 2, feature: [4, 7]<br>depth: 1, feature: [9, 6]<br>depth: 2, feature: [8, 1]</p></blockquote><p><a href="http://www.atyun.com/37601.html">http://www.atyun.com/37601.html</a><br><a href="https://zhuanlan.zhihu.com/p/110066200">https://zhuanlan.zhihu.com/p/110066200</a><br><a href="https://blog.csdn.net/sinat_30353259/article/details/80901746">https://blog.csdn.net/sinat_30353259/article/details/80901746</a><br><a href="http://www.scholarpedia.org/article/K-nearest_neighbor">http://www.scholarpedia.org/article/K-nearest_neighbor</a><br><a href="https://zh.wikipedia.org/wiki/K-%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95">https://zh.wikipedia.org/wiki/K-%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95</a><br><a href="https://zhuanlan.zhihu.com/p/45346117">https://zhuanlan.zhihu.com/p/45346117</a><br><a href="https://sebastianraschka.com/pdf/lecture-notes/stat479fs18/02_knn_notes.pdf">https://sebastianraschka.com/pdf/lecture-notes/stat479fs18/02_knn_notes.pdf</a><br><a href="https://zhuanlan.zhihu.com/p/23966698">https://zhuanlan.zhihu.com/p/23966698</a><br><a href="https://zhuanlan.zhihu.com/p/45346117">https://zhuanlan.zhihu.com/p/45346117</a><br><a href="https://zhuanlan.zhihu.com/p/53826008">https://zhuanlan.zhihu.com/p/53826008</a><br><a href="https://www.cnblogs.com/eyeszjwang/articles/2429382.html">https://www.cnblogs.com/eyeszjwang/articles/2429382.html</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;k近邻算法&quot;&gt;&lt;a href=&quot;#k近邻算法&quot; class=&quot;headerlink&quot; title=&quot;k近邻算法&quot;&gt;&lt;/a&gt;k近邻算法&lt;/h1&gt;&lt;p&gt;k近邻算法是一种基本分类与回归方法&lt;/p&gt;
&lt;h2 id=&quot;1、历史背景&quot;&gt;&lt;a href=&quot;#1、历史背景&quot; cl</summary>
      
    
    
    
    <category term="Machine learning" scheme="http://example.com/categories/Machine-learning/"/>
    
    
  </entry>
  
  <entry>
    <title>Least_Sqaure</title>
    <link href="http://example.com/2021/09/18/Machine%20Learning/Least_Square/"/>
    <id>http://example.com/2021/09/18/Machine%20Learning/Least_Square/</id>
    <published>2021-09-17T16:00:00.000Z</published>
    <updated>2022-01-04T07:08:57.356Z</updated>
    
    <content type="html"><![CDATA[<p>最小二乘法<br><span id="more"></span></p><h1 id="最小二乘法"><a href="#最小二乘法" class="headerlink" title="最小二乘法"></a>最小二乘法</h1><p>最小二乘法又称为最小平方法，是一种数学优化建模方法。它通过最小化误差的平方和寻找数据的最佳函数匹配。最小二乘法是对线性方程组，即方程个数比未知数更多的方程组，以回归分析求得近似解的标准方法。在这个解决方案中，最小二乘法演算为每一方程的结果中，将残差平方和的总和最小化。</p><p>最小二乘法分为线性和非线性，取决于所有未知数中的残差是否为线性。线性的最小二乘问题发生在统计回归分析中，具有一个封闭形式的解决方案。非线性的问题通常由迭代细致化来解决，每次迭代中，系统由线性近似，因此这两种情况下核心演算是相同的。</p><h2 id="1、历史背景"><a href="#1、历史背景" class="headerlink" title="1、历史背景"></a>1、历史背景</h2><p>最小二乘法发展于天文学和大地测量学领域，科学家和数学家尝试为大航海探索时期的海洋航行挑战提供解决方案。准确描述天体的行为是船舰在大海洋上航行的关键，水手不能再依靠陆上目标导航作航行。</p><p>这个方法是在十八世纪期间一些进步的集大成：</p><ul><li>不同观测值的组合是真实值的最佳估计；多次观测会减少误差而不是增加，也许在1722年由Roger Cotes首先阐明。</li><li>在相同条件下采取的不同观察结果，与只尝试记录一次最精确的观察结果是对立的。这个方法被称为平均值方法。托马斯·马耶尔（Tobias Mayer）在1750年研究月球的天平动时，特别使用这种方法，而拉普拉斯（Pierre-Simon Laplace）在1788年他的工作成果中以此解释木星和土星的运动差异。</li><li>在不同条件下进行的不同观测值组合。该方法被称为最小绝对偏差法，出现在Roger Joseph Boscovich在1757年他对地球形体的著名作品，而拉普拉斯在1799年也表示了同样的问题。</li><li>评定对误差达到最小的解决方案标准，拉普拉斯指明了误差的概率密度的数学形式，并定义了误差最小化的估计方法。为此，拉普拉斯使用了一双边对称的指数分布，现在称为拉普拉斯分布作为误差分布的模型，并将绝对偏差之和作为估计误差。他认为这是他最简单的假设，他期待得出算术平均值而成为最佳的估计。可相反地，他的估计是后验中位数。</li></ul><p>1801年，意大利天文学家朱塞普·皮亚齐发现了第一颗小行星谷神星。经过40天的追踪观测后，由于谷神星运行至太阳背后，使得皮亚齐失去了谷神星的位置。随后全世界的科学家利用皮亚齐的观测数据开始寻找谷神星，但是根据大多数人计算的结果来寻找谷神星都没有结果。当年24岁的高斯也计算了谷神星的轨道。奥地利天文学家海因里希·奥伯斯根据高斯计算出来的轨道重新发现了谷神星。</p><p>高斯使用的最小二乘法的方法发表于1809年他的著作《天体运动论》中，而法国科学家勒让德于1806年独立发现“最小二乘法”，但因不为世人所知而没没无闻。两人曾为谁最早创立最小二乘法原理发生争执。</p><p>1829年，高斯提供了最小二乘法的优化效果强于其他方法的证明，见高斯-马尔可夫定理。</p><h2 id="2、问题引入"><a href="#2、问题引入" class="headerlink" title="2、问题引入"></a>2、问题引入</h2><p>如果对同一目标例如手机宽度通过尺子测量长度，通过同一尺子不同测量员或者不同尺子同一测量员得到的结果都有可能不同，那么如何通过所有的测量结果来得到准确的真值呢？</p><p>假设测量结果分别为72.5mm、72.2mm、72.9mm、72.4mm、72.5mm。</p><p>只要做过初中物理实验的都知道，通常都会对同一实验进行多次重复操作，把得到的结果进行平均求和，最后的结果作为实验的准确结果</p><script type="math/tex; mode=display">\overline{x}=\frac{72.5+72.2+72.9+72.4+72.5}{5}=72.5</script><p>直觉告诉我们取平均值是正确的，那么这么做得依据是什么呢？</p><p>再比如，我们知道营业税税收总额与社会商品零售总额有关，假设收集了如下数据</p><div class="table-container"><table><thead><tr><th style="text-align:center">序号</th><th style="text-align:center">社会商品零售总额</th><th style="text-align:center">营业税税收总额</th></tr></thead><tbody><tr><td style="text-align:center">1</td><td style="text-align:center">142.08</td><td style="text-align:center">3.93</td></tr><tr><td style="text-align:center">2</td><td style="text-align:center">177.30</td><td style="text-align:center">5.96</td></tr><tr><td style="text-align:center">3</td><td style="text-align:center">204.68</td><td style="text-align:center">7.85</td></tr><tr><td style="text-align:center">4</td><td style="text-align:center">242.88</td><td style="text-align:center">9.82</td></tr><tr><td style="text-align:center">5</td><td style="text-align:center">316.24</td><td style="text-align:center">12.50</td></tr><tr><td style="text-align:center">6</td><td style="text-align:center">341.99</td><td style="text-align:center">15.55</td></tr><tr><td style="text-align:center">7</td><td style="text-align:center">332.69</td><td style="text-align:center">15.79</td></tr><tr><td style="text-align:center">8</td><td style="text-align:center">389.29</td><td style="text-align:center">16.39</td></tr><tr><td style="text-align:center">9</td><td style="text-align:center">453.40</td><td style="text-align:center">18.45</td></tr></tbody></table></div><p>如何通过这些已有数据在给定社会商品零售总额的情况下预测营业税税收总额呢？</p><h2 id="3、问题求解"><a href="#3、问题求解" class="headerlink" title="3、问题求解"></a>3、问题求解</h2><p>我们将第一个问题用公式进行描述，假设要猜测的真实值是$y$，测量值则为$y_i$，上述求平均公式则为：</p><script type="math/tex; mode=display">\displaystyle{\frac{\sum y_i}{n}}=y</script><p>公式可以转化为：</p><script type="math/tex; mode=display">\sum (y_i-y)=0</script><p>满足上式时，误差平方和取得最小值，因为导数为0</p><script type="math/tex; mode=display">\frac{d}{dy}S_{\epsilon^2}=\frac{d}{dy}\sum (y_i-y)^2=0</script><p>至此，我们明白求平均的实质是希望预测值与测量值的误差平方和最小，最小二乘法也就是最小平方法，二乘实际上也是日语中平方的意思。</p><p>第二个问题中，社会商品零售总额$x$是自变量，其值是可以控制或精确测量的，是非随机变量；营业税税收总额$y$是因变量，取值不能事先确定，是随机变量。假定它们具有线性相关关系，$y_i$的表达式：</p><script type="math/tex; mode=display">{\displaystyle{y_i=\beta_0+\beta_1x_i+\epsilon_i}}</script><p>其中$\beta_0,\beta_1$称为回归系数，由于它们未知，因此需要从收集到的数据出发进行估计，记为$\hat{\beta_0},\hat{\beta_1}$。</p><p>通过最小二乘法估计回归系数，目标是希望偏差平方和$Q$达到最小：</p><script type="math/tex; mode=display">Q(\beta_0,\beta_1)=\sum_{i=1}^n (y_i-\beta_0-\beta_1x_i)^2</script><p>由于$Q$是一个非负二次型，因此可通过令$Q$对$\beta_0,\beta_1$的偏导为零来求：</p><script type="math/tex; mode=display">\begin{cases}    {\displaystyle \frac{\partial Q}{\partial \beta_0}=-2\sum(y_i-\beta_0-\beta_1x_i)} \\    \\    {\displaystyle \frac{\partial Q}{\partial \beta_1}=-2\sum(y_i-\beta_0-\beta_1x_i)x_i}\end{cases}</script><p>经整理有</p><script type="math/tex; mode=display">\begin{cases}    {\displaystyle n\beta_0 + n\overline{x}\cdot\beta_1}=n\overline{y} \\    \\    {\displaystyle n\overline{x}\beta_0+\sum{x_i^2\beta_1}=\sum{x_iy_i}}\end{cases}</script><p>两式合并后</p><script type="math/tex; mode=display">\begin{aligned}    \left( \sum{x_i^2} -n\overline{x}^2 \right)\beta_1&=\sum{x_iy_i}-n\overline{x}\overline{y}\\    \left( \sum{x_i^2} + \sum{\overline{x}^2}-2n\overline{x}^2 \right)\beta_1&=\sum{x_iy_i}-\sum{x_i}\overline{y}\\    \left( \sum{x_i^2} + \sum{\overline{x}^2}-2\sum{x_i}\overline{x} \right)\beta_1&=\sum{x_iy_i}-\sum{x_i}\overline{y}+n\overline{x}\overline{y}-n\overline{x}\overline{y}\\    \left( \sum{(x_i -\overline{x})^2} \right)\beta_1&=\sum{x_iy_i}-\sum{x_i}\overline{y}+\sum{\overline{x}y_i}-\sum\overline{x}\overline{y}\\    \left( \sum{(x_i -\overline{x})^2} \right)\beta_1&=\sum{(x_i-\overline{x})(y_i-\overline{y})}\\    \beta_1&=\frac{cov(x,y)}{Var(x)}\end{aligned}</script><p>实际上 $\beta_1$ 是 $x$ 与 $y$ 的协方差与 $x$ 方差的商</p><p>通过以上的方法可以推导出更多特征的求解方法，通过高斯消元法可以求解多元线性方程组，求得解析解。</p><p>同时也可以通过矩阵法求解，将多个方程看做一个整体进行求解。</p><script type="math/tex; mode=display">{\displaystyle     {\begin{pmatrix}    1& x_{11}& \cdots & x_{1j}\cdots & x_{1q}\\    1& x_{21}& \cdots & x_{2j}\cdots & x_{2q}\\    \vdots \\    1& x_{i1}& \cdots & x_{ij}\cdots & x_{iq}\\    \vdots \\    1& x_{n1}& \cdots & x_{nj}\cdots & x_{nq}    \end{pmatrix}}     \cdot     {\begin{pmatrix}\beta_{0}\\\beta_{1}\\\beta_{2}\\\vdots \\\beta_{j}\\\vdots \\\beta_{q}\end{pmatrix}}=    {\begin{pmatrix}y_{1}\\y_{2}\\\vdots \\y_{i}\\\vdots \\y_{n}\end{pmatrix}}}</script><p>矩阵表达式为：</p><script type="math/tex; mode=display">    Q=min{||Xw-y||}^2</script><p>求 $w$ 的最小二乘估计，即求 $\frac{\partial Q}{\partial w}$ 的零点。其中 $y$ 是 $m\times 1$ 列向量，$X$ 是 $m\times n$ 矩阵，$w$是$n\times 1$列向量，$Q$是标量。</p><p>将向量模平方改写成向量与自身的内积：</p><script type="math/tex; mode=display">Q=(Xw-y)^T(Xw-y)</script><p>求微分：</p><script type="math/tex; mode=display">\begin{aligned}    dQ&=(Xdw)^T(Xw-y)+(Xw-y)^T(Xdw)\\    &=2(Xw-y)^T(Xdw)\end{aligned}</script><p>这里是因为两个向量的内积满足$u^Tv=v^Tu$。</p><p>导数与微分的关系式</p><script type="math/tex; mode=display">dQ={\frac{\partial Q}{\partial w}}^Tdw</script><p>得到</p><script type="math/tex; mode=display">{\frac{\partial Q}{\partial w}}=(2(Xw-y)^TX)^T=2X^T(Xw-y)=0</script><p>求解可得</p><script type="math/tex; mode=display">\begin{aligned}    X^TXw&=X^Ty\\    w&=(X^TX)^{-1}X^Ty\end{aligned}</script><h2 id="4、思考"><a href="#4、思考" class="headerlink" title="4、思考"></a>4、思考</h2><p>为什么损失函数不用误差的绝对值？</p><script type="math/tex; mode=display">Q(\beta_0,\beta_1)=\sum_{i=1}^n |y_i-\beta_0-\beta_1x_i|</script><p>网上有人说是太麻烦，但我觉得主要原因是<strong>绝对值函数连续但不可导</strong>，无法通过求导数的为零的极值点，也无法通过梯度下降法进行求解。</p><h2 id="5、算例"><a href="#5、算例" class="headerlink" title="5、算例"></a>5、算例</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line">x = np.array([<span class="number">142.08</span>, <span class="number">177.30</span>, <span class="number">204.68</span>, <span class="number">242.88</span>, <span class="number">316.24</span>, <span class="number">332.69</span>, <span class="number">341.99</span>, <span class="number">389.29</span>, <span class="number">453.40</span>])</span><br><span class="line">y = np.array([<span class="number">3.93</span>,   <span class="number">5.96</span>,   <span class="number">7.85</span>,   <span class="number">9.82</span>,   <span class="number">12.50</span>,  <span class="number">15.79</span>,  <span class="number">15.55</span>,  <span class="number">16.39</span>,  <span class="number">18.45</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过代数方法求解</span></span><br><span class="line"><span class="comment"># numpy的协方差默认是样本方差，无偏的，自由度n-1，因此要加bias = True</span></span><br><span class="line">beta_0 = np.cov(x, y, bias = <span class="literal">True</span>)[<span class="number">0</span>,<span class="number">1</span>] / np.var(x)  </span><br><span class="line">beta_1 = np.<span class="built_in">sum</span>(y) / <span class="number">9</span> - np.<span class="built_in">sum</span>(x) / <span class="number">9</span> * beta_0</span><br><span class="line"><span class="built_in">print</span>(beta_0, beta_1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过公式计算，与上面相同</span></span><br><span class="line"><span class="comment"># a = np.sum(np.multiply(x, y)) - np.sum(x) * np.sum(y) / 9</span></span><br><span class="line"><span class="comment"># b = np.sum(np.multiply(x, x)) - np.sum(x) * np.sum(x) / 9</span></span><br></pre></td></tr></table></figure><blockquote><p>计算结果<br>0.04867773628668675 -2.2609874555936926</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过矩阵法实现最小二乘法</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">least_sqaure</span>(<span class="params">X, Y</span>):</span></span><br><span class="line">    <span class="keyword">return</span> (X.T * X).I * X.T * Y</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成多项式</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">polynomial</span>(<span class="params">x, n</span>):</span></span><br><span class="line">    X = np.mat(x)</span><br><span class="line">    X = np.append(np.ones((<span class="number">1</span>, <span class="number">9</span>)), X, axis = <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n):</span><br><span class="line">        X = np.append(X, np.mat(x**(i + <span class="number">1</span>)), axis = <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> X.T</span><br><span class="line"></span><br><span class="line">Y = np.mat(y).T</span><br><span class="line"></span><br><span class="line"><span class="comment"># 线性拟合</span></span><br><span class="line">X = polynomial(x, <span class="number">1</span>)</span><br><span class="line">beta = np.array(least_sqaure(X, Y)).flatten()[::-<span class="number">1</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;beta:&#x27;</span>, beta)</span><br><span class="line">plt.subplot(<span class="number">221</span>)</span><br><span class="line">plt.plot(x, y, <span class="string">&#x27;bo&#x27;</span>, label=<span class="string">&#x27;noise&#x27;</span>)</span><br><span class="line">plt.plot(x, np.poly1d(beta)(x), label=<span class="string">&#x27;fitted curve&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 二次拟合</span></span><br><span class="line">X = polynomial(x, <span class="number">2</span>)</span><br><span class="line">beta = np.array(least_sqaure(X, Y)).flatten()[::-<span class="number">1</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;beta:&#x27;</span>, beta)</span><br><span class="line">plt.subplot(<span class="number">222</span>)</span><br><span class="line">plt.plot(x, y, <span class="string">&#x27;bo&#x27;</span>, label=<span class="string">&#x27;noise&#x27;</span>)</span><br><span class="line">plt.plot(x, np.poly1d(beta)(x), label=<span class="string">&#x27;fitted curve&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 三次拟合</span></span><br><span class="line">X = polynomial(x, <span class="number">3</span>)</span><br><span class="line">beta = np.array(least_sqaure(X, Y)).flatten()[::-<span class="number">1</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;beta:&#x27;</span>, beta)</span><br><span class="line">plt.subplot(<span class="number">223</span>)</span><br><span class="line">plt.plot(x, y, <span class="string">&#x27;bo&#x27;</span>, label=<span class="string">&#x27;noise&#x27;</span>)</span><br><span class="line">plt.plot(x, np.poly1d(beta)(x), label=<span class="string">&#x27;fitted curve&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 六次拟合</span></span><br><span class="line">X = polynomial(x, <span class="number">6</span>)</span><br><span class="line">beta = np.array(least_sqaure(X, Y)).flatten()[::-<span class="number">1</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;beta:&#x27;</span>, beta)</span><br><span class="line">plt.subplot(<span class="number">224</span>)</span><br><span class="line">plt.plot(x, y, <span class="string">&#x27;bo&#x27;</span>, label=<span class="string">&#x27;noise&#x27;</span>)</span><br><span class="line">plt.plot(x, np.poly1d(beta)(x), label=<span class="string">&#x27;fitted curve&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/He1o/Cyrus_NoteBook/main/Machine%20Learning/1.least_square/source/Figure_1.png" alt="最小二乘法拟合图"></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://www.zhihu.com/question/37031188">1. 最小二乘法的本质是什么？</a></p><p><a href="https://www.cnblogs.com/pinard/p/5976811.html">2. 最小二乘法小结-刘建平Pinard</a></p><p><a href="https://zh.wikipedia.org/wiki/%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95">3. 最小二乘法-维基百科</a></p><p><a href="https://zhuanlan.zhihu.com/p/84133777">4. 最小二乘法详细推导 - 知乎</a></p><p><a href="https://zhuanlan.zhihu.com/p/89373759">5. 最小二乘法 - 知乎</a></p><p><a href="https://zhuanlan.zhihu.com/p/87582571">6. 矩阵形式下的最小二乘法推导 - 知乎</a></p><p><a href="https://zhuanlan.zhihu.com/p/24709748">7. 矩阵求导术（上）- 知乎</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;最小二乘法&lt;br&gt;</summary>
    
    
    
    <category term="Machine learning" scheme="http://example.com/categories/Machine-learning/"/>
    
    
  </entry>
  
  <entry>
    <title>Perceptron</title>
    <link href="http://example.com/2021/09/18/Machine%20Learning/Perceptrom/"/>
    <id>http://example.com/2021/09/18/Machine%20Learning/Perceptrom/</id>
    <published>2021-09-17T16:00:00.000Z</published>
    <updated>2022-01-04T01:38:11.921Z</updated>
    
    <content type="html"><![CDATA[<p>感知机<br><span id="more"></span></p><h1 id="感知机"><a href="#感知机" class="headerlink" title="感知机"></a>感知机</h1><p>感知机（英语：Perceptron）是弗兰克·罗森布拉特（Frank Rosenblatt）在1957年就职于康奈尔航空实验室（Cornell Aeronautical Laboratory）时所发明的一种人工神经网络。在人工神经网络领域中，感知机也被指为单层的人工神经网络，以区别于较复杂的多层感知机（Multilayer Perceptron）。</p><p>感知机作为一种二分分类的线性分类模型，其输入为实例的特征向量，输出为实例的类别，取+1或-1二值。感知机学习旨在求出将训练数据进行线性划分的分离超平面，因此，导入基于误分类的损失函数，利用梯度下降法对损失函数进行极小化，求得感知机模型。（单层）感知机可说是最简单的前向人工神经网络形式。尽管结构简单，感知机能够学习并解决相当复杂的问题。感知机主要的本质缺陷是它不能处理线性不可分问题。</p><p>感知机是生物神经细胞的简单抽象。神经细胞结构大致可分为：树突、突触、细胞体及轴突。单个神经细胞可被视为一种只有两种状态的机器——激活时为‘是’，而未激活时为‘否’。神经细胞的状态取决于从其它的神经细胞收到的输入信号量，及突触的强度（抑制或加强）。当信号量总和超过了某个阈值时，细胞体就会激活，产生电脉冲。电脉冲沿着轴突并通过突触传递到其它神经元。为了模拟神经细胞行为，与之对应的感知机基础概念被提出，如权量（突触）、偏置（阈值）及激活函数（细胞体）。</p><h2 id="1、历史背景"><a href="#1、历史背景" class="headerlink" title="1、历史背景"></a>1、历史背景</h2><p>1943年，心理学家沃伦·麦卡洛克和数理逻辑学家沃尔特·皮茨在合作的《A logical calculus of the ideas immanent in nervous activity》[1]论文中提出并给出了人工神经网络的概念及人工神经元的数学模型，从而开创了人工神经网络研究的时代。</p><p>1949年，心理学家唐纳德·赫布在《The Organization of Behavior》[2]论文中描述了神经元学习法则——赫布型学习。</p><p>人工神经网络更进一步被美国神经学家弗兰克·罗森布拉特所发展。他提出了可以模拟人类感知能力的机器，并称之为‘感知机’。1957年，在Cornell航空实验室中，他成功在IBM 704机上完成了感知机的仿真。两年后，他又成功实现了能够识别一些英文字母、基于感知机的神经计算机——Mark1，并于1960年6月23日，展示与众。</p><p>为了‘教导’感知机识别图像，弗兰克·罗森布拉特在Hebb学习法则的基础上，发展了一种迭代、试错、类似于人类学习过程的学习算法——感知机学习。除了能够识别出现较多次的字母，感知机也能对不同书写方式的字母图像进行概括和归纳。但是，由于本身的局限，感知机除了那些包含在训练集里的图像以外，不能对受干扰（半遮蔽、不同大小、平移、旋转）的字母图像进行可靠的识别。</p><p>首个有关感知机的成果，由弗兰克·罗森布拉特于1958年发表在《The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain》的文章里。1962年，他又出版了《Principles of Neurodynamics: Perceptrons and the theory of brain mechanisms》一书，向大众深入解释感知机的理论知识及背景假设。此书介绍了一些重要的概念及定理证明，例如感知机收敛定理。</p><p>虽然最初被认为有着良好的发展潜能，但感知机最终被证明不能处理诸多的模式识别问题。1969年，马文·明斯基和西摩尔·派普特在《Perceptrons》书中，仔细分析了以感知机为代表的单层神经网络系统的功能及局限，证明感知机不能解决简单的异或（XOR）等线性不可分问题，但弗兰克·罗森布拉特和马文·明斯基和西摩尔·派普特等人在当时已经了解到多层神经网络能够解决线性不可分的问题。</p><p>由于弗兰克·罗森布拉特等人没能够及时推广感知机学习算法到多层神经网络上，又由于《Perceptrons》在研究领域中的巨大影响，及人们对书中论点的误解，造成了人工神经领域发展的长年停滞及低潮，直到人们认识到多层感知机没有单层感知机固有的缺陷及反向传播算法在80年代的提出，才有所恢复。1987年，书中的错误得到了校正，并更名再版为《Perceptrons - Expanded Edition》。</p><p>近年，在Freund及Schapire（1998）使用核技巧改进感知机学习算法之后，愈来愈多的人对感知机学习算法产生兴趣。后来的研究表明除了二元分类，感知机也能应用在较复杂、被称为structured learning类型的任务上（Collins, 2002），又或使用在分布式计算环境中的大规模机器学习问题上（McDonald, Hall and Mann, 2011）。</p><h2 id="2、感知机模型"><a href="#2、感知机模型" class="headerlink" title="2、感知机模型"></a>2、感知机模型</h2><p>假设输入空间（特征空间）是 ${\displaystyle X\subseteq R^n}$ （代表n维的实数空间），输出空间是 $Y=\{+1, -1\}$ 。输入 $\mathbf x\in X$ 表示实例的特征向量，对应于输入空间（特征空间）的点；输出 $\mathbf y\in Y$ 表示实例的类别。把矩阵上的输入 $x$ 映射到输出值$f(x)$上。</p><script type="math/tex; mode=display">{\displaystyle f(x)=\text{sign}(\mathbf w\cdot \mathbf x+b)}</script><p>$\mathbf w$ 是实数的表示权重的向量，与 $\mathbf x$ 维度相同，$\mathbf {w\cdot x}$ 是点积，$b$ 是偏置，一个不依赖于任何输入值的常数。偏置可以认为是激励函数的偏移量，或者给神经元一个基础活跃等级。sign是符号函数：</p><script type="math/tex; mode=display">{\displaystyle \text{sign}(n)={    \begin{cases}        +1&n\geq 0\\        -1&n<0    \end{cases}    }}</script><p>映射函数同样可以写成如下的表述形式：</p><script type="math/tex; mode=display">{\displaystyle y=\text{sign}(\sum_{i=1}^{n}{ {w}_{i}{x}_{i}+b})=\text{sign}( {W}^{T} {X})}</script><h2 id="3、感知机学习算法"><a href="#3、感知机学习算法" class="headerlink" title="3、感知机学习算法"></a>3、感知机学习算法</h2><p>假设训练数据是线性可分的，感知机的学习目标是寻找模型参数$w,b$来将数据集正负实例点通过超平面进行区分，因此需要一个损失函数并得到损失函数的极小值。</p><blockquote><p>我们很自然的想到能否还用最小二乘法中的损失函数呢？</p><script type="math/tex; mode=display">Q=min{||\text{sign}(\mathbf w\cdot \mathbf x+b)-y||}^2</script><p>感知机与最小二乘法的不同之处在于 $y$ 即映射函数 $f(x)$ 的不同。最小二乘法是线性函数，用于线性回归；而感知机是sign函数，用于分类。很明显sign是个阶跃的不连续函数，这就导致损失函数同样是不连续的，就无法通过微分来得到极值点。</p><p>为了减少输入，以下还是用不加粗的字母 $w$ 取代向量 $\mathbf w$。</p></blockquote><p>损失函数还有一个自然选择是误分类点的总数，但同样这种损失函数也不是参数 $w,b$ 的连续可导函数。损失函数的另一个选择是误分类点到超平面的距离，通过距离公式可以得到任一点 $x_0$ 到超平面 $S$ 的距离：</p><script type="math/tex; mode=display">\frac{1}{\left\|w\right\|}|w\cdot x_0+b|</script><blockquote><p>在一维空间，一个线性函数如 $Ax+By+C=0$ 的法向量就是 $(A,B)$，点到直线距离相当于该点到法向量投影的长度，假设点 $Q$ 坐标 $(x_0,y_0)$，直线上任意一点坐标 $P(x,y)$</p><script type="math/tex; mode=display">d=|PQ|\cdot \cos\theta</script><script type="math/tex; mode=display">d=\frac{|n|\cdot|PQ|\cdot \cos\theta}{|n|}</script><script type="math/tex; mode=display">d=\frac{\vec{n}\cdot \vec{PQ}}{|n|}</script><p>同理可以推广到多维空间</p><p><strong>记住一点：</strong></p><p><strong>权重向量是超平面的法线</strong></p></blockquote><p>对于误分类的数据$(x_i,y_i)$，当$w\cdot x_i+b&gt;0$，$y_i=-1$；反之，$y_i=+1$，因此任一点$x_i$到超平面$S$的距离是</p><script type="math/tex; mode=display">-\frac{1}{\left\|w\right\|}y_i(w\cdot x_i+b)</script><p>不考虑$\displaystyle \frac{1}{\left|w\right|}$， 感知机学习的损失函数定义为</p><script type="math/tex; mode=display">L(w,b)=-\sum_{x_i\in M}y_i(w\cdot x_i+b)</script><p>$M$ 是误分类点的集合。</p><p>显然，损失函数是非负的。如果没有误分类点，损失函数值是0。而且，误分类点越少，误分类点离超平面越近，损失函数值就越小。注意，在分类正确时，损失函数为0。因此，给定训练数据$T$，损失函数$L(w,b)$ 是$w,b$ 的连续可导函数。</p><p>感知机学习算法采用随机梯度下降法。首先选取一个超平面$w_0,b_0$，然后用梯度下降法不断地极小化目标函数。极小化过程不是一次使$M$中所有误分类点的梯度下降，而是一次随机选取一个误分类点使其梯度下降。</p><p>损失函数 $L(w,b)$ 的梯度：</p><script type="math/tex; mode=display">\bigtriangledown_w L(w,b)=-\sum_{x_i\in M}y_ix_i</script><script type="math/tex; mode=display">\bigtriangledown_b L(w,b)=-\sum_{x_i\in M}y_i</script><p>随机选取一个误分类点$(x_i,y_i)$，对 $w,b$ 进行更新：</p><script type="math/tex; mode=display">w_{t+1}\leftarrow w_{t}+ \eta y_ix_i</script><script type="math/tex; mode=display">b_{t+1}\leftarrow b_{t}+ \eta y_i</script><p>式中$\eta$是步长，在统计学习中称为学习率。通过迭代可以期待损失函数不断减少。</p><p>为了便于推导，可以将偏置 $b$ 并入权重向量 $w$，将 $d$ 维的输入转为 $(d+1)$ 维的输入，同时传入到对应的 $(d+1)$ 维感知机当中。</p><script type="math/tex; mode=display">w\cdot x+b=[w,b]\cdot [x,1]=\mathbf w\cdot \mathbf x</script><p>总结算法流程如下:</p><ol><li>初始化参数，迭代次数$t=1$并且$\mathbf w_1$为全是0的权重向量。</li><li>对每一个样本 $\mathbf x_t$ 预测，$f(x_t)=+1$ if $\mathbf w_t\cdot \mathbf x_t\ge 0$ else $-1$</li><li>对一个分类错误的样本 $y_i(\mathbf w_t \cdot \mathbf x_i) \le 0$ 进行学习，$\mathbf w_{t+1}\leftarrow \mathbf w_{t}+ \eta y_i\mathbf x_i$ </li><li>$t\leftarrow t + 1$</li></ol><h2 id="4、算法收敛性"><a href="#4、算法收敛性" class="headerlink" title="4、算法收敛性"></a>4、算法收敛性</h2><p>在证明感知机算法收敛性之前，先证明两个假设。<br><strong>定理1(线性可分性).</strong> 对于线性可分的数据集，存在 $\mathbf w^\ast$ 满足 $\left|\mathbf w^\ast \right|=1$，使得超平面 $\mathbf w^\ast \cdot \mathbf x=0$ 能够将所有训练数据完全正确分开，且存在 $\gamma &gt; 0$，对于所有 $i\in \{1,2,…,n\}$，</p><script type="math/tex; mode=display">y_i(\mathbf w^\ast \cdot \mathbf x_i) > \gamma</script><p>证明：由于训练数据集是线性可分的，则存在超平面将数据集完全分开，取此超平面为 $\mathbf w^* \cdot \mathbf x=0$，通过向量单位化使 $\left|\mathbf w^\ast \right|=1$ 。由于对于有限的数据集 $i\in \{1,2,…,n\}$，均有</p><script type="math/tex; mode=display">y_i(\mathbf w^\ast \cdot \mathbf x_i) > 0</script><p>所以存在</p><script type="math/tex; mode=display">y_i(\mathbf w^\ast \cdot \mathbf x_i) \ge \min_i(y_i(\mathbf w^\ast \cdot \mathbf x_i)) > \gamma</script><p><strong>定理2(有界性).</strong> 存在 ${R \in R^n}$ 对于有限的数据集 $i\in \{1,2,…,n\}$，均有</p><script type="math/tex; mode=display">\left\|\mathbf x_i \right\| \le R</script><p><strong>定理3(收敛性).</strong> 感知机学习算法最多迭代次数（之后得到一个分离超平面），满足不等式：</p><script type="math/tex; mode=display">k\le \left( \frac{R}{\gamma}\right) ^2</script><p>证明：我们需要推导出 $k$ 的上边界是上式，策略是根据 $k$ 推导出 $\mathbf w_{k+1}$ 长度的上下限并将它们关联起来。</p><p>注意到 $\mathbf w_{1}=0$，对于 $k\ge1$ ，如果 $\mathbf{x}_j$ 是迭代 $k$ 期间的误分类点，由定理1，我们有:</p><script type="math/tex; mode=display">\begin{aligned}    \mathbf w_{k+1} \cdot \mathbf{w}^*&=(\mathbf{w}_k + \eta y_j\mathbf{x}_j)\cdot \mathbf{w}^* \\    &=\mathbf w_{k} \cdot \mathbf{w}^* + \eta y_j(\mathbf{x}_j \cdot \mathbf{w}^*) \\    &>\mathbf w_{k} \cdot \mathbf{w}^* + \eta \gamma \\    &>\mathbf w_{k-1} \cdot \mathbf{w}^* + 2\eta \gamma \\    &>k\eta \gamma\end{aligned}</script><p>由于</p><script type="math/tex; mode=display">\begin{aligned}\mathbf w_{k+1} \cdot \mathbf{w}^* &= \left\|\mathbf w_{k+1} \right\|\left\|\mathbf w^* \right\| \cos \theta \\&\le \left\|\mathbf w_{k+1} \right\| \left\|\mathbf w^* \right\| = \left\|\mathbf w_{k+1} \right\|\end{aligned}</script><p>我们有：</p><script type="math/tex; mode=display">\left\|\mathbf w_{k+1} \right\| >k\eta \gamma</script><p>至此我们得到了$\left|\mathbf w_{k+1} \right|$的下界，为了得到上界，我们推断：</p><script type="math/tex; mode=display">\begin{aligned}\left\|\mathbf w_{k+1} \right\|^2 &=\left\|\mathbf w_{k}+\eta y_j\mathbf{x}_j \right\|^2 \\&=\left\|\mathbf w_{k}\right\|^2 + \left\| \eta y_j\mathbf{x}_j \right\|^2 + 2(\mathbf w_{k}\cdot \mathbf x_{j})\eta y_j \\&=\left\|\mathbf w_{k}\right\|^2 + \left\| \eta \mathbf{x}_j \right\|^2 + 2(\mathbf w_{k}\cdot \mathbf x_{j})\eta y_j \\&\le \left\|\mathbf w_{k}\right\|^2 + \left\| \eta \mathbf{x}_j \right\|^2 \\&\le \left\|\mathbf w_{k}\right\|^2 + \eta ^2R^2 \\&\le \left\|\mathbf w_{k-1}\right\|^2 + 2\eta ^2R^2 \\&\le k\eta ^2R^2 \\\end{aligned}</script><p>联立$\left|\mathbf w_{k+1} \right|$的上下界，我们得到不等式：</p><script type="math/tex; mode=display">(k\eta \gamma)^2<\left\|\mathbf w_{k+1} \right\|\le k\eta ^2R^2</script><p>最终得到：</p><script type="math/tex; mode=display">k\le \left( \frac{R}{\gamma}\right) ^2</script><p>Our proof is done!</p><p><a href="https://zh.wikipedia.org/wiki/%E6%84%9F%E7%9F%A5%E5%99%A8">1. 感知器-维基百科</a></p><p><a href="https://www.cnblogs.com/graphics/archive/2010/07/10/1774809.html">2. 点到平面的距离公式</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;感知机&lt;br&gt;</summary>
    
    
    
    <category term="Machine learning" scheme="http://example.com/categories/Machine-learning/"/>
    
    
  </entry>
  
  <entry>
    <title>1.random events and their probabilities</title>
    <link href="http://example.com/2021/09/18/Probability%20Theory%20and%20Statistics/1.random%20events%20and%20their%20probabilities/"/>
    <id>http://example.com/2021/09/18/Probability%20Theory%20and%20Statistics/1.random%20events%20and%20their%20probabilities/</id>
    <published>2021-09-17T16:00:00.000Z</published>
    <updated>2022-01-01T02:36:18.000Z</updated>
    
    
    
    
    <category term="Probability Theory and Statistics" scheme="http://example.com/categories/Probability-Theory-and-Statistics/"/>
    
    
  </entry>
  
</feed>
