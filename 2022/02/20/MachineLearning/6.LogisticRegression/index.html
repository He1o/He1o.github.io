<!DOCTYPE html>
<html lang="zh-Hans">

<!-- Head tag -->
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="icon" href="https://s3.bmp.ovh/imgs/2022/01/7b8dab551214d77c.png" type="image/png">
    <link rel="shortcut icon" href="https://s3.bmp.ovh/imgs/2022/01/7b8dab551214d77c.png" type="image/png">


    <!--Description-->
    
        <meta name="description" content="逻辑回归1. 历史背景1830 年代和 1840 年代，在 Adolphe Quetelet 的指导下，Pierre François Verhulst 将逻辑函数（logistic function）作为人口增长模型并命名为“逻辑函数”。Verhulst 在 1830 年代中期首先设计了这个函数，">
    

    <!--Author-->
    
        <meta name="author" content="He1o">
    

    <!-- Title -->
    
    <title>6. Logistic Regression | He1o</title>

    <!-- Bootstrap Core CSS -->
    <link href="//cdn.jsdelivr.net/npm/bootstrap@3.3.6/dist/css/bootstrap.min.css" rel="stylesheet">
    <!-- <link href="//cdn.bootcss.com/bootstrap/3.3.6/css/bootstrap.min.css" rel="stylesheet"> -->

    <!-- Custom CSS -->
    
<link rel="stylesheet" href="/css/style.css">


    <!-- Custom Fonts -->
    <link href="//cdn.jsdelivr.net/npm/font-awesome@4.5.0/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <!-- <link href="//cdn.bootcss.com/font-awesome/4.5.0/css/font-awesome.min.css" rel="stylesheet" type="text/css"> -->
    <link href='//fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>
    <link href='//fonts.googleapis.com/css?family=Noto+Serif:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>

    <!-- jQuery -->
    <script src="//cdn.bootcss.com/jquery/2.2.1/jquery.min.js"></script>
    <!-- Bootstrap -->
    <script src="//cdn.jsdelivr.net/npm/bootstrap@3.3.6/dist/js/bootstrap.min.js"></script>
    <!-- <script src="//cdn.bootcss.com/bootstrap/3.3.6/js/bootstrap.min.js"></script> -->

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
    <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->
    <link rel="stylesheet"
        href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/base16/tomorrow.min.css">
    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
    <script >hljs.initHighlightingOnLoad();</script> 
<meta name="generator" content="Hexo 5.4.0"><link rel="alternate" href="/atom.xml" title="He1o" type="application/atom+xml">
</head>

<body>

    <!-- Content -->
    <section class="article-container">
    <!-- Back Home -->
    <a class="nav-back" href="/">
    <!-- <i class="fa fa-puzzle-piece"></i> -->
    <!-- <svg t="1641282408158" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="1990" width="20" height="20"><path d="M176.01639 524.791207s-27.260759-212.767201-4.060113-284.047483c13.969751-42.859491 33.233691-71.366668 76.352339-70.132591 70.490473 2.048568 194.416477 193.34283 194.416476 193.34283L192.478976 529.591766z" fill="#C95065" p-id="1991"></path><path d="M193.490919 536.329826l-23.077238-6.812104-0.518312-3.998409c-1.12301-8.737265-27.149692-214.914494-3.813298-286.688408 12.291406-37.614664 31.654073-75.846367 82.399315-74.390156 31.098738 0.900876 74.229726 34.430746 128.220592 99.664051a1175.61869 1175.61869 0 0 1 71.206238 96.480133l3.319666 5.121419z m-11.847138-16.36386l9.872615 2.900081L434.172939 362.201574c-9.058125-13.574846-35.380985-52.078046-67.047398-90.309749-50.86865-61.444689-92.012775-94.332839-119.002037-95.122648-34.788628-0.999602-54.521518 17.462188-70.342384 65.887366-20.781855 63.86348 0.296178 247.839666 3.862661 277.309423z" fill="#A5213D" p-id="1992"></path><path d="M847.971269 524.791207s27.310122-212.705497 4.072454-283.985779c-13.982091-42.921195-33.184328-71.428372-76.37702-70.194295-70.502814 2.048568-194.428818 193.34283-194.428818 193.34283l250.270798 165.637803z" fill="#C95065" p-id="1993"></path><path d="M830.49674 536.329826l-2.394109-1.591959-255.330513-169.068537 3.319666-5.121419a1170.361522 1170.361522 0 0 1 71.23092-96.443111c53.990865-65.233306 97.134194-98.726153 128.220591-99.664051 50.782265-1.468552 70.132591 36.775492 82.399316 74.390156 23.447461 71.773913-2.690288 277.951143-3.813298 286.688408l-0.505972 3.998409z m-240.644998-174.128252l242.668884 160.664473 9.872616-2.900081c3.566482-29.482097 24.681538-213.495306 3.825638-277.309423-15.783844-48.425178-35.504393-66.886969-70.342384-65.887366-26.989262 0.789809-68.133386 33.677959-119.002037 95.122648-31.666414 38.231703-58.001615 76.747243-67.059739 90.309749z" fill="#A5213D" p-id="1994"></path><path d="M867.198187 614.434554c0 82.880605-27.75439 129.787869-99.108716 181.656122-63.530279 46.166817-165.144172 57.347554-258.045483 57.347554-102.428384 0-210.101594-18.794991-275.199151-73.612688-60.358702-50.794606-78.018342-101.453463-78.018343-176.806199 0-156.666064 155.987322-316.910951 353.242176-316.910952s357.129518 171.660099 357.129517 328.326163z" fill="#C95065" p-id="1995"></path><path d="M510.043988 859.596274c-56.384974 0-107.870663-5.516324-153.025537-16.413223-52.559336-12.686311-95.023922-32.40686-126.172024-58.630994-30.691493-25.841571-50.831628-51.744845-63.357508-81.523121-11.501597-27.347144-16.857491-59.124625-16.857491-99.96023 0-78.610699 38.120636-159.418056 104.600359-221.714258a375.801102 375.801102 0 0 1 114.041048-73.415236 367.471082 367.471082 0 0 1 140.771153-27.951842 353.63708 353.63708 0 0 1 141.474577 29.728913 387.426106 387.426106 0 0 1 115.36151 77.364281c32.554949 31.493643 59.470166 68.602336 77.82089 107.364692 18.757969 39.490461 28.667607 81.066512 28.667607 120.038661 0 42.970558-7.490847 76.512769-23.570869 105.451872-15.72214 28.383769-39.79898 53.373826-78.080046 81.189921-54.175976 39.330031-139.771551 58.470564-261.673669 58.470564z m0-567.317498c-91.408077 0-178.941152 34.81331-246.371115 98.03507-63.999229 59.963797-100.700676 137.50085-100.700676 212.705497 0 78.980922 20.547381 125.567326 75.821686 172.092025s151.606349 72.144136 271.250105 72.144137c119.199489 0 202.388614-18.363064 254.417296-56.162841 71.292623-51.83123 96.566518-98.047411 96.566519-176.65811 0-74.599949-38.404474-156.271159-102.724562-218.431613-69.145329-66.91165-157.307784-103.724165-248.259253-103.724165z" fill="#A5213D" p-id="1996"></path><path d="M323.673693 613.89156m-86.237295 0a86.237295 86.237295 0 1 0 172.474589 0 86.237295 86.237295 0 1 0-172.474589 0Z" fill="#FFFFFF" p-id="1997"></path><path d="M337.828555 595.42977m-44.61188 0a44.61188 44.61188 0 1 0 89.22376 0 44.61188 44.61188 0 1 0-89.22376 0Z" fill="#42393B" p-id="1998"></path><path d="M342.024416 587.605722m-16.265133 0a16.265134 16.265134 0 1 0 32.530267 0 16.265134 16.265134 0 1 0-32.530267 0Z" fill="#FFFFFF" p-id="1999"></path><path d="M700.326307 613.89156m-86.237294 0a86.237295 86.237295 0 1 0 172.474589 0 86.237295 86.237295 0 1 0-172.474589 0Z" fill="#FFFFFF" p-id="2000"></path><path d="M686.159104 595.42977m-44.61188 0a44.61188 44.61188 0 1 0 89.223761 0 44.61188 44.61188 0 1 0-89.223761 0Z" fill="#42393B" p-id="2001"></path><path d="M681.975584 587.605722m-16.265134 0a16.265134 16.265134 0 1 0 32.530267 0 16.265134 16.265134 0 1 0-32.530267 0Z" fill="#FFFFFF" p-id="2002"></path></svg> -->
    <svg t="1641282544096" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="2285" width="25" height="25"><path d="M169.607158 516.323824s-28.478046-221.801895-4.233745-296.130495c14.56717-44.705255 34.654938-74.457285 79.617564-73.144695 73.505014 2.136175 202.717873 201.598311 202.717873 201.598311L186.77377 521.394023z" fill="#647471" p-id="2286"></path><path d="M187.828989 528.407369l-24.064141-7.103426-0.540478-4.156533c-1.171036-9.123784-28.375097-224.105361-3.976374-298.948702 12.791314-39.223264 33.033503-79.064217 85.92314-77.584336 32.428683 0.939402 77.404177 35.903184 133.703974 103.926207a1228.416518 1228.416518 0 0 1 74.251389 100.606127l3.461633 5.353307z m-12.353784-17.063664l10.29482 3.024104 253.033805-167.535326c-9.445497-14.155377-36.894061-54.292307-69.914695-94.171865-53.04406-64.072385-95.947722-98.367004-124.091186-99.177722-36.263503-1.068088-56.853143 18.208963-73.350592 68.705055-21.670596 66.633222 0.308845 258.374243 4.027848 289.155754z" fill="#283330" p-id="2287"></path><path d="M870.298339 516.323824s28.478046-221.801895 4.246613-296.130495c-14.580039-44.705255-34.667806-74.457285-79.617564-73.144695-73.517883 2.136175-202.74361 201.598311-202.743609 201.598311l260.947947 172.747078z" fill="#647471" p-id="2288"></path><path d="M852.076507 528.407369l-2.496493-1.647171-266.249781-176.298791 3.461634-5.353306A1222.908789 1222.908789 0 0 1 661.030387 244.501973c56.299796-68.023023 101.288159-102.948199 133.703973-103.926207 52.966848-1.518486 73.131827 38.361073 85.923141 77.584337 24.450197 74.843341-2.805338 289.824918-3.976374 298.948701l-0.52761 4.156534z m-250.936235-181.574886l253.046673 167.535326 10.29482-3.024104c3.719004-30.730037 25.73705-222.625481 3.989243-289.155754-16.510317-50.470355-37.099957-69.773142-73.311986-68.705055-28.143464 0.810717-71.047126 35.105336-124.091186 99.177722-33.05924 39.879559-60.520673 80.042225-69.96617 94.171865z" fill="#283330" p-id="2289"></path><path d="M892.380727 609.852263c0 86.425013-28.928444 135.338276-103.334255 189.424687-66.247166 48.15402-172.219469 59.800035-269.093724 59.800035-106.744414 0-219.086636-19.598763-286.968105-76.747883-62.939955-52.966848-81.367683-105.792143-81.367683-184.380224 0-163.353055 162.658155-330.463719 368.348657-330.46372s372.41511 179.01405 372.41511 342.367105z" fill="#647471" p-id="2290"></path><path d="M519.952748 865.511247c-58.79629 0-112.483776-5.765099-159.569708-17.128006-54.807048-13.215975-99.087642-33.792746-131.55493-61.138362-32.01689-26.933823-53.005454-53.944856-66.079876-85.009475-11.993465-28.503783-17.578405-61.653103-17.578405-104.235052 0-81.959635 39.763742-166.235605 109.073617-231.18305a391.975268 391.975268 0 0 1 118.930907-76.567723 383.121723 383.121723 0 0 1 146.778395-29.147209 368.760449 368.760449 0 0 1 147.52477 31.051751 404.290447 404.290447 0 0 1 120.294971 80.685651c33.960037 32.827607 62.02629 71.523261 81.161786 111.956166 19.547289 41.243622 29.880715 84.520472 29.880715 125.159274 0 44.808204-7.798326 79.784854-24.566014 109.961545-16.407369 29.597607-41.513861 55.65637-81.419157 84.674894-56.505693 40.960515-145.761781 60.919597-272.877071 60.919596z m0-591.591826c-95.304295 0-186.593611 36.314977-256.907231 102.24043-66.73617 62.528162-105.007163 143.368236-105.007163 221.789027 0 82.358559 21.426094 130.950109 79.077086 179.451579s158.089828 75.242265 282.837308 75.242265c124.30995 0 211.043808-19.161234 265.310378-58.564656 74.341468-54.047805 100.683339-102.24043 100.683339-184.225803 0-77.790233-40.033981-162.954131-107.117601-227.77289-72.102345-69.760273-164.035087-108.159952-258.876116-108.159952z" fill="#283330" p-id="2291"></path><path d="M516.735617 428.328851a117.42529 117.42529 0 0 1-5.031593-23.536532 199.886797 199.886797 0 0 1 0-47.060195 116.614573 116.614573 0 0 1 5.05733-23.536532 117.541106 117.541106 0 0 1 5.070199 23.536532 201.739865 201.739865 0 0 1 0 47.060195 117.463895 117.463895 0 0 1-5.095936 23.536532zM586.959158 381.255787a48.2441 48.2441 0 0 1 0.527609 15.056174 71.343102 71.343102 0 0 1-2.959761 13.949481 70.468042 70.468042 0 0 1-5.559202 13.113027 47.201749 47.201749 0 0 1-9.110916 12.006334 47.870913 47.870913 0 0 1-0.540478-15.056174 73.350592 73.350592 0 0 1 8.583306-27.023903 47.973861 47.973861 0 0 1 9.059442-12.044939zM615.591625 452.495941a67.482545 67.482545 0 0 1 7.721115-16.201473 105.521904 105.521904 0 0 1 10.436374-13.769322 103.37286 103.37286 0 0 1 12.495338-11.916254 65.758162 65.758162 0 0 1 15.236333-9.471234 65.809636 65.809636 0 0 1-7.721115 16.201473 101.494056 101.494056 0 0 1-10.449242 13.756453 104.453817 104.453817 0 0 1-12.508206 11.916254 66.517405 66.517405 0 0 1-15.210597 9.484103zM661.467917 469.611079a25.865735 25.865735 0 0 1 4.169402-9.741473 32.338603 32.338603 0 0 1 14.605775-11.581673 25.634102 25.634102 0 0 1 10.449243-1.82733 25.608365 25.608365 0 0 1-4.156534 9.76721 31.566492 31.566492 0 0 1-6.434262 6.858924 32.171312 32.171312 0 0 1-8.158645 4.70988 25.73705 25.73705 0 0 1-10.474979 1.814462zM690.692337 501.846734a38.682786 38.682786 0 0 1 10.397768-8.493227 55.21884 55.21884 0 0 1 11.478724-5.018724 54.356649 54.356649 0 0 1 12.30231-2.367809 38.20665 38.20665 0 0 1 13.383266 1.158167 38.258125 38.258125 0 0 1-10.397768 8.506095 53.790434 53.790434 0 0 1-11.478725 4.992988 54.61402 54.61402 0 0 1-12.302309 2.35494 38.747128 38.747128 0 0 1-13.383266-1.13243zM446.524945 381.255787a48.282705 48.282705 0 0 1 9.085179 12.019202 73.003142 73.003142 0 0 1 8.570437 27.023903 47.124538 47.124538 0 0 1-0.540478 15.056174 47.446251 47.446251 0 0 1-9.098047-12.006334 70.519516 70.519516 0 0 1-5.546334-13.08729 72.269636 72.269636 0 0 1-2.946892-13.949481 48.2441 48.2441 0 0 1 0.476135-15.056174zM417.879609 452.495941a66.144218 66.144218 0 0 1-15.210597-9.484103 108.095609 108.095609 0 0 1-22.957448-25.672707 66.73617 66.73617 0 0 1-7.721115-16.201473 65.629477 65.629477 0 0 1 15.236334 9.471234 107.272024 107.272024 0 0 1 22.918842 25.73705 66.517405 66.517405 0 0 1 7.733984 16.149999zM372.003318 469.611079a25.64697 25.64697 0 0 1-10.436374-1.827331 31.720914 31.720914 0 0 1-8.158645-4.70988 31.257647 31.257647 0 0 1-6.434262-6.858923 25.608365 25.608365 0 0 1-4.156534-9.767211 25.634102 25.634102 0 0 1 10.449242 1.827331 32.171312 32.171312 0 0 1 14.592908 11.581672 25.659839 25.659839 0 0 1 4.143665 9.754342zM342.791766 501.846734a38.772866 38.772866 0 0 1-13.383266 1.13243 54.472466 54.472466 0 0 1-12.30231-2.35494 53.507327 53.507327 0 0 1-11.478724-4.992988 38.258125 38.258125 0 0 1-10.397768-8.506095 38.20665 38.20665 0 0 1 13.383266-1.158167 54.485334 54.485334 0 0 1 12.30231 2.367809 55.514816 55.514816 0 0 1 11.478724 5.018724 38.682786 38.682786 0 0 1 10.397768 8.493227zM520.467489 722.168749a73.955413 73.955413 0 0 1 5.031594 18.196094 118.519114 118.519114 0 0 1 1.3898 18.157488 116.884812 116.884812 0 0 1-1.364063 18.183226 72.835851 72.835851 0 0 1-5.057331 18.183226 72.333978 72.333978 0 0 1-5.070198-18.183226 116.884812 116.884812 0 0 1-1.364064-18.183226 118.519114 118.519114 0 0 1 1.389801-18.183225 73.440672 73.440672 0 0 1 5.044461-18.170357zM590.678161 758.522331a33.535376 33.535376 0 0 1-9.007967-8.5447 45.374419 45.374419 0 0 1-5.572071-9.934501 44.821072 44.821072 0 0 1-3.011235-10.976852 33.213663 33.213663 0 0 1 0.411792-12.366652 33.007766 33.007766 0 0 1 9.007968 8.5447 44.422148 44.422148 0 0 1 5.546334 9.921633 45.631789 45.631789 0 0 1 2.998366 10.98972 33.728404 33.728404 0 0 1-0.373187 12.366652zM619.323498 703.496519a53.584538 53.584538 0 0 1 14.760198 6.125418 81.277603 81.277603 0 0 1 12.379521 8.866413 79.656169 79.656169 0 0 1 10.565059 10.963984 52.490713 52.490713 0 0 1 8.158644 13.730716 53.082665 53.082665 0 0 1-14.773066-6.086813 80.428281 80.428281 0 0 1-12.366653-8.89215 81.30334 81.30334 0 0 1-10.565059-10.976852 53.700354 53.700354 0 0 1-8.158644-13.730716zM665.18692 690.319149a23.240556 23.240556 0 0 1 9.94737 0.167291 27.023902 27.023902 0 0 1 8.02996 3.294343 26.547767 26.547767 0 0 1 6.588684 5.649282 22.777289 22.777289 0 0 1 4.658406 8.789203 23.047528 23.047528 0 0 1-9.947369-0.154423 27.358484 27.358484 0 0 1-14.605776-9.007967 23.008923 23.008923 0 0 1-4.671275-8.737729zM694.41134 665.36708a37.486013 37.486013 0 0 1 13.061553-2.058964 52.760952 52.760952 0 0 1 12.21223 1.492749 51.911629 51.911629 0 0 1 11.581673 4.169402 37.524619 37.524619 0 0 1 10.719481 7.721115 37.537487 37.537487 0 0 1-13.074421 2.071832 52.001709 52.001709 0 0 1-12.199362-1.505617 53.314299 53.314299 0 0 1-11.581672-4.195139 37.666172 37.666172 0 0 1-10.719482-7.695378zM450.282555 758.522331a33.728404 33.728404 0 0 1-0.386056-12.366652 45.631789 45.631789 0 0 1 2.998366-10.98972 44.936889 44.936889 0 0 1 5.546334-9.921633 33.007766 33.007766 0 0 1 9.007968-8.5447 33.213663 33.213663 0 0 1 0.411793 12.366652 44.821072 44.821072 0 0 1-3.011235 10.976852 45.374419 45.374419 0 0 1-5.572071 9.934501 33.535376 33.535376 0 0 1-8.995099 8.5447zM421.598613 703.496519a53.700354 53.700354 0 0 1-8.158645 13.730716 81.30334 81.30334 0 0 1-10.565059 10.976852 80.428281 80.428281 0 0 1-12.366653 8.89215 53.082665 53.082665 0 0 1-14.773066 6.086813 52.851032 52.851032 0 0 1 8.158645-13.743585 79.656169 79.656169 0 0 1 10.565059-10.963983 81.277603 81.277603 0 0 1 12.379521-8.866414 53.584538 53.584538 0 0 1 14.760198-6.112549zM375.73519 690.319149a23.008923 23.008923 0 0 1-4.671275 8.776334 27.229799 27.229799 0 0 1-14.605775 9.007968 23.047528 23.047528 0 0 1-9.94737 0.154422 22.777289 22.777289 0 0 1 4.684143-8.930756 26.547767 26.547767 0 0 1 6.588685-5.649283 27.023902 27.023902 0 0 1 8.029959-3.294342 23.240556 23.240556 0 0 1 9.921633-0.064343zM346.51077 665.36708a37.666172 37.666172 0 0 1-10.719481 7.721115 53.314299 53.314299 0 0 1-11.581673 4.195139 52.001709 52.001709 0 0 1-12.199361 1.505617 37.537487 37.537487 0 0 1-13.074422-2.071832 37.524619 37.524619 0 0 1 10.719482-7.721115 51.911629 51.911629 0 0 1 11.581672-4.169402 52.760952 52.760952 0 0 1 12.21223-1.492749 37.434539 37.434539 0 0 1 13.061553 2.033227z" fill="#4B5451" p-id="2292"></path><path d="M331.248699 594.114057m-74.740392 0a74.740393 74.740393 0 1 0 149.480785 0 74.740393 74.740393 0 1 0-149.480785 0Z" fill="#E8BE48" p-id="2293"></path><path d="M708.656797 594.114057m-74.740392 0a74.740393 74.740393 0 1 0 149.480785 0 74.740393 74.740393 0 1 0-149.480785 0Z" fill="#E8BE48" p-id="2294"></path></svg>
</a>


        <!-- Page Header -->
        <header class="intro-header">
            <div class="container">
                <div class="row">
                    <div class="col-lg-8  col-md-10 ">
                        <div class="post-heading">
                            <h1>
                                6. Logistic Regression
                            </h1>
                        </div>
                    </div>
                </div>
            </div>
        </header>

        <!-- Post Content -->
        <article>
            <div class="container">
                <div class="row">
                    <!-- Post Main Content -->
                    <div class="post-content col-lg-10  col-md-10 ">
                        <span id="more"></span>
<h1 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h1><h2 id="1-历史背景"><a href="#1-历史背景" class="headerlink" title="1. 历史背景"></a>1. 历史背景</h2><p>1830 年代和 1840 年代，在 Adolphe Quetelet 的指导下，Pierre François Verhulst 将逻辑函数（logistic function）作为人口增长模型并命名为“逻辑函数”。Verhulst 在 1830 年代中期首先设计了这个函数，但没有具体说明他如何将曲线拟合到数据中。随后，在 1838 年发表了一篇简短的笔记。最后在 1844 年进行了扩展分析并命名了这个函数（1845 年出版），Verhulst 通过使曲线通过三个观察点来确定模型的三个参数，这产生了较差的预测。逻辑函数最早应用就是作为人口增长的通用模型，满足微分方程式 $\displaystyle {\frac {dP}{dt}}=rP\left(1-{\frac {P}{K}}\right)$，其中，$P$ 代表种群大小，$t$ 代表时间，常数 $r$ 定义为增长率，$K$ 是承载容量，微分方程结果为 $\displaystyle P(t)={\frac {K}{1+\left({\frac {K-P_{0}}{P_{0}}}\right)e^{-rt}}}$，$\displaystyle \frac {K-P_{0}}{P_{0}}$ 是常数可以移动到指数上，相当于对 $t$ 进行平移，就可以得到逻辑函数标准形式。</p>
<p>1883 年，逻辑函数在化学领域中独立开发出来作为自催化模型（Wilhelm Ostwald）。自催化反应是指反应中的一种产物本身就是同一反应的催化剂，而其中一种反应物的供应是固定的。出于与人口增长相同的原因，这自然会产生逻辑方程：反应是自我强化的，但受到限制。</p>
<p>1920 年，逻辑函数被 Raymond Pearl 和 Lowell Reed 再次独立地发现为人口增长的模型，这导致它在现代统计学中的应用。Pearl 和 Reed 将该模型首次应用于美国人口，并通过三个点对曲线进行了初步拟合，与 Verhulst 一样，这再次产生了糟糕的结果。他们最初不知道 Verhulst 的工作，大概是从 L. Gustave du Pasquier 那里了解到的，但他们没有给他多少信任，也没有采用他的术语。 在 1925 年，Verhulst 的优先权得到承认，Udny Yule 重新使用了“logistic”一词，并一直沿用至今。</p>
<p>1930 年代，概率模型（probit model）由 Chester Ittner Bliss 开发和系统化，并且 John Gaddum 通过 Ronald A. Fisher 的最大似然估计拟合该模型。probit 模型主要用于生物测定，早在 1860 年的工作就已经开始了。probit 模型影响了 logit 模型的后续发展。</p>
<p>1943 年，逻辑模型（logistic model）可能首先被 Edwin Bidwell Wilson 和他的学生 Jane Worcester 用作生物测定中概率模型的替代方案。然而，logistic 模型作为 probit 模型的一般替代方案的发展主要归功于 Joseph Berkson 数十年来的工作，从 1944年开始，他通过与“probit”类比创造了“logit”。logit 模型最初被认为不如 probit 模型，但“逐渐达到了与 probit 平等的地位”，特别是在 1960 年至 1970 年间。到 1970 年，统计期刊中使用logit 模型与 probit 模型数量相当，然后前者超越了后者。这种相对流行是由于在生物测定之外采用了 logit，而不是在生物测定中取代 probit，以及它在实践中的非正式使用；logit 的受欢迎程度归功于 logit 模型的计算简单性、数学特性和通用性，允许其在各种领域中使用。</p>
<p>1966年，多项式logit模型由 David Cox 引入，大大增加了logit模型的应用范围和普及度。1973 年，Daniel McFadden 将多项式 logit 与离散选择理论，特别是 Luce 的选择公理联系起来，表明多项式 logit 遵循 independence of irrelevant alternatives (IIA) 假设并将备选方案的几率解释为相对偏好，这给出了一个逻辑回归的理论基础。</p>
<h2 id="2-算法模型"><a href="#2-算法模型" class="headerlink" title="2. 算法模型"></a>2. 算法模型</h2><p>逻辑回归实际上是分类模型，之所以名字中带有回归，我想是因为它是对概率值进行回归的。逻辑回归是对事件发生的概率进行建模，根据分类事件的数量分为二元逻辑回归和多元逻辑回归，</p>
<h3 id="2-1-逻辑函数"><a href="#2-1-逻辑函数" class="headerlink" title="2.1 逻辑函数"></a>2.1 逻辑函数</h3><p>阐述逻辑回归模型之前，先介绍一下逻辑函数（logistic function）。</p>
<p>逻辑函数的公式如下</p>
<script type="math/tex; mode=display">\displaystyle f(x)={\frac {L}{1+e^{-k(x-x_{0})}}}</script><p>式中，$x_0$ 是曲线的中点坐标，$L$ 是曲线的最大值，$k$ 是逻辑增长率或曲线的陡度。逻辑函数最早用于人口增长预测，$f(x)$ 是人口数量，$x$ 代表时间。</p>
<p>当 $\displaystyle L=1,k=1,x_{0}=0$ 时就得到了 sigmoid 函数</p>
<script type="math/tex; mode=display">\displaystyle f(x)={\frac {1}{1+e^{-x}}}</script><p>与正态分布函数进行类比，逻辑函数改为</p>
<script type="math/tex; mode=display">\displaystyle f(x)={\frac {1}{1+e^{-(x-\mu )/s}}}</script><p>该函数图形如图所示，是一条 S 形曲线（sigmoid curve），可以发现形状与正态分布函数相似。该函数曲线以点 $\displaystyle(\mu,\frac{1}{2})$ 为中心对称，满足 </p>
<script type="math/tex; mode=display">f(x)=1-f(-x)</script><p>逻辑函数还有一个非常好的性质，就是其导数易求</p>
<script type="math/tex; mode=display">f'(x)=f(x)(1-f(x))</script><p>逻辑函数的导函数满足概率密度公式的要求，因此逻辑函数就是一种概率分布函数，至于为什么在分类问题中比正态分布好用还需考证。</p>
<p>logistic model 与 probit model 的区别就是在于使用逻辑函数取代了正态分布函数。</p>
<h3 id="2-2-二元逻辑回归"><a href="#2-2-二元逻辑回归" class="headerlink" title="2.2 二元逻辑回归"></a>2.2 二元逻辑回归</h3><p>假设有数据集</p>
<script type="math/tex; mode=display">D=\{(x_1,y_1),(x_2,y_2),\dots,(x_k,y_k),\dots,(x_N,y_N)\}</script><p>二元逻辑回归就是在分类数是 2 的情况下，也就是二项分布的情况下的逻辑回归。回忆一下伯努利分布，其中的 $y=1$ 事件对应的概率分布就是由逻辑函数 $f(x)$ 给出，由于与 $x$ 有关，因此很明显是条件概率，如下所示</p>
<script type="math/tex; mode=display">p(y=1|\, x)={\frac {1}{1+e^{-(x-\mu )/s}}}</script><p>由于是二项分布，因此</p>
<script type="math/tex; mode=display">p(y=0|\, x)=1 - p(y=1|\, x)</script><p>我们用 $p(x)$ 对 $p(y=1|\, x)$ 进行简写，同时把上式标准化</p>
<script type="math/tex; mode=display">\displaystyle p(x)={\frac {1}{1+e^{-(\beta _{0}+\beta _{1}x)}}}</script><p>其中 $\beta_0=-\mu/s$ 称为截距，$\beta_1=1/s$。那么对特征 $x_k$ 的类别 $y_k$ 概率拟合就是</p>
<script type="math/tex; mode=display">p_k=p(x_k)</script><p>有了概率分布之后，我们只需要用最大似然估计确定参数的值即可。如前所说，二元逻辑回归就是在伯努利分布的情况下，那么伯努利分布的似然函数就是</p>
<script type="math/tex; mode=display">
\begin{aligned}
    \displaystyle L&= \prod _{k:y_{k}=1}p_{k}\,\prod _{k:y_{k}=0}(1-p_{k})\\
    &= \prod^N_{k=1}p_{k}^{y_k}(1-p_{k})^{1-y_k}\\
\end{aligned}</script><p>对 $L$ 取对数，得到对数似然函数</p>
<script type="math/tex; mode=display">
\begin{aligned}
    \displaystyle \ell&=\sum _{k:y_{k}=1}\ln(p_{k})+\sum _{k:y_{k}=0}\ln(1-p_{k})\\
    &= \sum _{k=1}^{N}\left(\,y_{k}\ln(p_{k})+(1-y_{k})\ln(1-p_{k})\right)\\
\end{aligned}</script><p>上式是有关 $\beta_0,\beta_1$ 的函数，为了使对数似然函数最大，分别对 $\beta_0,\beta_1$ 求偏导数</p>
<script type="math/tex; mode=display">\displaystyle {\frac {\partial \ell }{\partial \beta _{0}}}=\sum _{k=1}^{K}(y_{k}-p_{k})</script><script type="math/tex; mode=display">\displaystyle {\frac {\partial \ell }{\partial \beta _{1}}}=\sum _{k=1}^{K}(y_{k}-p_{k})x_{k}</script><p>结果和线性回归太相似了，$p_k$ 如果是线性函数，那么上式就是线性回归。事实上，$p(x)$ 的作用就是将线性函数 $\beta _{0}+\beta _{1}x$ 的结果映射到 $[0,1]$ 概率区间。如果让上式分别等于 0，则得到似然方程。</p>
<p>不过，尽管结果相似，但上式是非线性的，因此无法像线性回归一样，求出似然方程组的解析解。因此，需要用数值方法，如牛顿迭代法和梯度下降法等等。牛顿迭代法是求解的似然方程组，梯度下降法则是直接求似然函数的极值，区别在于牛顿迭代法需要二次导数。下面给出梯度下降法的计算公式，其实很简单，沿与梯度相反的方向移动即可，导数已经给出，梯度则为</p>
<script type="math/tex; mode=display">\bigtriangledown p(\beta_0,\beta_1)=(\frac {\partial \ell }{\partial \beta _{0}},\frac {\partial \ell }{\partial \beta _{1}})</script><p>假设第 $t$ 次的迭代结果为 $\beta^t_0,\beta^t_1$，则</p>
<script type="math/tex; mode=display">\beta^{t+1}_0=\beta^t_0-\alpha\sum _{k=1}^{K}(y_{k}-{\frac {1}{1+e^{-(\beta^t_0+\beta^t_1x_k)}}})</script><script type="math/tex; mode=display">\beta^{t+1}_1=\beta^t_1-\alpha\sum _{k=1}^{K}(y_{k}-{\frac {1}{1+e^{-(\beta^t_0+\beta^t_1x_k)}}})x_{k}</script><p>上面求解的是简单的情况，只有一个特征。进一步地，将上面结果推广到具有多个特征的情况。</p>
<p>用矩阵形式表述如下：$X$ 是样本特征矩阵，大小为 $m\times n$，$n$ 是特征维度加1，$m$ 是样本数量，第 $i$ 行形如 $[1, x_1^{(i)}, x_2^{(i)}, x_3^{(i)},\dots, , x_{n-1}^{(i)}]$；$\mathbf{w}$ 是 $n\times1$ 系数向量，形如 $[\beta_1,\beta_2,\dots,\beta_n]^T$；$\mathbf y$ 是 $m\times1$ 列向量，形如 $[y_1,y_2,\dots,y_m]^T$，每个元素取值 $0$ 或 $1$，其中只有一个元素为 $1$；$\mathbf 1^T$ 为值均为 $1$ 的列向量。逻辑函数为</p>
<script type="math/tex; mode=display">p(X\mathbf{w})={\frac {1}{1+e^{-X\mathbf{w}}}}</script><p>对数似然函数的矩阵形式为</p>
<script type="math/tex; mode=display">\ell(\mathbf{w})=\mathbf y^T\ln p(X\mathbf{w})+(\mathbf 1^T-\mathbf y^T)\ln(\mathbf 1-p(X\mathbf{w}))</script><p>根据逐元素函数法则，逻辑函数的微分</p>
<script type="math/tex; mode=display">
\begin{aligned}
    \mathrm{d}p(X\mathbf{w})&=p'(X\mathbf{w})\odot\mathrm{d}X\mathbf{w} \\
    &= p(X\mathbf{w})\odot(\mathbf 1-p(X\mathbf{w}))\odot ((\mathrm{d}X)\mathbf{w}+X\mathrm{d}\mathbf{w}) \\
    &=p(X\mathbf{w})\odot(\mathbf 1-p(X\mathbf{w}))\odot X\mathrm{d}\mathbf{w}
\end{aligned}</script><p>$\ln$ 函数的微分</p>
<script type="math/tex; mode=display">\mathrm{d}\ln Y=Y^{-1}\odot\mathrm{d}Y</script><p>因此可以得到</p>
<script type="math/tex; mode=display">
\begin{aligned}
\mathrm{d}\ell(\mathbf{w})&=\mathbf y^T(p(X\mathbf{w}))^{-1}\odot p(X\mathbf{w})\odot(\mathbf 1-p(X\mathbf{w}))\odot X\mathrm{d}\mathbf{w} \\
&+(\mathbf 1^T-\mathbf y^T)(\mathbf 1-p(X\mathbf{w}))^{-1}\odot -p(X\mathbf{w})\odot(\mathbf 1-p(X\mathbf{w}))\odot X\mathrm{d}\mathbf{w} \\
&=\mathbf y^T(\mathbf 1-p(X\mathbf{w}))\odot X\mathrm{d}\mathbf{w} - (\mathbf 1^T-\mathbf y^T)p(X\mathbf{w})\odot X\mathrm{d}\mathbf{w} \\
\end{aligned}</script><p>将上式套上迹</p>
<script type="math/tex; mode=display">
\begin{aligned}
\mathrm{d}\ell(\mathbf{w})&=\mathrm{tr}(\mathbf y^T(\mathbf 1-p(X\mathbf{w}))\odot X\mathrm{d}\mathbf{w}- (\mathbf 1^T-\mathbf y^T)p(X\mathbf{w})\odot X\mathrm{d}\mathbf{w})\\
&=\mathrm{tr}([\mathbf y\odot(\mathbf 1-p(X\mathbf{w}))]^TX\mathrm{d}\mathbf{w} - [(\mathbf 1-\mathbf y)\odot p(X\mathbf{w})]^T X\mathrm{d}\mathbf{w})\\
&=\mathrm{tr}([\mathbf y-\mathbf y\odot p(X\mathbf{w})]^TX\mathrm{d}\mathbf{w} - [p(X\mathbf{w})-\mathbf y\odot p(X\mathbf{w})]^T X\mathrm{d}\mathbf{w})\\
&=\mathrm{tr}(\mathbf y ^ TX\mathrm{d}\mathbf{w} - p(X\mathbf{w})^T X\mathrm{d}\mathbf{w})\\
&=\mathrm{tr}([X^T(\mathbf y- p(X\mathbf{w}) )]^T\mathrm{d}\mathbf{w})\\
\end{aligned}</script><p>根据微分与导数的关系</p>
<script type="math/tex; mode=display">\mathrm{d}f=\mathrm{tr}(\frac {\partial f }{\partial X}^T \mathrm{d}X)</script><p>就可以得到</p>
<script type="math/tex; mode=display">\frac {\partial f }{\partial \mathbf{w}}=X^T(\mathbf y- p(X\mathbf{w}) )</script><p>迭代公式</p>
<script type="math/tex; mode=display">\mathbf{w}=\mathbf{w}-\alpha X^T(\mathbf y- p(X\mathbf{w}) )</script><h3 id="2-3-多元逻辑回归"><a href="#2-3-多元逻辑回归" class="headerlink" title="2.3 多元逻辑回归"></a>2.3 多元逻辑回归</h3><p>一个事件的几率（odds）是指该事件的发生的概率与不发生的概率的比值。如果一个事件发生的概率 $p$，则不发生的概率为 $1-p$，那么该事件的几率为 $\displaystyle\frac{p}{1-p}$，该事件的对数几率或 logit 函数是</p>
<script type="math/tex; mode=display">\mathrm{logit}(p)=\ln \frac{p}{1-p}</script><p>对于逻辑回归，一定满足</p>
<script type="math/tex; mode=display">\ln \frac{p}{1-p}=\mathbf{w}\cdot\mathbf x</script><p>假设多元逻辑回归事件的类别分别为 $1,2,\dots,K$。多元逻辑回归相较于二元逻辑回归，概率计算公式不变</p>
<script type="math/tex; mode=display">\displaystyle p(\mathbf x)={\frac {1}{1+e^{-\mathbf{w}\cdot\mathbf x}}}</script><p>区别在于对于同一个实例，它所有类别的概率和为 1。在二元逻辑回归中，$y=0$ 时的概率就是 $1-p$，因此只需要一组参数即可，而在多元逻辑回归中， $K$ 个类别分别对应一组参数</p>
<script type="math/tex; mode=display">\ln \frac{p(y=1|\mathbf x)}{1-p(y=0|\mathbf x)}=\mathbf w_1\cdot\mathbf x</script><script type="math/tex; mode=display">\ln \frac{p(y=2|\mathbf x)}{1-p(y=0|\mathbf x)}=\mathbf w_2\cdot\mathbf x</script><script type="math/tex; mode=display">\cdots</script><p>同时满足</p>
<script type="math/tex; mode=display">\sum_{i=1}^Kp(y=i|\mathbf x)=1</script><p>最终解得</p>
<script type="math/tex; mode=display">p(y=k|\mathbf x)=\frac{e^{\mathbf w_k\cdot\mathbf x}}{\displaystyle 1+\sum_{i=1}^{K-1}e^{\mathbf w_i\cdot\mathbf x}}</script><script type="math/tex; mode=display">p(y=K|\mathbf x)=\frac{1}{\displaystyle 1+\sum_{i=1}^{K-1}e^{\mathbf w_i\cdot\mathbf x}}</script><p>实际上就是 softmax 函数的一种特例，输出的结果为 $k-1$ 个，因为概率和为 1，自然可以求出最后剩下的一个概率值。其实也可以输出 $k$ 个结果，概率的计算公式为</p>
<script type="math/tex; mode=display">p(y=k|\mathbf x)=\frac{e^{\mathbf w_k\cdot\mathbf x}}{\displaystyle \sum_{i=1}^Ke^{\mathbf w_i\cdot\mathbf x}}</script><p>我们采用后一种方式，似然函数为</p>
<script type="math/tex; mode=display">L=\prod_{i=0}^mp_1(\mathbf{x}_i)^{y_i^1}p_2(\mathbf{x}_i)^{y_i^2}\dots p_k(\mathbf{x}_i)^{y_i^K}</script><p>对于一组输入 $\mathbf{x}_i$，$(y_i^1,y_i^2,\dots,y_i^K)$ 中只有一个值为 $1$，其他为 $0$。对数似然函数即为</p>
<script type="math/tex; mode=display">\ell=\sum_{i}\sum_ky_i^k\log p_k(\mathbf{x}_i)</script><p>将对数似然函数写成矩阵形式，对于每一组实例</p>
<script type="math/tex; mode=display">\ell=\mathbf{y}^T\log \mathrm{softmax}(W\mathbf{x})</script><p>其中，$\mathbf{y}$ 是除一个元素为 $1$ 外其元素为 $0$ 的 $m\times1$ 列向量，$W$ 为 $m\times n$ 矩阵，$\mathbf{x}$ 为 $n\times1$ 列向量，$\displaystyle\mathrm{softmax}(\mathbf{z})=\frac{\exp(\mathbf{z})}{\mathbf{1}^T\exp(\mathbf{z})}$，其中 $\exp$ 表示逐元素求指数，$\mathbf{1}$ 表示值均为 $1$ 的列向量；$\ell$ 为标量。</p>
<p>将 $\mathrm{softmax}$ 带入 $\ell$</p>
<script type="math/tex; mode=display">\begin{aligned}
    \ell&=\mathbf{y}^T\left(\log \exp(W\mathbf{x})-\mathbf{1}\log \mathbf{1}^T\exp(W\mathbf{x})\right)\\
    &=\mathbf{y}^TW\mathbf{x}-\log\left( \mathbf{1}^T\exp(W\mathbf{x})\right)
\end{aligned}</script><p>在这里用到了 $\log(\mathbf{u}/c)=\log\mathbf{u}-\mathbf{1}\log c$，以及 $\mathbf{y}^T\mathbf{1}=1$。</p>
<p>进行微分</p>
<script type="math/tex; mode=display">\begin{aligned}
    \mathrm{d}\ell&=\mathrm{d}(\mathbf{y}^TW\mathbf{x})-\mathrm{d}\left(\log( \mathbf{1}^T\exp(W\mathbf{x}))\right)\\
    &=\mathbf{y}^T\mathrm{d}W\mathbf{x}-(\mathbf{1}^T\exp(W\mathbf{x}))^{-1}\odot \mathrm{d}(\mathbf{1}^T\exp(W\mathbf{x}))\\
    &=\mathbf{y}^T\mathrm{d}W\mathbf{x}-(\mathbf{1}^T\exp(W\mathbf{x}))^{-1}\odot \mathbf{1}^T\mathrm{d}(\exp(W\mathbf{x}))\\
    &=\mathbf{y}^T\mathrm{d}W\mathbf{x}-(\mathbf{1}^T\exp(W\mathbf{x}))^{-1}\odot \mathbf{1}^T\left(\exp(W\mathbf{x})\odot\mathrm{d}(W\mathbf{x})\right)\\
\end{aligned}</script><p>其中</p>
<script type="math/tex; mode=display">\mathbf{1}^T\left(\exp(W\mathbf{x})\odot\mathrm{d}(W\mathbf{x})\right)=(\mathbf{1}\odot \exp(W\mathbf{x}))^T\mathrm{d}(W\mathbf{x})=\exp(W\mathbf{x})^T\mathrm{d}(W\mathbf{x})</script><p>带入原式并且套上迹</p>
<script type="math/tex; mode=display">\begin{aligned}
    \mathrm{d}\ell&=\mathrm{tr}\left(\mathbf{y}^T\mathrm{d}W\mathbf{x}-\frac{\exp(W\mathbf{x})^T\mathrm{d}(W\mathbf{x})}{\mathbf{1}^T\exp(W\mathbf{x})}\right)\\
    &=\mathrm{tr}\left(\mathbf{y}^T\mathrm{d}W\mathbf{x}-\mathrm{softmax}(W\mathbf{x})^T\mathrm{d}(W\mathbf{x})\right)\\
    &=\mathrm{tr}\left(\mathbf{x}(\mathbf{y}-\mathrm{softmax}(W\mathbf{x}))^T\mathrm{d}W\right)\\
\end{aligned}</script><p>根据微分与导数的关系</p>
<script type="math/tex; mode=display">\mathrm{d}f=\mathrm{tr}(\frac {\partial f }{\partial X}^T \mathrm{d}X)</script><p>因此</p>
<script type="math/tex; mode=display">\frac {\partial f }{\partial W}=(\mathbf y- \mathrm{softmax}(W\mathbf{x}))\mathbf{x}^T</script><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><blockquote>
<p><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Logistic_regression">1. Logistic regression - Wikipedia</a><br><a target="_blank" rel="noopener" href="https://papers.tinbergen.nl/02119.pdf">2. Cramer, J. S. (2002). The origins of logistic regression</a><br><a target="_blank" rel="noopener" href="https://synapse.koreamed.org/upload/synapsedata/pdfdata/0006jkan/jkan-43-154.pdf">3. An Introduction to Logistic Regression: From Basic Concepts to Interpretation with Particular Attention to Nursing Domain</a><br><a target="_blank" rel="noopener" href="https://ftp.idu.ac.id/wp-content/uploads/ebook/ip/REGRESI%20LOGISTIK/Practical%20Guide%20to%20Logistic%20Regression%20(%20PDFDrive%20">4. Practical Guide to Logistic Regression</a>.pdf)<br><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Logistic_function">5. Logistic function - Wikipedia</a><br><a target="_blank" rel="noopener" href="https://www.cnblogs.com/pinard/p/6029432.html">6. 逻辑回归原理小结 - 刘建平</a></p>
</blockquote>


                            <!-- Meta -->
                            <div class="post-meta">
                                <hr>
                                <br>
                                <div class="post-tags">
                                    
                                                

<a href="/categories/Machine-learning/">Machine learning</a>

                                                    
                                </div>
                                <div class="post-date">
                                    
                                        2022-02-20
                                    
                                </div>
                            </div>
                    </div>

                    <!-- Comments -->
                    <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                        <!-- Disqus Comments -->


                    </div>
                </div>
            </div>
        </article>
</section>

<!-- Image viewer-->

    <!-- Custom picture view-->
    <link href="/css/viewer.min.css" rel="stylesheet" />
    <script
      src="/js/viewer.min.js"
      type="text/javascript"
      charset="utf-8"
    ></script>
    
    <script type="text/javascript">
      // set image viewer
      Viewer.setDefaults({
        zoomRatio: [0.5],
        navbar: false,
        toolbar: false,
        button: false,
        title: [2, (image, imageData) => `${image.alt}`],
        show: function() {
          this.viewer.zoomTo(0.5);
        }
      });
      var imageList = document.getElementsByTagName("img");
      Array.prototype.forEach.call(imageList, element => {
        var viewer = new Viewer(element);
      });
    </script>

    

<!-- TOC -->

    <aside id="article-toc" role="navigation" class="fixed">
        <div id="article-toc-inner">
            <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92"><span class="toc-text">逻辑回归</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E5%8E%86%E5%8F%B2%E8%83%8C%E6%99%AF"><span class="toc-text">1. 历史背景</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E7%AE%97%E6%B3%95%E6%A8%A1%E5%9E%8B"><span class="toc-text">2. 算法模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-%E9%80%BB%E8%BE%91%E5%87%BD%E6%95%B0"><span class="toc-text">2.1 逻辑函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-%E4%BA%8C%E5%85%83%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92"><span class="toc-text">2.2 二元逻辑回归</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-%E5%A4%9A%E5%85%83%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92"><span class="toc-text">2.3 多元逻辑回归</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83"><span class="toc-text">参考</span></a></li></ol></li></ol>
        </div>
    </aside>

    <!-- Scripts -->
    <script type="text/javascript">
    console.log("© He1o 2021-" + new Date().getFullYear());
</script>
  
    <!-- Google Analytics -->
    

    <!-- Service Worker -->
    <!-- if using service worker -->

    
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
</body>

</html>